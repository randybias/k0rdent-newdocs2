{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the k0rdent docs","text":""},{"location":"#introduction","title":"Introduction","text":"<p>k0rdent has been developed to provide a way to manage distributed infrastructure at massive scale leveraging kubernetes.</p> <p>The project is based on the premise that:</p> <ul> <li>Kubernetes and its ecosystem is mature and inherently stable.</li> <li>Large scale adoption means that it can run anywhere.</li> <li>Community standards and open source projects ensure support and reduces adoption risk.</li> </ul> <p>The goal of the k0rdent project is to provide platform engineers with the means to deliver a distributed container management environment (DCME) and enable them to compose unique internal developer platforms (IDP) to support a diverse range of complex modern application workloads.</p> <p>Another way to think of k0rdent is as a \"super control plane\" designed to ensure the consistent provisioning and lifecycle management of kubernetes clusters and the services that make it useful.</p> <p>In short: Kubernetes clusters at scale, managed centrally, template driven, based on open community driven standards, enabling Golden Paths ... k0rdent aspires to do that.</p> <p>Whether you want to manage Kubernetes clusters on-premises, in the cloud, or a combination of both, k0rdent provides a consistent way to do it. With full life-cycle management, including provisioning, configuration, and maintenance, k0rdent is designed to be a repeatable and secure way to manage your Kubernetes clusters in a central location.</p>"},{"location":"#k0rdent-vs-project-2a","title":"k0rdent vs Project 2A","text":"<p>k0rdent is the official name of this open source project developed by Mirantis. It started out life as an internal project codenamed \"Project 2A\". 2A references the hexadecimal 0x2A, which encompasses our hopes for the project.</p>"},{"location":"#k0rdent-components","title":"k0rdent Components","text":"<p>The main components of k0rdent include:</p> <ul> <li> <p>k0rdent Cluster Manager (kcm)</p> <p>Deployment and life-cycle management of Kubernetes clusters, including configuration, updates, and other CRUD operations.</p> </li> <li> <p>k0rdent State Manager (ksm)</p> <p>Installation and life-cycle management of beach-head services, policy, Kubernetes API configurations, and more.</p> </li> <li> <p>k0rdent Observability and FinOps (kof)</p> <p>Cluster and beach-head services monitoring, events and log management.</p> </li> </ul>"},{"location":"#structure-and-history","title":"Structure and History","text":"<p>The project has a number of components, including:</p> <ul> <li>k0rdent: the overall project<ul> <li>k0rdent Cluster Manager (kcm)</li> <li>k0rdent State Manager (ksm)<ul> <li>This is currently rolled into kcm, but may be split out in the future</li> <li>ksm leverages Project Sveltos   for an increasing amount of functionality</li> </ul> </li> <li>k0rdent Observability and FinOps (kof)</li> </ul> </li> </ul> <p>There are a few historical names that may show up in the code and older docs.</p> <ul> <li>Project 2A: the original codename of k0rdent (may occasionally show   up in some documentation)</li> <li>HMC or hmc: the original repository name for k0rdent and kcm   development (may occasionally show up in some documentation and code)</li> <li>motel: the original repository and codename for kof (may   occasionally show up in some documentation and code)</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>See the k0rdent Quick Start Guide.</p>"},{"location":"#supported-providers","title":"Supported Providers","text":"<p>k0rdent leverages the Cluster API provider ecosystem, the following providers have had <code>ProviderTemplates</code> created and validated, and more are in the works. </p> <ul> <li>AWS</li> <li>Azure</li> <li>vSphere</li> <li>OpenStack</li> </ul> <p>k0rdent also includes a way to add custom providers.</p>"},{"location":"#development-documentation","title":"Development Documentation","text":"<p>Documentation related to the development process and developer-specific notes is located in the main k0rdent repository.</p>"},{"location":"admin-adopting-clusters/","title":"Adopting an Existing Cluster","text":"<p>Creating a new cluster isn't the only way to use k0rdent. Adopting an existing Kubernetes cluster enables you to  bring it under the k0rdent's management. This process is useful when you already have a running cluster but want  to centralize management and leverage k0rdent's capabilities, such as unified monitoring, configuration, and automation.</p> <p>To adopt a cluster, k0rdent establishes communication between the management cluster (where kcm is installed)  and the target cluster. This requires proper credentials, network connectivity, and a standardized configuration. </p> <p>Follow these steps to adopt an existing cluster:</p> <ol> <li> <p>Prerequisites</p> <p>Before you start, make sure you have the following:</p> <ul> <li>A kubeconfig file for the cluster you want to adopt (this file provides access credentials and configuration details    for the cluster).</li> <li>A management cluster with k0rdent installed and running. See the installation instructions    if you need to set it up.</li> <li>Network connectivity between the management cluster and the cluster to be adopted (for example, ensure firewall    rules and VPNs allow communication).</li> </ul> </li> <li> <p>Create a Credential</p> <p>Start by creating a <code>Credential</code> object that includes all required authentication details for your chosen infrastructure  provider. Follow the instructions in the Credential System, as well as the specific instructions  for your target infrastructure.</p> <p>Tip</p> <p>Double-check that your credentials have sufficient permissions to create resources on the target infrastructure.</p> </li> <li> <p>Configure the Adopted Cluster Template</p> <p>Set the <code>KUBECONFIG</code> environment variable to the path of your management cluster's kubeconfig file so you can  execute commands against the management cluster.</p> <p>For example:</p> <pre><code>export KUBECONFIG=/path/to/management-cluster-kubeconfig\n</code></pre> </li> <li> <p>Create the <code>ClusterDeployment</code> YAML Configuration</p> <p>The <code>ClusterDeployment</code> object is used to define how k0rdent should manage the adopted cluster. Create a  YAML file for the <code>ClusterDeployment</code> object, as shown below:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;CLUSTER_NAME&gt;\n  namespace: &lt;NAMESPACE&gt;\nspec:\n  template: adopted-cluster-&lt;VERSION&gt;\n  credential: &lt;CREDENTIAL_NAME&gt;\n  dryRun: &lt;BOOLEAN&gt;\n  config:\n    &lt;CONFIGURATION&gt;\n</code></pre> <p>Replace placeholders like <code>&lt;CLUSTER_NAME&gt;</code>, <code>&lt;NAMESPACE&gt;</code>, <code>&lt;VERSION&gt;</code>, <code>&lt;CREDENTIAL_NAME&gt;</code>, and <code>&lt;CONFIGURATION&gt;</code> with actual values. The <code>dryRun</code> flag is useful for testing the configuration without making changes to the cluster. For more details, see the Dry Run section.</p> <p>You can also get a list of the available templates with:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n\nPutting it all together, your YAML would look something like this:\n\n```yaml\napiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster\n  namespace: kcm-system\nspec:\n  template: adopted-cluster-0-0-2\n  credential: my-cluster-credential\n  dryRun: false\n  config: {}\n</code></pre></p> </li> <li> <p>Apply the <code>ClusterDeployment</code> configuration</p> <p>Once your configuration file is ready, apply it to the management cluster using <code>kubectl</code>:</p> <pre><code>kubectl apply -f clusterdeployment.yaml\n</code></pre> <p>This step submits the <code>ClusterDeployment</code> object to k0rdent, initiating the adoption process.</p> </li> <li> <p>Check the Status of the <code>ClusterDeployment</code> Object</p> <p>To ensure the adoption process is progressing as expected, check the status of the <code>ClusterDeployment</code> object:</p> <pre><code>kubectl -n &lt;namespace&gt; get clusterdeployment.kcm &lt;cluster-name&gt; -o=yaml\n</code></pre> <p>The output includes the current state and any conditions (for example, errors or progress updates). Review  this information to confirm that the adoption is successful.</p> </li> </ol>"},{"location":"admin-adopting-clusters/#whats-happening-behind-the-scenes","title":"What's Happening Behind the Scenes?","text":"<p>When you adopt a cluster, k0rdent performs several actions: 1. It validates the credentials and configuration provided in the <code>ClusterDeployment</code> object. 2. It ensures network connectivity between the management cluster and the adopted cluster. 3. It registers the adopted cluster within the k0rdent system, enabling it to be monitored and managed like     any k0rdent-deployed cluster.</p> <p>This process doesn't change the adopted cluster's existing workloads or configurations. Instead, it enhances your  ability to manage the cluster through k0rdent.</p>"},{"location":"admin-adopting-clusters/#additional-tips","title":"Additional Tips","text":"<ul> <li>If you encounter issues, double-check that kubeconfig file you used for the adopted cluster is valid    and matches the cluster you're trying to adopt.</li> <li>Use the <code>dryRun</code> option during the first attempt to validate the configuration without making actual changes.</li> </ul>"},{"location":"admin-backup/","title":"Backing up and Restoring a k0rdent Management Cluster","text":"<p>Any production system needs to provide Disaster Recovery features, and the heart of these capabilities is the ability to perform backup and restore operations. In this chapter we'll look at backing up and restoring k0rdent management cluster so that in case of an emergency, you can restore your system to its previous condition.</p> <p>While it's possible to back up a Kubernetes cluster manually, it's better to build on the work of others. In this case we're going to leverage the <code>velero</code> project for backup management on the backend and see how it integrates with k0rdent to ensure data persistence and recovery.</p>"},{"location":"admin-backup/#motivation","title":"Motivation","text":"<p>The primary goal of this feature is to provide a reliable and efficient way to back up and restore a k0rdent deployment in the event of a disaster that impacts the management cluster. By using <code>velero</code> as the backup provider, we can create consistent backups across different cloud storage while maintaining the integrity of critical resources.</p> <p>The main goal of the feature is to provide:</p> <ul> <li>Management Backup: The ability to backup all configuration objects created and managed by k0rdent, including   into an offsite location.</li> <li>Restore: The ability to create configuration objects from a specific Management Backup in order to create a management   cluster in the same state that existed at the time of backup without (re)provisioning of cloud resources.</li> <li>Disaster Recovery: The ability to restore k0rdent on another management cluster   using restore capability, plus ensuring that clusters are not recreated or lost.</li> <li>Rollback: the possibility to manually restore after a specific event, such as a failed k0rdent upgrade</li> </ul>"},{"location":"admin-backup/#velero-as-provider-for-management-backups","title":"Velero as Provider for Management Backups","text":"<p><code>Velero</code> is an open-source tool that simplifies backing up and restoring clusters as well as individual resources. It seamlessly integrates into k0rdent management environment to provide robust disaster recovery capabilities.</p> <p>The <code>velero</code> instance is part of the Helm chart that installs k0rdent, which means that it can be customized if necessary. (You can find information on customizing Velero below)</p> <p>k0rdent manages the schedule and is responsible for collecting data to be included in a backup.</p>"},{"location":"admin-backup/#scheduled-management-backups","title":"Scheduled Management Backups","text":"<p>Backups should be scheduled on a regular basis, depending on how often information changes.</p>"},{"location":"admin-backup/#preparation","title":"Preparation","text":"<p>Before you create a scheduled backup, you need to perform a few preparatory steps:</p> <ol> <li> <p>If no <code>velero</code> plugins have been installed as suggested      in the corresponding section,      install it by modifying the <code>Management</code> object:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  # ... \n  core:\n    kcm:\n      config:\n        velero:\n          initContainers:\n          - name: velero-plugin-for-&lt;provider-name&gt;\n            image: velero/velero-plugin-for-&lt;provider-name&gt;:&lt;provider-plugin-tag&gt;\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - mountPath: /target\n              name: plugins\n  # ...\n</code></pre> </li> <li> <p>Prepare a cloud storage location, such as an Amazon S3 bucket, to which to save backups.</p> </li> <li> <p>Create a <code>BackupStorageLocation</code>     object referencing a <code>Secret</code> with credentials to access the cloud storage     (if the multiple credentials feature is supported by the plugin).</p> <p>For example, if you are using Amazon S3, your <code>BackupStorageLocation</code> and the related <code>Secret</code> might look like this:</p> <pre><code>---\napiVersion: v1\ndata:\n  # base64-encoded credentials for Amazon S3 in the following format:\n  # [default]\n  # aws_access_key_id = EXAMPLE_ACCESS_KEY_ID\n  # aws_secret_access_key = EXAMPLE_SECRET_ACCESS_KEY\n       cloud: W2RlZmF1bHRdCmF3c19hY2Nlc3Nfa2V5X2lkID0gRVhBTVBMRV9BQ0NFU1NfS0VZX0lECmF3c19zZWNyZXRfYWNjZXNzX2tleSA9IEVYQU1QTEVfU0VDUkVUX0FDQ0VTU19LRVkKICA=\nkind: Secret\nmetadata:\n  name: cloud-credentials\n  namespace: kcm-system\ntype: Opaque\n---\napiVersion: velero.io/v1\nkind: BackupStorageLocation\nmetadata:\n  name: aws-s3\n  namespace: kcm-system\nspec:\n  config:\n    region: &lt;your-region-name&gt;\n  default: true # optional, if not set, then storage location name must always be set in ManagementBackup\n  objectStorage:\n    bucket: &lt;your-bucket-name&gt;\n  provider: aws\n  backupSyncPeriod: 1m\n  credential:\n    name: cloud-credentials\n    key: cloud\n</code></pre> </li> </ol> <p>You can get more information how to build these objects at the official Velero documentation.</p>"},{"location":"admin-backup/#create-a-management-backup","title":"Create a Management Backup","text":"<p>Periodic backups are handled by a <code>ManagementBackup</code> object, which uses a Cron expression for its <code>.spec.schedule</code> field.  If you don't set the <code>.spec.schedule</code> field, Velero will instead work with  backup on demand.</p> <p>Optionally, set the name of the <code>BackupStorageLocation</code> <code>.spec.backup.storageLocation</code>. The default location is the <code>BackupStorageLocation</code> object with <code>.spec.default</code> set to <code>true</code>.</p> <p>For example, you can create a <code>ManagementBackup</code> object that backs up to the storage object created in the previous step every 6 minutes would look like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ManagementBackup\nmetadata:\n  name: kcm\nspec:\n  schedule: \"0 */6 * * *\"\n  storageLocation: aws-s3\n</code></pre>"},{"location":"admin-backup/#management-backup-on-demand","title":"Management Backup on Demand","text":"<p>To create a single backup of the existing k0rdent management cluster information, you can create a <code>ManagementBackup</code> object using a YAML document and the <code>kubectl</code> CLI. The object then creates only one instance of a backup. For example you can backup to the location created earlier:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ManagementBackup\nmetadata:\n  name: example-backup\nspec:\n  storageLocation: aws-s3\n</code></pre>"},{"location":"admin-backup/#whats-included-in-the-management-backup","title":"What's Included in the Management Backup","text":"<p>The backup includes all of k0rdent <code>kcm</code> component resources, parts of the <code>cert-manager</code> components required for other components creation, and all the required resources of <code>CAPI</code> and the <code>ClusterDeployment</code>s currently in use in the management cluster.</p> <p>Example</p> <p>An example set of labels, and objects satisfying these labels will be included in the backup:</p> <pre><code>cluster.x-k8s.io/cluster-name=\"cluster-deployment-name\"\ncluster.x-k8s.io/provider=\"bootstrap-k0sproject-k0smotron\"\ncluster.x-k8s.io/provider=\"cluster-api\"\ncluster.x-k8s.io/provider=\"control-plane-k0sproject-k0smotron\"\ncluster.x-k8s.io/provider=\"infrastructure-aws\"\ncontroller.cert-manager.io/fao=\"true\"\nhelm.toolkit.fluxcd.io/name=\"cluster-deployment-name\"\nk0rdent.mirantis.com/component=\"kcm\"\n</code></pre>"},{"location":"admin-backup/#restoration","title":"Restoration","text":"<p>Note</p> <p>Please refer to the official migration documentation to familiarize yourself with potential limitations of the Velero backup system.</p> <p>In the event of disaster, you can restore from a backup by doing the following:</p> <ol> <li> <p>Create a clean k0rdent installation, including <code>velero</code> and its plugins. Specifically, you want to avoid creating a <code>Management</code> object    and similar objects because they will be part of your restored cluster. You can remove these objects after    installation, but you can also install k0rdent without them in the first place:</p> <pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm \\\n --version &lt;version&gt; \\\n --create-namespace \\\n --namespace kcm-system \\\n --set controller.createManagement=false \\\n --set controller.createAccessManagement=false \\\n --set controller.createRelease=false \\\n --set controller.createTemplates=false \\\n --set velero.initContainers[0].name=velero-plugin-for-&lt;provider-name&gt; \\\n --set velero.initContainers[0].image=velero/velero-plugin-for-&lt;provider-name&gt;:&lt;provider-plugin-tag&gt; \\\n --set velero.initContainers[0].volumeMounts[0].mountPath=/target \\\n --set velero.initContainers[0].volumeMounts[0].name=plugins\n</code></pre> </li> <li> <p>Create the <code>BackupStorageLocation</code>/<code>Secret</code> objects that were    created during the preparation stage of creating a backup (preferably the same depending on a plugin).</p> </li> <li> <p>Restore the <code>kcm</code> system creating the <code>Restore</code> object,    it is important to set the <code>.spec.existingResourcePolicy</code> field value to <code>update</code>:</p> <pre><code> apiVersion: velero.io/v1\n kind: Restore\n metadata:\n   name: &lt;restore-name&gt;\n   namespace: kcm-system\n spec:\n   backupName: &lt;backup-name&gt;\n   existingResourcePolicy: update\n   includedNamespaces:\n   - '*'\n</code></pre> </li> <li> <p>Wait until the <code>Restore</code> status is <code>Completed</code> and all <code>kcm</code> components are up and running.</p> </li> </ol>"},{"location":"admin-backup/#caveats","title":"Caveats","text":"<p>For some CAPI providers it is necessary to make changes to the <code>Restore</code> object due to the large number of different resources and logic in each provider. The resources described below are not excluded from a <code>ManagementBackup</code> by default to avoid logical dependencies on one or another provider and continue to be provider-agnostic.</p> <p>Note</p> <p> The described caveats apply only to the step with the <code>Restore</code> object creation and do not affect the other steps.</p>"},{"location":"admin-backup/#azure-capz","title":"Azure (CAPZ)","text":"<p>The following resources should be excluded from the <code>Restore</code> object:</p> <ul> <li><code>natgateways.network.azure.com</code></li> <li><code>resourcegroups.resources.azure.com</code></li> <li><code>virtualnetworks.network.azure.com</code></li> <li><code>virtualnetworkssubnets.network.azure.com</code></li> </ul> <p>Due to the webhook conversion, objects of these resources cannot be restored, and they will be created in the management cluster by the <code>CAPZ</code> provider automatically with the same <code>spec</code> as in the backup.</p> <p>The resulting <code>Restore</code> object:</p> <pre><code> apiVersion: velero.io/v1\n kind: Restore\n metadata:\n   name: &lt;restore-name&gt;\n   namespace: kcm-system\n spec:\n   backupName: &lt;backup-name&gt;\n   existingResourcePolicy: update\n   excludedResources:\n   - natgateways.network.azure.com\n   - resourcegroups.resources.azure.com\n   - virtualnetworks.network.azure.com\n   - virtualnetworkssubnets.network.azure.com\n   includedNamespaces:\n   - '*'\n</code></pre>"},{"location":"admin-backup/#vsphere-capv","title":"vSphere (CAPV)","text":"<p>The following resources should be excluded from the <code>Restore</code> object:</p> <ul> <li><code>mutatingwebhookconfiguration.admissionregistration.k8s.io</code></li> <li><code>validatingwebhookconfiguration.admissionregistration.k8s.io</code></li> </ul> <p>Due to the Velero Restoration Order, some of the <code>CAPV</code> core objects cannot be restored, and they will not be restored automatically. Since all of the objects have already passed both mutations and validations, there is not much sense in validating them again. The webhook configurations will be restored during installation of the <code>CAPV</code> provider.</p> <p>The resulting <code>Restore</code> object:</p> <pre><code> apiVersion: velero.io/v1\n kind: Restore\n metadata:\n   name: &lt;restore-name&gt;\n   namespace: kcm-system\n spec:\n   backupName: &lt;backup-name&gt;\n   existingResourcePolicy: update\n   excludedResources:\n   - mutatingwebhookconfiguration.admissionregistration.k8s.io\n   - validatingwebhookconfiguration.admissionregistration.k8s.io\n   includedNamespaces:\n   - '*'\n</code></pre>"},{"location":"admin-backup/#upgrades-and-rollbacks","title":"Upgrades and rollbacks","text":"<p>The Disaster Recovery Feature provides a way to create backups on each <code>kcm</code> upgrade automatically.</p>"},{"location":"admin-backup/#automatic-management-backups","title":"Automatic Management Backups","text":"<p>Each <code>ManagementBackup</code> with non-empty <code>.spec.schedule</code> field can enable the automatic creation of backups before upgrading to a new version.</p> <p>To enable, set the <code>.spec.performOnManagementUpgrade</code> to <code>true</code>.</p> <p>Example</p> <p>An example of a <code>ManagementBackup</code> object with enabled auto-backup before the <code>kcm</code> version upgrade:</p> <pre><code> apiVersion: k0rdent.mirantis.com/v1alpha1\n kind: ManagementBackup\n metadata:\n   name: example-backup\n spec:\n   schedule: \"0 */6 * * *\"\n   performOnManagementUpgrade: true\n</code></pre> <p>After the enablement, before each upgrade of <code>kcm</code> to a new version, a new backup will be created.</p> <p>Automatically created backups have the following name template to make it easier to find them: the name of the <code>ManagementBackup</code> object with enabled <code>performOnManagementUpgrade</code> concatenates with the name of the release before the upgrade, e.g. <code>example-backup-kcm-0-1-0</code>.</p> <p>Automatically created backups have the following label <code>k0rdent.mirantis.com/release-backup</code> with the name of the release before the upgrade as its value to simplify querying if required.</p>"},{"location":"admin-backup/#rollbacks","title":"Rollbacks","text":"<p>If during the <code>kcm</code> upgrade a failure happens, a rollback operation should be performed to restore the <code>kcm</code> to its before-the-upgrade state:</p> <ol> <li> <p>Follow the first 2 steps from the restoration section,    creating a clean <code>kcm</code> installation and <code>BackupStorageLocation</code>/<code>Secret</code>. </p> <p>Warning</p> <p> Please consider the restoration caveats section before proceeding.</p> </li> <li> <p>Create the <code>ConfigMap</code> object with patches to revert the <code>Management</code> <code>.spec.release</code>, substitute the <code>&lt;version-before-upgrade&gt;</code> with     the version of <code>kcm</code> before the upgrade, and create the <code>Restore</code> object     propagating the <code>ConfigMap</code> to it:</p> <pre><code>---\napiVersion: v1\ndata:\n  patch-mgmt-spec-release: |\n    version: v1\n    resourceModifierRules:\n    - conditions:\n        groupResource: managements.k0rdent.mirantis.com\n      patches:\n      - operation: replace\n        path: \"/spec/release\"\n        value: \"&lt;version-before-upgrade&gt;\"\nkind: ConfigMap\nmetadata:\n  name: patch-mgmt-spec-release\n  namespace: kcm-system\n---\napiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: &lt;restore-name&gt;\n  namespace: kcm-system\nspec:\n  backupName: &lt;backup-name&gt;\n  existingResourcePolicy: update\n  includedNamespaces:\n  - '*'\n  resourceModifier: # propagate patches\n    kind: ConfigMap\n    name: patch-mgmt-spec-release\n</code></pre> </li> <li> <p>Wait until the <code>Restore</code> status is <code>Completed</code> and all <code>kcm</code> components are up and running.</p> </li> <li>Optionally delete the created <code>ConfigMap</code>.</li> </ol>"},{"location":"admin-backup/#caveats-limitations","title":"Caveats / Limitations","text":"<p>The credentials stored in backups might and will get stale, so a proper rotation should be considered beforehand.</p> <p>All <code>velero</code> caveats and limitations are transitively implied in the <code>k0rdent</code>.</p> <p>In particular, that means no backup encryption is provided until it is implemented by a <code>velero</code> plugin that supports encryption and cloud storage backups.</p>"},{"location":"admin-backup/#velero-backups-restores-deletion","title":"Velero Backups / Restores deletion","text":""},{"location":"admin-backup/#delete-restores","title":"Delete Restores","text":"<p>To delete <code>velero</code> <code>Restore</code> from the management cluster and from the cloud storage, delete <code>restores.velero.io</code> object(s), e.g. with the following command:</p> <pre><code>kubectl delete restores.velero.io -n kcm-system &lt;restore-name&gt;\n</code></pre> <p>Warning</p> <p> Deletion of a <code>Restore</code> object deletes it from both the management cluster and from the cloud storage.</p>"},{"location":"admin-backup/#delete-backups","title":"Delete Backups","text":"<p>To remove <code>velero</code> <code>Backup</code> from the management cluster, delete <code>backups.velero.io</code> object(s), e.g. with the following command:</p> <pre><code>kubectl delete backups.velero.io -n kcm-system &lt;velero-backup-name&gt;\n</code></pre> <p>Hint</p> <p> The command above only removes objects from the cluster, the data continues to persist on the cloud storage.</p> <p>The deleted object will be recreated in the cluster if its <code>BackupStorageLocation</code> <code>.spec.backupSyncPeriod</code> is set and does not equal <code>0</code>.</p> <p>To delete <code>velero</code> <code>Backup</code> from the management cluster and from the cloud storage, create the following <code>DeleteBackupRequest</code> object:</p> <pre><code>apiVersion: velero.io/v1\nkind: DeleteBackupRequest\nmetadata:\n  name: delete-backup-completely\n  namespace: kcm-system\nspec:\n  backupName: &lt;velero-backup&gt;\n</code></pre> <p>Warning</p> <p> Deletion of a <code>Backup</code> object via the <code>DeleteBackupRequest</code> deletes it from both the management cluster and from the cloud storage.</p> <p>Optionally, delete the created <code>DeleteBackupRequest</code> object from the cluster after <code>Backup</code> has been deleted.</p> <p>For reference, follow the official documentation.</p>"},{"location":"admin-backup/#customization","title":"Customization","text":"<p>This section covers different topics of customization regarding backing up and restoring k0rdent.</p>"},{"location":"admin-backup/#velero-installation","title":"Velero installation","text":"<p>The Velero helm chart is supplied with the k0rdent helm chart and is enabled by default. There are 2 ways of customizing the chart values:</p> <ol> <li> <p>Install using <code>helm</code> and add corresponding parameters to the <code>helm install</code> command.</p> <p>Note</p> <p> Only a plugin is required during the restoration, the other parameters are optional to be set.</p> <p>Example</p> <p>An example of <code>helm install</code> with a configured plugin, <code>BackupStorageLocation</code> and propagated credentials: <pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm \\\n --version &lt;version&gt; \\\n --create-namespace \\\n --namespace kcm-system \\\n --set-file velero.credentials.secretContents.cloud=&lt;FULL PATH TO FILE&gt; \\\n --set velero.credentials.useSecret=true \\\n --set velero.backupsEnabled=true \\\n --set velero.configuration.backupStorageLocation[0].name=&lt;backup-storage-location-name&gt; \\\n --set velero.configuration.backupStorageLocation[0].provider=&lt;provider-name&gt; \\\n --set velero.configuration.backupStorageLocation[0].bucket=&lt;bucket-name&gt; \\\n --set velero.configuration.backupStorageLocation[0].config.region=&lt;region&gt; \\\n --set velero.initContainers[0].name=velero-plugin-for-&lt;provider-name&gt; \\\n --set velero.initContainers[0].image=velero/velero-plugin-for-&lt;provider-name&gt;:&lt;provider-plugin-tag&gt; \\\n --set velero.initContainers[0].volumeMounts[0].mountPath=/target \\\n --set velero.initContainers[0].volumeMounts[0].name=plugins\n</code></pre></p> </li> <li> <p>Create or modify the existing <code>Management</code> object in the <code>.spec.config.kcm</code>.     &gt; NOTE:     &gt; Only a plugin is required during the restoration, the other parameters     &gt; are optional to be set.</p> <p>Example</p> <p>An example of a <code>Management</code> object with a configured plugin and enabled metrics: <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  # ...\n  core:\n    kcm:\n      config:\n        velero:\n          initContainers:\n          - name: velero-plugin-for-&lt;provider-name&gt;\n            image: velero/velero-plugin-for-&lt;provider-name&gt;:&lt;provider-plugin-tag&gt;\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - mountPath: /target\n              name: plugins\n          metrics:\n            enabled: true\n  # ...\n</code></pre></p> </li> </ol> <p>To fully disable <code>velero</code>, set the <code>velero.enabled</code> parameter to <code>false</code>.</p>"},{"location":"admin-backup/#schedule-expression-format","title":"Schedule expression format","text":"<p>The <code>ManagementBackup</code> <code>.spec.schedule</code> field accepts a correct Cron expression, along with the nonstandard predefined scheduling definitions and an extra definition <code>@every</code> with a number and a valid time unit (valid time units are <code>ns, us (or \u00b5s), ms, s, m, h</code>).</p> <p>The following list contains <code>.spec.schedule</code> acceptable example values:</p> <ul> <li><code>0 */1 * * *</code> (standard Cron expression)</li> <li><code>@hourly</code> (nonstandard predefined definition)</li> <li><code>@every 1h</code> (extra definition)</li> </ul>"},{"location":"admin-backup/#putting-extra-objects-in-a-management-backup","title":"Putting extra objects in a Management Backup","text":"<p>If you need to back up objects other than those backed up by default, you can add the label <code>k0rdent.mirantis.com/component=\"kcm\"</code> to these objects.</p> <p>All objects containing the label will be automatically added to the management backup.</p>"},{"location":"admin-before/","title":"Before you start","text":"<p>Before you start working with k0rdent, it helps to understand a few basics.</p>"},{"location":"admin-before/#how-k0rdent-works","title":"How k0rdent works","text":"<p>k0rdent has several important subsystems, notably:</p> <ul> <li>KCM - k0rdent Cluster Manager - KCM wraps and manages Kubernetes Cluster API, and lets you treat clusters as  Kubernetes objects. Within a k0rdent management cluster, you'll have a <code>ClusterDeployment</code> object that  represents a deployed cluster, with <code>Machine</code> objects, and so on. When you create a <code>ClusterDeployment</code>,  k0rdent deploys the cluster. When you delete it, k0rdent deletes it, and so on.</li> <li>KSM - k0rdent Service Manager - KSM wraps and manages several interoperating open source projects like Helm and Sveltos, which let you treat services and applications as Kubernetes objects.</li> </ul> <p>Together, KCM and KSM interoperate to manifest a complete, template-driven system for defining and managing complete Internal Development Platforms (IDPs) comprising suites of services, plus a cluster and its components as realized on a particular cloud or infrastructure substrate. </p> <ul> <li> <p>ClusterAPI providers: ClusterAPI uses <code>providers</code> to manage different clouds and infrastructures, including bare metal. k0rdent ships with providers for AWS, Azure, OpenStack and vSphere, and you can add additional providers in order to control other clouds or infrastructures that ClusterAPI supports.</p> </li> <li> <p>Templates: When you create a cluster, that cluster is based on a template, which specifies all of the various information about the cluster, such as where to find images, and so on. These templates get installed into k0rdent, but they don't do  anything until you reference them in a <code>ClusterDeployment</code> that represents an actual cluster.</p> </li> </ul> <p></p> <p>k0rdent can also manage these clusters, upgrading, scaling them, or installing software and services.</p> <ul> <li>Services: To add (or manage) services, you also use templates. These <code>ServiceTemplate</code>s are like <code>ClusterTemplate</code>s, in that you install them into the cluster, but until they're actually referenced, they don't do anything. When you reference a <code>ServiceTemplate</code> as part of a <code>ClusterDeployment</code>, k0rdent knows to install that service into that cluster.</li> </ul> <p></p> <p>These services can be actual services, such as Nginx or Kyverno, or they can be user applications.</p>"},{"location":"admin-before/#how-credentials-work","title":"How Credentials work","text":"<p>Of course you can't do any of this without permissions. As a human, you can log into, say, AWS, and tell it to create a new instance on which you are going to install Kubernetes, but how does k0rdent get that permission? It gets it through the use of  <code>Credential</code>s. </p> <p>When you create a <code>ClusterDeployment</code> or deploy an application, you include a reference to a <code>Credential</code> object that has been installed in the k0rdent management cluster. Depending on whether the target infrastructure is AWS, Azure, or something else, that <code>Credential</code> might reference an access key and secret, or it might reference a service provider, but all of that gets abstracted out by the time you get to the <code>Credential</code>, which is what you'll actually reference.</p> <p></p> <p>By abstracting everything out to create a standard <code>Credential</code> object, users never have to have access to actual credentials (lowercase \"c\"). This enables the administrator to keep those credentials private, and to rotate them as necessary without disturbing users or their applications. The administrator simply updates the <code>Credential</code> object and everything continues to work.</p> <p>You can find more information on creating these <code>Credential</code>s in the Credentials chapter.</p>"},{"location":"admin-before/#k0rdent-and-gitops","title":"k0rdent and GitOps","text":"<p>At its heart, k0rdent is a Kubernetes-native way to declaratively specify what should be happening in the infrastructure and have that maintained. In other words, if you want to, say, scale up a cluster, you would give that cluster a new definition that includes the additional nodes, and then k0rdent, seeing that reality doesn't match that definition,  will make it happen.</p> <p>In some ways that is very similar to GitOps, in which you commit definitions and tools such as Flux or ArgoCD  ensure that reality matches the definition. We can say that k0rdent is GitOps-compatible, in the sense that you can (and should) consider storing k0rdent templates and YAML object definitions in Git repos, and can (and may want to) use GitOps tools like ArgoCD to modify and manage them upstream of k0rdent itself.</p> <p>The main difference is that k0rdent's way of representing clusters and services is fully compliant with Kubernetes-native tools like ClusterAPI, Sveltos and Helm. So you can, in fact, port much of what you do with k0rdent templates and objects directly to other solution environments that leverage these standard tools.</p>"},{"location":"admin-before/#k0rdent-initialization-process","title":"k0rdent initialization process","text":""},{"location":"admin-before/#the-process","title":"The process","text":"<p>The k0rdent initialization process involves tools such as Helm and FluxCD.</p> <ol> <li>helm install kcm brings up the bootstrap components (yellow on the picture above)</li> <li>kcm-controller-manager sets up webhooks to validate its <code>CustomResource</code>s, then cert-manager handles the webhooks\u2019 certificates</li> <li>kcm-controller-manager generates <code>Release</code> object corresponding to the kcm helm chart version</li> <li>kcm-controller-manager (or rather the release-controller inside it) generates template objects (<code>ProviderTemplate</code>/<code>ClusterTemplate</code>/<code>ServiceTemplate</code>) corresponding to a <code>Release</code> to be further processed</li> <li>kcm-controller-manager generates a <code>HelmRelease</code> object for every template from p.3 (Important: it includes also kcm helm chart itself)</li> <li>Flux (source-controller and helm-controller pods) reconciles the HelmRelease objects. In other words, it installs all the helm charts referred to in the templates. After this point, the deployment is completely controlled by Flux.</li> <li>kcm-controller-manager creates a <code>Management</code> object that refers to the above <code>Release</code> and the <code>ProviderTemplate</code> objects. The <code>Management</code> object represents the k0rdent management cluster as a whole. The management cluster Day-2 operations (such as upgrade) are  executed by manipulating the <code>Release</code> and <code>Management</code> objects.</li> <li>kcm-controller-manager generates an empty <code>AccessManagement</code> object. <code>AccessManagement</code> defines access rules for <code>ClusterTemplate</code>/<code>ServiceTemplate</code> propagation across user namespaces. Further <code>AccessManagement</code> might be edited and used along with admin-created <code>ClusterTemplateChain</code> and <code>ServiceTemplateChain</code> objects.</li> </ol>"},{"location":"admin-create-multiclusterservice/","title":"Deploy beach-head services using MultiClusterService","text":"<p>The <code>MultiClusterService</code> object is used to deploy beach-head services on multiple matching clusters.</p>"},{"location":"admin-create-multiclusterservice/#creation","title":"Creation","text":"<p>The <code>MultiClusterService</code> object can be created with the following YAML: <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: MultiClusterService\nmetadata:\n  name: &lt;name&gt;\nspec:\n  clusterSelector:\n    matchLabels:\n      &lt;key1&gt;: &lt;value1&gt;\n      &lt;key2&gt;: &lt;value2&gt;\n      . . .\n serviceSpec:\n    services:\n    - template: &lt;servicetemplate-1-name&gt;\n      name: &lt;release-name&gt;\n      namespace: &lt;release-namespace&gt;\n    priority: 100\n</code></pre></p>"},{"location":"admin-create-multiclusterservice/#matching-multiple-clusters","title":"Matching Multiple Clusters","text":"<p>Consider the following example where 2 clusters have been deployed using ClusterDeployment objects:</p> <p>Command:</p> <pre><code>kubectl get clusterdeployments.k0rdent.mirantis.com -n kcm-system\n</code></pre> <p>Output: <pre><code>NAME             READY   STATUS\ndev-cluster-1   True    ClusterDeployment is ready\ndev-cluster-2   True    ClusterDeployment is ready\n</code></pre></p> <p>Command: <pre><code> kubectl get cluster -n kcm-system --show-labels\n</code></pre> Output: <pre><code>NAME           CLUSTERCLASS     PHASE         AGE     VERSION   LABELS\ndev-cluster-1                  Provisioned   2h41m             app.kubernetes.io/managed-by=Helm,helm.toolkit.fluxcd.io/name=dev-cluster-1,helm.toolkit.fluxcd.io/namespace=kcm-system,sveltos-agent=present\ndev-cluster-2                  Provisioned   3h10m             app.kubernetes.io/managed-by=Helm,helm.toolkit.fluxcd.io/name=dev-cluster-2,helm.toolkit.fluxcd.io/namespace=kcm-system,sveltos-agent=present\n</code></pre> </p> <p>Example</p> <p>Spec for <code>dev-cluster-1</code> ClusterDeployment (only sections relevant to beach-head services): <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: dev-cluster-1\n  namespace: kcm-system\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: kyverno\n      namespace: kyverno\n      template: kyverno-3-2-6\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-0\n    priority: 100\n  . . .\n</code></pre></p> <p>Spec for <code>dev-cluster-2</code> ClusterDeployment (only sections relevant to beach-head services): <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: dev-cluster-2\n  namespace: kcm-system\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-0\n    priority: 500\n  . . .\n</code></pre></p> <p>Note</p> <p> See Deploy beach-head Services using Cluster Deployment for how to use beach-head services with ClusterDeployment. Now the following <code>global-ingress</code> MultiClusterService object is created with the following spec: <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: MultiClusterService\nmetadata:\n  name: global-ingress\nspec:\n  clusterSelector:\n    matchLabels:\n      app.kubernetes.io/managed-by: Helm\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-3\n    priority: 300\n</code></pre></p> <p>This MultiClusterService will match any CAPI cluster with the label <code>app.kubernetes.io/managed-by: Helm</code> and deploy chart version 4.11.3 of ingress-nginx service on it.</p>"},{"location":"admin-create-multiclusterservice/#configuring-custom-values","title":"Configuring Custom Values","text":"<p>Refer to \"Configuring Custom Values\" in Deploy beach-head Services using Cluster Deployment.</p>"},{"location":"admin-create-multiclusterservice/#templating-custom-values","title":"Templating Custom Values","text":"<p>Refer to \"Templating Custom Values\" in Deploy beach-head Services using Cluster Deployment.</p>"},{"location":"admin-create-multiclusterservice/#services-priority-and-conflict","title":"Services Priority and Conflict","text":"<p>The <code>.spec.serviceSpec.priority</code> field is used to specify the priority for the services managed by a ClusterDeployment or MultiClusterService object. Considering the example above:</p> <ol> <li>ClusterDeployment <code>dev-cluster-1</code> manages deployment of kyverno (v3.2.6) and ingress-nginx (v4.11.0) with <code>priority=100</code> on its cluster.</li> <li>ClusterDeployment <code>dev-cluster-2</code> manages deployment of ingress-nginx (v4.11.0) with <code>priority=500</code> on its cluster.</li> <li>MultiClusterService <code>global-ingress</code> manages deployment of ingress-nginx (v4.11.3) with <code>priority=300</code> on both clusters. This scenario presents a conflict on both the clusters as the MultiClusterService is attempting to deploy v4.11.3 of ingress-nginx on both whereas the ClusterDeployment for each is attempting to deploy v4.11.0 of ingress-nginx.</li> </ol> <p>This is where <code>.spec.serviceSpec.priority</code> can be used to specify who gets the priority. Higher number means higer priority and vice versa. In this example: 1. MultiClusterService \"global-ingress\" will take precedence over ClusterDeployment \"dev-cluster-1\" and ingress-nginx (v4.11.3) defined in MultiClusterService object will be deployed on the cluster. 2. ClusterDeployment \"dev-cluster-2\" will take precedence over MultiClusterService \"global-ingress\" and ingress-nginx (v4.11.0) defined in ClusterDeployment object will be deployed on the cluster.</p> <p>Note</p> <p>If <code>priority</code> are equal, the first one to reach the cluster wins and deploys its beach-head services.</p>"},{"location":"admin-create-multiclusterservice/#checking-status","title":"Checking Status","text":"<p>The status for the MultiClusterService object will show the deployment status for the beach-head services managed by it on each of the CAPI target clusters that it matches. Consider the same example where 2 ClusterDeployments and 1 MultiClusterService is deployed.</p> <p>Example</p> <p>Status for <code>global-ingress</code> MultiClusterService <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: MultiClusterService\nmetadata:\n  . . .\n  name: global-ingress\n  resourceVersion: \"38146\"\n  . . .\nspec:\n  clusterSelector:\n    matchLabels:\n      app.kubernetes.io/managed-by: Helm\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-3\n    . . .\n  . . .\nstatus:\n  conditions:\n  - lastTransitionTime: \"2024-10-25T08:36:24Z\"\n    message: \"\"\n    reason: Succeeded\n    status: \"True\"\n    type: SveltosClusterProfileReady\n  - lastTransitionTime: \"2024-10-25T08:36:24Z\"\n    message: MultiClusterService is ready\n    reason: Succeeded\n    status: \"True\"\n    type: Ready\n  observedGeneration: 1\n  services:\n  - clusterName: dev-cluster-2\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:36:35Z\"\n      message: |\n        cannot manage chart ingress-nginx/ingress-nginx. ClusterSummary p--dev-cluster-2-capi-dev-cluster-2 managing it.\n      reason: Failed\n      status: \"False\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T08:36:25Z\"\n      message: 'Release ingress-nginx/ingress-nginx: ClusterSummary p--dev-cluster-2-capi-dev-cluster-2\n        managing it'\n      reason: Conflict\n      status: \"False\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n  - clusterName: dev-cluster-1\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:36:24Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T08:36:25Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre></p> <p>The status under <code>.status.services</code> shows a conflict for <code>dev-cluster-2</code> as expected because the MultiClusterService has a lower priority. Whereas, it shows provisioned for <code>dev-cluster-1</code> because the MultiClusterService has a higher priority.</p> <p>Example</p> <p>Status for <code>dev-cluster-1</code> ClusterDeployment (only sections relevant to beach-head services): <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  . . . \n  name: dev-cluster-1\n  namespace: kcm-system\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: kyverno\n      namespace: kyverno\n      template: kyverno-3-2-6\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-0\n    priority: 100\n    . . .\n  . . .\nstatus:\n  . . .\n  services:\n  - clusterName: dev-cluster-1\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:36:35Z\"\n      message: |\n        cannot manage chart ingress-nginx/ingress-nginx. ClusterSummary global-ingress-capi-dev-cluster-1 managing it.\n      reason: Provisioning\n      status: \"False\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T07:44:43Z\"\n      message: Release kyverno/kyverno\n      reason: Managing\n      status: \"True\"\n      type: kyverno.kyverno/SveltosHelmReleaseReady\n    - lastTransitionTime: \"2024-10-25T08:36:25Z\"\n      message: 'Release ingress-nginx/ingress-nginx: ClusterSummary global-ingress-capi-dev-cluster-1\n        managing it'\n      reason: Conflict\n      status: \"False\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre></p> <p>The status under <code>.status.services</code> for ClusterDeployment <code>dev-cluster-1</code> shows that it is managing Kyverno but unable to manage ingress-nginx because another object with higher priority is managing it, so it shows a conflict instead.</p> <p>Example</p> <p>Status for <code>dev-cluster-2</code> ClusterDeployment (only sections relevant to beach-head services): <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  . . .\n  name: dev-cluster-2\n  namespace: kcm-system\n  resourceVersion: \"30889\"\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-0\n    priority: 500\n    . . .\n  . . .\nstatus:\n  . . .\n  services:\n  - clusterName: dev-cluster-2\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:18:22Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T08:18:22Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre></p> <p>The status under <code>.status.services</code> for ClusterDeployment <code>dev-cluster-2</code> shows that it is managing ingress-nginx as expected since it has a higher priority.</p>"},{"location":"admin-create-multiclusterservice/#parameter-list","title":"Parameter List","text":"<p>Refer to \"Parameter List\" in Deploy beach-head Services using Cluster Deployment.</p>"},{"location":"admin-creating-clusters/","title":"Creating and lifecycle-managing managed clusters","text":"<p>Once you've installed k0rdent, you can use it to create, manage, update and even upgrade clusters.</p>"},{"location":"admin-creating-clusters/#deploying-a-cluster","title":"Deploying a Cluster","text":"<p>k0rdent is designed to simplify the process of deploying and managing Kubernetes clusters across various cloud platforms. It does this through the use of <code>ClusterDeployment</code>s, which include all of the information k0rdent needs to know in order to create the cluster you're looking for. This <code>ClusterDeployment</code> system relies on predefined templates and credentials. </p> <p>A cluster deployment typically involves:</p> <ol> <li>Setting up credentials for the infrastructure provider (for example, AWS, vSphere).</li> <li>Choosing a template that defines the desired cluster configuration (for example, number of nodes, instance types).</li> <li>Submitting the configuration for deployment and monitoring the process.</li> </ol> <p>Follow these steps to deploy a standalone Kubernetes cluster tailored to your specific needs:</p> <ol> <li> <p>Create the <code>Credential</code> object</p> <p>Credentials are essential for k0rdent to communicate with the infrastructure provider (for example, AWS, Azure, vSphere). These credentials enable k0rdent to provision resources such as virtual machines, networking components, and storage.</p> <p><code>Credential</code> objects are generally created ahead of time and made available to users, so before you look into creating a new one be sure what you're looking for doesn't already exist. You can see all of the existing <code>Credential</code> objects by  querying the management cluster:</p> <pre><code>kubectl get credentials -n A\n</code></pre> <p>If the <code>Credential</code> you need doesn't yet exist, go ahead and create it.</p> <p>Start by creating a <code>Credential</code> object that includes all required authentication details for your chosen infrastructure provider. Follow the instructions in the chapter about credential management, as well as the specific instructions for your target infrastructure.</p> <p>Tip</p> <p>Double-check to make sure that your credentials have sufficient permissions to create resources on the target infrastructure.</p> </li> <li> <p>Select a Template</p> <p>Templates in k0rdent are predefined configurations that describe how the cluster should be set up. Templates include details such as:</p> <ul> <li>The number and type of control plane and worker nodes.</li> <li>Networking settings.</li> <li>Regional deployment preferences.</li> </ul> <p>Templates act as a blueprint for creating a cluster. To see the list of available templates, use the following command:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n</code></pre></p> <p>You can then get information on the actual template by describing it, as in:</p> <pre><code>kubectl describe clustertemplate aws-standalone-cp-0-0-5 -n ksm-system\n</code></pre> </li> <li> <p>Create a ClusterDeployment YAML Configuration</p> <p>The <code>ClusterDeployment</code> object is the main configuration file that defines your cluster's specifications. It includes:</p> <ul> <li>The template to use.</li> <li>The credentials for the infrastructure provider.</li> <li>Optional customizations such as instance types, regions, and networking.</li> </ul> <p>Create a <code>ClusterDeployment</code> configuration in a YAML file, following this structure:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;cluster-name&gt;\n  namespace: &lt;kcm-system-namespace&gt;\nspec:\n  template: &lt;template-name&gt;\n  credential: &lt;infrastructure-provider-credential-name&gt;\n  dryRun: &lt;\"true\" or \"false\" (default: \"false\")&gt;\n  config:\n    &lt;cluster-configuration&gt;\n</code></pre> <p>You will of course want to replace the placeholders with actual values. (For more information about <code>dryRun</code> see Understanding the Dry Run) For example, this is a simple AWS infrastructure provider <code>ClusterDeployment</code>:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-managed-cluster\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-3\n  credential: aws-credential\n  dryRun: false\n  config:\n    clusterLabels: {}\n    region: us-west-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre> Note that the <code>.spec.credential</code> value should match the <code>.metadata.name</code> value of a created <code>Credential</code> object.</p> </li> <li> <p>Apply the Configuration</p> <p>Once the <code>ClusterDeployment</code> configuration is ready, apply it to the k0rdent management cluster:</p> <pre><code>kubectl apply -f clusterdeployment.yaml\n</code></pre> <p>This step submits your deployment request to k0rdent. If you've set <code>dryRun</code> to <code>true</code> you can observe what would happen. Otherwise, k0rdent will go ahead and begin provisioning the necessary infrastructure.</p> </li> <li> <p>Verify Deployment Status</p> <p>After submitting the configuration, verify that the <code>ClusterDeployment</code> object has been created successfully:</p> <pre><code>kubectl -n &lt;namespace&gt; get clusterdeployment.kcm &lt;cluster-name&gt; -o=yaml\n</code></pre> <p>The output shows the current status and any errors.</p> </li> <li> <p>Monitor Provisioning</p> <p>k0rdent will now start provisioning resources (e.g., VMs, networks) and setting up the cluster. To monitor this process, run:</p> <pre><code>kubectl -n &lt;namespace&gt; get cluster &lt;cluster-name&gt; -o=yaml\n</code></pre> <p>Tip</p> <p>For a detailed view of the provisioning process, use the <code>clusterctl describe</code> command (note that this requires the <code>clusterctl</code> CLI):</p> <pre><code>clusterctl describe cluster &lt;cluster-name&gt; -n &lt;namespace&gt; --show-conditions all\n</code></pre> </li> <li> <p>Retrieve the Kubernetes Configuration</p> <p>When provisioning is complete, retrieve the kubeconfig file for the new cluster. This file enables you to interact with the cluster using <code>kubectl</code>:</p> <p><pre><code>kubectl get secret -n &lt;namespace&gt; &lt;cluster-name&gt;-kubeconfig -o=jsonpath={.data.value} | base64 -d &gt; kubeconfig\n</code></pre> You can then use this file to access the cluster, as in:</p> <pre><code>export KUBECONFIG=kubeconfig\nkubectl get pods -A\n</code></pre> <p>Store the kubeconfig file securely, as it contains authentication details for accessing the cluster.</p> </li> </ol>"},{"location":"admin-creating-clusters/#updating-a-single-standalone-cluster","title":"Updating a Single Standalone Cluster","text":"<p>k0rdent <code>ClusterTemplate</code>s are immutable, so the only way to change a <code>ClusterDeployment</code> is to change the template that forms its basis. </p> <p>To update the <code>ClusterDeployment</code>, modify the <code>.spec.template</code> field to use the name of the new <code>ClusterTemplate</code>.  This enables you to apply changes to the cluster configuration. These changes will then be applied to the actual  cluster. For example, if the cluster currently uses <code>t2.large</code> instances, that will be specified in its current template.  To change the cluster to use <code>t2.xlarge</code> instances, you would simply apply a template that references that new size;  k0rdent will then realize the cluster is out of sync and will attempt to remedy the situation by updating the cluster.</p> <p>Follow these steps to update the <code>ClusterDeployment</code>:</p> <ol> <li> <p>Patch the <code>ClusterDeployment</code> with the new template</p> <p>Run the following command, replacing the placeholders with the appropriate values:</p> <pre><code>kubectl patch clusterdeployment.kcm &lt;cluster-name&gt; -n &lt;namespace&gt; --patch '{\"spec\":{\"template\":\"&lt;new-template-name&gt;\"}}' --type=merge\n</code></pre> </li> <li> <p>Check the status of the <code>ClusterDeployment</code></p> <p>After applying the patch, verify the status of the <code>ClusterDeployment</code> object:</p> <pre><code>kubectl get clusterdeployment.kcm &lt;cluster-name&gt; -n &lt;namespace&gt;\n</code></pre> </li> <li> <p>Inspect the detailed status</p> <p>For more details, use the <code>-o=yaml</code> option to check the <code>.status.conditions</code> field:</p> <pre><code>kubectl get clusterdeployment.kcm &lt;cluster-name&gt; -n &lt;namespace&gt; -o=yaml\n</code></pre> </li> </ol> <p>Note that not all updates are possible; <code>ClusterTemplateChain</code> objects limit what templates can be applied.  Consider, for example, this <code>ClusterTemplateChain</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterTemplateChain\nmetadata:\n  name: aws-standalone-cp-0.0.2\n  namespace: kcm-system\nspec:\n  supportedTemplates:\n    - name: aws-standalone-cp-0.0.1\n      availableUpgrades:\n        - name: aws-standalone-cp-0.0.2\n    - name: aws-standalone-cp-0.0.2\n</code></pre> <p>As you can see from the <code>.spec</code>, the <code>aws-standalone-co-0.0.2</code> template can be applied to a cluster that also uses the <code>aws-standalone-co-0.0.2</code> template, or it can be used as an upgrade from a cluster that uses <code>aws-standalone-co-0.0.1</code>. You wouldn't be able to use this template to update a cluster that uses any other <code>ClusterTemplate</code>.</p> <p>Similarly, the <code>AccessManagement</code> object must have properly configured <code>spec.accessRules</code> with a list of allowed  <code>ClusterTemplateChain</code> object names and their namespaces. For more information, see Template Life Cycle Management.</p> <p>Note</p> <p>Support for displaying all available <code>ClusterTemplates</code> for updates in the <code>ClusterDeployment</code> status is planned.</p>"},{"location":"admin-creating-clusters/#cleanup","title":"Cleanup","text":"<p>Especially when you are paying for cloud resources, it's crucial to clean up when you're finished. Fortunately, k0rdent makes that straightforward.</p> <p>Because a Kubernetes cluster is represented by a <code>ClusterDeployment</code>, when you delete that <code>ClusterDeployment</code>, k0rdent deletes the cluster. For example:</p> <p><pre><code>kubectl delete clusterdeployment my-cluster-deployment -n kmc-system\n</code></pre> <pre><code>ClusterDeployment my-cluster-deployment deleted.\n</code></pre></p> <p>It takes time to delete these resources.</p>"},{"location":"admin-credentials/","title":"The Credential System","text":"<p>In order for k0rdent to be able to take action on a particular provider, it must have the proper credentials. This chapter explains how that system works.</p>"},{"location":"admin-credentials/#the-process","title":"The process","text":"<p>In order to pass credentials to k0rdent so it can take action, the following has to happen:</p> <ol> <li> <p>The lead platform engineer or whoever has access to the actual provider credentials creates a <code>Secret</code> that includes that information. For example, for an AWS cluster, it might look like this:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n    name: aws-cluster-identity-secret\n    namespace: kcm-system\ntype: Opaque\nstringData:\n    AccessKeyID: EXAMPLE_ACCESS_KEY_ID\n    SecretAccessKey: EXAMPLE_SECRET_ACCESS_KEY\n</code></pre> <p>Once this secret is created, it can be referenced without the user having access to the actual content, and thus the actual credentials.</p> </li> <li> <p>A provider-specific <code>ClusterIdentity</code> gets created. The <code>ClusterIdentity</code> references the <code>Secret</code> from step one. For example, for an AWS cluster, this object might look like this:</p> <pre><code>kind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Notice that it references the <code>aws-cluster-identity-secret</code> we created earlier. It also specifies the namespaces in which this <code>ClusterIdentity</code> can be used. (In this case there are no restrictions.)</p> </li> <li> <p>Now you can create a <code>Credential</code> object that references the <code>ClusterIdentity</code>, thus making the credentials available and specifying the namespaces where it can be used. Continuing our AWS example:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: aws-cluster-credential\n  namespace: kcm-system\nspec:\n  description: \"Credential Example\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: aws-cluster-identity\n</code></pre> Notice that it references the previous <code>ClusterIdentity</code> (in this case an <code>AWSClusterStaticIdentity</code>). Also notice that you can use the <code>.spec.description</code> field to add additional text about the <code>Credential</code> so users can choose if multiple <code>Credential</code>s are available.</p> </li> <li> <p>Finally, when you create a <code>ClusterDeployment</code>, you reference the <code>Credential</code> object in order to enable k0rdent to pass that information to the infrastructure provider:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-aws-clusterdeployment\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-5\n  credential: aws-cluster-credential\n  config:\n    clusterLabels: {}\n    region: us-east-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre> <p>As you can see, the user doesn't have to pass anything but the name of the <code>Credential</code> in order to deploy the cluster. So all an administrator has to do is add these <code>Credential</code>s to the system and make them available. Note also that the <code>Credential</code> has to be available in the <code>ClusterDeployment</code>s namespace. (See Cloud provider credentials propagation for more information on how that works. )</p> </li> <li> <p>Optionally, certain credentials MAY be propagated to the <code>ClusterDeployment</code> after it is created.</p> <p>The following diagram illustrates the process:</p> <pre><code>flowchart TD\n  Step1[\"&lt;b&gt;Step 1&lt;/b&gt; (Lead Engineer):&lt;br/&gt;Create ClusterIdentity and Secret objects where ClusterIdentity references Secret\"]\n  Step1 --&gt; Step2[\"&lt;b&gt;Step 2&lt;/b&gt; (Any Engineer):&lt;br/&gt;Create Credential object referencing ClusterIdentity\"]\n  Step2 --&gt; Step3[\"&lt;b&gt;Step 3&lt;/b&gt; (Any Engineer):&lt;br/&gt;Create ClusterDeployment referencing Credential object\"]\n  Step3 --&gt; Step4[\"&lt;b&gt;Step 4&lt;/b&gt; (Any Engineer):&lt;br/&gt;Apply ClusterDeployment, wait for provisioning &amp; reconciliation, then propagate credentials to nodes if necessary\"]</code></pre> <p>By design steps 1 and 2 should be executed by the lead engineer who has access to the credentials. Thus credentials could be used by engineers without a need to have access to actual credentials or underlying resources, like <code>ClusterIdentity</code>.</p> </li> </ol>"},{"location":"admin-credentials/#cloud-provider-credentials-propagation","title":"Cloud provider credentials propagation","text":"<p>Some components in the cluster deployment require cloud provider credentials to be passed for proper functioning. As an example, Cloud Controller Manager (CCM) requires provider credentials to create load balancers and provide other functionality.</p> <p>This poses a challenge of credentials delivery. Currently <code>cloud-init</code> is used to pass all necessary credentials. This approach has several problems:</p> <ul> <li>Credentials stored unencrypted in the instance metadata.</li> <li>Rotation of the credentials is impossible without complete instance   redeployment.</li> <li>Possible leaks, since credentials are copied to several <code>Secret</code> objects   related to bootstrap data.</li> </ul> <p>To solve these problems in k0rdent we're using the Sveltos controller, which can render the CCM template with all necessary data from the CAPI provider resources (like <code>ClusterIdentity</code>) and can create secrets directly on the cluster deployment.</p> <p>Note</p> <p> CCM template examples can be found in <code>*-credentials.yaml</code> here. Look for the <code>ConfigMap</code> object that has the <code>projectsveltos.io/template: \"true\"</code> annotation and <code>*-resource-template</code> as the object name.</p> <p>This eliminates the need to pass anything credentials-related to <code>cloud-init</code> and makes it possible to rotate credentials automatically without the need for instance redeployment.</p> <p>Also this automation makes it possible to separate roles and responsibilities where only the lead engineer has access to credentials and other engineers can use them without seeing values and even any access to underlying infrastructure platform.</p> <p>The process is fully automated and credentials will be propagated automatically within the <code>ClusterDeployment</code> reconciliation process; user only needs to provide the correct <code>Credential</code> object.</p>"},{"location":"admin-credentials/#provider-specific-notes","title":"Provider-specific notes","text":"<p>Since this feature depends on the provider, it's important to review any provider-specific notes and clarifications.</p> <p>Note</p> <p>More detailed research notes can be found here.</p>"},{"location":"admin-credentials/#aws","title":"AWS","text":"<p>Since AWS uses roles, which are assigned to instances, no additional credentials will be created.</p> <p>The AWS provider supports 3 types of <code>ClusterIdentity</code> and, which one to use depends on your specific use case. More information regarding CAPA <code>ClusterIdentity</code> resources can be found in the CRD Reference.</p>"},{"location":"admin-credentials/#azure","title":"Azure","text":"<p>Currently the Cluster API Azure (CAPZ) provider creates <code>azure.json</code> <code>Secret</code>s in the same namespace as the <code>Cluster</code> object. By design they should be referenced in the <code>cloud-init</code> YAML later during bootstrap process.</p> <p>In k0rdent these Secrets aren't used and will not be added to the <code>cloud-init</code>, but engineers can access them without restrictions, which is a security issue.</p>"},{"location":"admin-credentials/#openstack","title":"OpenStack","text":"<p>For OpenStack, CAPO relies on a <code>clouds.yaml</code> file. In k0rdent, you provide this file in a Kubernetes <code>Secret</code> that references OpenStack credentials (ideally application credentials for enhanced security). During reconciliation, kcm automatically generates the cloud-config required by OpenStack\u2019s cloud-controller-manager.</p> <p>For more details, refer to the kcm OpenStack Credential Propagation doc.</p>"},{"location":"admin-credentials/#adopted-clusters","title":"Adopted clusters","text":"<p>Credentials for adopted clusters consist of a secret containing a kubeconfig file to access the existing kubernetes cluster.  The kubeconfig file for the cluster should be contained in the value key of the secret object. The following is an example of  a secret which contains the kubeconfig for an adopted cluster. To create this secret, first create or obtain a kubeconfig file  for the cluster that is being adopted and then run the following command to base64 encode it:</p> <pre><code>cat kubeconfig | base64 -w 0\n</code></pre> <p>Once you have obtained a base64 encoded kubeconfig file create a secret:</p> <pre><code>apiVersion: v1\ndata:\n  value: &lt;base64 encoded kubeconfig file&gt;\nkind: Secret\nmetadata:\n  name: adopted-cluster-kubeconf\n  namespace: &lt;namespace&gt;\ntype: Opaque\n</code></pre>"},{"location":"admin-credentials/#the-credential-distribution-system","title":"The Credential Distribution System","text":"<p>k0rdent provides a mechanism to distribute <code>Credential</code> objects across namespaces using the <code>AccessManagement</code> object. This object defines a set of <code>accessRules</code> that determine how credentials are distributed.</p> <p>Each access rule specifies:</p> <ol> <li>The target namespaces where credentials should be delivered.</li> <li>A list of <code>Credential</code> names to distribute to those namespaces.</li> </ol> <p>The kcm controller copies the specified <code>Credential</code> objects from the <code>system</code> namespace to the target namespaces based on the <code>accessRules</code> in the <code>AccessManagement</code> spec.</p> <p>Info</p> <p> Access rules can also include <code>Cluster</code> and <code>Service</code> TemplateChains (<code>clusterTemplateChains</code> and <code>serviceTemplateChains</code>) to distribute templates to target namespaces. For more details, read: Template Life Cycle Management.</p>"},{"location":"admin-credentials/#how-to-configure-credential-distribution","title":"How to Configure Credential Distribution","text":"<p>To configure the distribution of <code>Credential</code> objects:</p> <ol> <li>Edit the <code>AccessManagement</code> object.</li> <li>Populate the <code>.spec.accessRules</code> field with the list of <code>Credential</code> names and the target namespaces.</li> </ol> <p>Here\u2019s an example configuration:</p> <pre><code>spec:\n  accessRules:\n  - targetNamespaces:\n      list:\n        - dev\n        - test\n    credentials:\n      - aws-demo\n      - azure-demo\n</code></pre> <p>In this example, the <code>aws-demo</code> and <code>azure-demo</code> <code>Credential</code> objects will be distributed to the <code>dev</code> and <code>test</code> namespaces.</p>"},{"location":"admin-hosted-control-planes/","title":"Deploying a Hosted Control Plane","text":"<p>A hosted control plane is a Kubernetes setup in which the control plane components (such as the API server,  etcd, and controllers) run inside the management cluster instead of separate controller nodes. This  architecture centralizes control plane management, and improves scalability by sharing resources in the management cluster. Hosted control planes are managed by k0smotron.</p> <p>Instructions for setting up a hosted control plane vary slighting depending on the provider.</p>"},{"location":"admin-hosted-control-planes/#aws-hosted-control-plane-deployment","title":"AWS Hosted Control Plane Deployment","text":"<p>Follow these steps to set up a k0smotron-hosted control plane on AWS: </p> <ol> <li> <p>Prerequisites</p> <p>Before proceeding, make sure you have the following:</p> <ul> <li>A management Kubernetes cluster (Kubernetes v1.28 or later) deployed on AWS with k0rdent installed.</li> <li>A default storage class configured on the management cluster to support Persistent Volumes.</li> <li>The VPC ID where the worker nodes will be deployed.</li> <li>The Subnet ID and Availability Zone (AZ) for the worker nodes.</li> <li>The AMI ID for the worker nodes (Amazon Machine Image ID for the desired OS and Kubernetes version).</li> </ul> <p>Important</p> <p>All control plane components for your hosted cluster will reside in the management cluster. The management cluster  must have sufficient resources to handle these additional workloads.</p> </li> <li> <p>Networking</p> <p>To deploy a hosted control plane, the necessary AWS networking resources must already exist or be created. If you're  using the same VPC and subnets as your management cluster, you can reuse these resources.</p> <p>If your management cluster was deployed using the Cluster API Provider AWS (CAPA), you can gather the required  networking details using the following commands:</p> <p>Retrieve the VPC ID: <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{.spec.network.vpc.id}}'\n</code></pre></p> <p>Retrieve Subnet ID: <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{(index .spec.network.subnets 0).resourceID}}'\n</code></pre></p> <p>Retrieve Availability Zone: <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{(index .spec.network.subnets 0).availabilityZone}}'\n</code></pre></p> <p>Retrieve Security Group: <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{.status.networkStatus.securityGroups.node.id}}'\n</code></pre></p> <p>Retrieve AMI ID: <pre><code>kubectl get awsmachinetemplate &lt;cluster-name&gt;-worker-mt -o go-template='{{.spec.template.spec.ami.id}}'\n</code></pre></p> <p>Tip</p> <p>If you want to use different VPCs or regions for your management and hosted clusters, you\u2019ll need to configure additional networking, such as VPC peering, to allow communication between them.</p> </li> <li> <p>Create the ClusterDeployment manifest</p> <p>Once you've collected all the necessary data, you can create the <code>ClusterDeployment</code> manifest. This file tells k0rdent how to  deploy and manage the hosted control plane. Below is an example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: aws-hosted-cp\nspec:\n  template: aws-hosted-cp-0-0-3\n  credential: aws-credential\n  config:\n    clusterLabels: {}\n    vpcID: vpc-0a000000000000000\n    region: us-west-1\n    publicIP: true\n    subnets:\n      - id: subnet-0aaaaaaaaaaaaaaaa\n        availabilityZone: us-west-1b\n        isPublic: true\n        natGatewayID: xxxxxx\n        routeTableId: xxxxxx\n      - id: subnet-1aaaaaaaaaaaaaaaa\n        availabilityZone: us-west-1b\n        isPublic: false\n        routeTableId: xxxxxx\n    instanceType: t3.medium\n    securityGroupIDs:\n      - sg-0e000000000000000\n</code></pre> <p>Note</p> <p>The example above uses the <code>us-west-1</code> region, but you should use the region of your VPC.</p> </li> <li> <p>Generate the <code>ClusterDeployment</code> Manifest</p> <p>To simplify the creation of a <code>ClusterDeployment</code> manifest, you can use the following template, which dynamically  inserts the appropriate values: <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: aws-hosted\nspec:\n  template: aws-hosted-cp-0-0-3\n  credential: aws-credential\n  config:\n    clusterLabels: {}\n    vpcID: \"{{.spec.network.vpc.id}}\"\n    region: \"{{.spec.region}}\"\n    subnets:\n    {{- range $subnet := .spec.network.subnets }}\n      - id: \"{{ $subnet.resourceID }}\"\n        availabilityZone: \"{{ $subnet.availabilityZone }}\"\n        isPublic: {{ $subnet.isPublic }}\n        {{- if $subnet.isPublic }}\n        natGatewayId: \"{{ $subnet.natGatewayId }}\"\n        {{- end }}\n        routeTableId: \"{{ $subnet.routeTableId }}\"\n    {{- end }}\n    instanceType: t3.medium\n    securityGroupIDs:\n      - \"{{.status.networkStatus.securityGroups.node.id}}\"\n</code></pre></p> <p>Save this template as <code>clusterdeployment.yaml.tpl</code>, then generate your manifest using the following command:</p> <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template=\"$(cat clusterdeployment.yaml.tpl)\" &gt; clusterdeployment.yaml\n</code></pre> </li> <li> <p>Apply the <code>ClusterTemplate</code></p> <p>Nothing actually happens until you apply the <code>ClusterDeployment</code> manifest to create a new cluster deployment:</p> <pre><code>kubectl apply -f clusterdeployment.yaml -n kcm-system\n</code></pre> </li> </ol>"},{"location":"admin-hosted-control-planes/#deployment-tips","title":"Deployment Tips","text":"<p>Here are some additional tips to help with deployment:</p> <ol> <li> <p>Controller and Template Availability:</p> <p>Make sure the kcm controller image and templates are available in a public or accessible repository.</p> </li> <li> <p>Install Charts and Templates:</p> <p>If you're using a custom repository, run the following commands with the appropriate <code>kubeconfig</code>:</p> <pre><code>KUBECONFIG=kubeconfig IMG=\"ghcr.io/k0rdent/kcm/controller-ci:v0.0.1-179-ga5bdf29\" REGISTRY_REPO=\"oci://ghcr.io/k0rdent/kcm/charts-ci\" make dev-apply\nKUBECONFIG=kubeconfig make dev-templates\n</code></pre> </li> <li> <p>Mark Infrastructure as Ready:</p> <p>To scale up the <code>MachineDeployment</code>, manually mark the infrastructure as ready: <pre><code>kubectl patch AWSCluster &lt;hosted-cluster-name&gt; --type=merge --subresource status --patch '{\"status\": {\"ready\": true}}' -n kcm-system\n</code></pre> For more details on why this is necessary, click here.</p> </li> </ol>"},{"location":"admin-hosted-control-planes/#azure-hosted-control-plane-deployment","title":"Azure Hosted Control Plane Deployment","text":"<p>Follow these steps to set up a k0smotron-hosted control plane on Azure:</p> <ol> <li> <p>Prerequisites</p> <p>Before you start, make sure you have the following:</p> <ul> <li>A management Kubernetes cluster (Kubernetes v1.28+) deployed on Azure with k0rdent installed.</li> <li>A default storage class configured    on the management cluster to support Persistent Volumes.</li> </ul> <p>Note</p> <p>All control plane components for managed clusters will run in the management cluster. Make sure the management cluster    has sufficient CPU, memory, and storage to handle the additional workload.</p> </li> <li> <p>Gather pre-existing resources</p> <p>In a hosted control plane setup, some Azure resources must exist before deployment and must be explicitly  provided in the <code>ClusterDeployment</code> configuration. These resources can also be reused by the management cluster.</p> <p>If you deployed your Azure Kubernetes cluster using the Cluster API Provider for Azure (CAPZ), you can retrieve  the required information using the following commands:</p> <p>Location: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.location}}'\n</code></pre></p> <p>Subscription ID: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.subscriptionID}}'\n</code></pre></p> <p>Resource Group: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.resourceGroup}}'\n</code></pre></p> <p>VNet Name: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.networkSpec.vnet.name}}'\n</code></pre></p> <p>Subnet Name: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).name}}'\n</code></pre></p> <p>Route Table Name: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).routeTable.name}}'\n</code></pre></p> <p>Security Group Name: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).securityGroup.name}}'\n</code></pre></p> </li> <li> <p>Create the ClusterDeployment manifest</p> <p>After collecting the required data, create a <code>ClusterDeployment</code> manifest to configure the hosted control plane. It should look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp-0-0-2\n  credential: azure-credential\n  config:\n    clusterLabels: {}\n    location: \"westus\"\n    subscriptionID: ceb131c7-a917-439f-8e19-cd59fe247e03\n    vmSize: Standard_A4_v2\n    resourceGroup: mgmt-cluster\n    network:\n      vnetName: mgmt-cluster-vnet\n      nodeSubnetName: mgmt-cluster-node-subnet\n      routeTableName: mgmt-cluster-node-routetable\n      securityGroupName: mgmt-cluster-node-nsg\n</code></pre> </li> <li> <p>Generate the <code>ClusterDeployment</code> manifest</p> <p>To simplify the creation of a <code>ClusterDeployment</code> manifest, you can use the following template, which dynamically inserts  the appropriate values:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp-0-0-2\n  credential: azure-credential\n  config:\n    clusterLabels: {}\n    location: \"{{.spec.location}}\"\n    subscriptionID: \"{{.spec.subscriptionID}}\"\n    vmSize: Standard_A4_v2\n    resourceGroup: \"{{.spec.resourceGroup}}\"\n    network:\n      vnetName: \"{{.spec.networkSpec.vnet.name}}\"\n      nodeSubnetName: \"{{(index .spec.networkSpec.subnets 1).name}}\"\n      routeTableName: \"{{(index .spec.networkSpec.subnets 1).routeTable.name}}\"\n      securityGroupName: \"{{(index .spec.networkSpec.subnets 1).securityGroup.name}}\"\n</code></pre> Save this YAML as <code>clusterdeployment.yaml.tpl</code> and render the manifest with the following command: <pre><code>kubectl get azurecluster &lt;management-cluster-name&gt; -o go-template=\"$(cat clusterdeployment.yaml.tpl)\" &gt; clusterdeployment.yaml\n</code></pre></p> </li> <li> <p>Create the <code>ClusterDeployment</code></p> <p>To actually create the cluster, apply the <code>ClusterDeployment</code> manifest to the management cluster, as in:</p> <pre><code>kubectl apply clusterdeployment.yaml -n kcm-system\n</code></pre> </li> <li> <p>Manually update the <code>AzureCluster</code> object</p> <p>Due to a limitation in k0smotron, (see k0sproject/k0smotron#668),  after applying the <code>ClusterDeployment</code> manifest, you must manually update the status of the <code>AzureCluster</code> object.</p> <p>Use the following command to set the <code>AzureCluster</code> object status to <code>Ready</code>:</p> <pre><code>kubectl patch azurecluster &lt;cluster-name&gt; --type=merge --subresource status --patch '{\"status\": {\"ready\": true}}'\n</code></pre> </li> </ol>"},{"location":"admin-hosted-control-planes/#important-notes-on-cluster-deletion","title":"Important Notes on Cluster Deletion","text":"<p>Due to these same k0smotron limitations, you must take some manual steps in order to delete a cluster properly:</p> <ol> <li> <p>Add a Custom Finalizer to the AzureCluster Object:</p> <p>To prevent the <code>AzureCluster</code> object from being deleted too early, add a custom finalizer:</p> <pre><code>kubectl patch azurecluster &lt;cluster-name&gt; --type=merge --patch '{\"metadata\": {\"finalizers\": [\"manual\"]}}'\n</code></pre> </li> <li> <p>Delete the ClusterDeployment:</p> <p>After adding the finalizer, delete the <code>ClusterDeployment</code> object as usual. Confirm that all <code>AzureMachines</code> objects have been deleted successfully.</p> </li> <li> <p>Remove Finalizers from Orphaned AzureMachines:</p> <p>If any <code>AzureMachines</code> are left orphaned, delete their finalizers manually after confirming no VMs remain in Azure. Use this command to remove the finalizer:</p> <pre><code>kubectl patch azuremachine &lt;machine-name&gt; --type=merge --patch '{\"metadata\": {\"finalizers\": []}}'\n</code></pre> </li> <li> <p>Allowing Updates to Orphaned Objects:</p> <p>If Azure admission controls prevent updates to orphaned objects, you must disable the associated <code>MutatingWebhookConfiguration</code> by deleting it:</p> <pre><code>kubectl delete mutatingwebhookconfiguration &lt;webhook-name&gt;\n</code></pre> </li> </ol>"},{"location":"admin-hosted-control-planes/#vsphere-hosted-control-plane-deployment","title":"vSphere Hosted Control Plane Deployment","text":"<p>Follow these steps to set up a k0smotron-hosted control plane on vSphere. </p> <ol> <li> <p>Prerequisites</p> <p>Before you start, make sure you have the following:</p> <ul> <li>A management Kubernetes cluster (Kubernetes v1.28+) deployed on vSphere with k0rdent installed.</li> </ul> <p>All control plane components for managed clusters will reside in the management cluster. Make sure the management  cluster has sufficient resources (CPU, memory, and storage) to handle these workloads.</p> </li> <li> <p>Create the <code>ClusterDeployment</code> Manifest</p> </li> </ol> <p>The <code>ClusterDeployment</code> manifest for vSphere-hosted control planes is similar to standalone control plane deployments.  For a detailed list of parameters, refer to our discussion of Template parameters for vSphere.</p> <p>Important</p> <p>The vSphere provider requires you to specify the control plane endpoint IP before deploying the cluster. This IP  address must match the one assigned to the k0smotron load balancer (LB) service. Use an annotation supported by your load balancer provider to assign the control plane endpoint IP to the k0smotron  service. For example, the manifest below includes a <code>kube-vip</code> annotation.</p> <p><code>ClusterDeployment</code>s for vSphere-based clusters include a <code>.spec.config.vsphere</code> object that contains vSphere-specific parameters. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: vsphere-hosted-cp-0-0-2\n  credential: vsphere-credential\n  config:\n    clusterLabels: {}\n    vsphere:\n      server: vcenter.example.com\n      thumbprint: \"00:00:00\"\n      datacenter: \"DC\"\n      datastore: \"/DC/datastore/DC\"\n      resourcePool: \"/DC/host/vCluster/Resources/ResPool\"\n      folder: \"/DC/vm/example\"\n    controlPlaneEndpointIP: \"172.16.0.10\"\n    ssh:\n      user: ubuntu\n      publicKey: |\n        ssh-rsa AAA...\n    rootVolumeSize: 50\n    cpus: 2\n    memory: 4096\n    vmTemplate: \"/DC/vm/template\"\n    network: \"/DC/network/Net\"\n    k0smotron:\n      service:\n        annotations:\n          kube-vip.io/loadbalancerIPs: \"172.16.0.10\"\n</code></pre> <p>For more information on these parameters, see the Template reference for vsphere. </p>"},{"location":"admin-installation/","title":"Installing k0rdent","text":"<p>The process of installing k0rdent is straightforward, and involves the following steps:</p> <ol> <li>Create a Kubernetes cluster to act as the management cluster</li> <li>Install k0rdent into the management cluster</li> <li>Add the necessary credentials and templates to work with the providers in your infrastructure.</li> </ol>"},{"location":"admin-installation/#create-and-prepare-the-management-kubernetes-cluster","title":"Create and prepare the Management Kubernetes cluster","text":"<p>The first step is to create the Kubernetes cluster.</p>"},{"location":"admin-installation/#cncf-certified-kubernetes","title":"CNCF-certified Kubernetes","text":"<p>k0rdent is designed to run on any CNCF-certified Kubernetes. This gives you great freedom in setting up k0rdent for convenient, secure, and reliable operations.</p> <ul> <li>Proof-of-concept scale k0rdent implementations can run on a beefy Linux desktop or laptop, using a single-node-capable CNCF Kubernetes distribution like k0s, running natively as bare processes, or inside a container with KinD.</li> <li>More capacious implementations can run on multi-node Kubernetes clusters on bare metal or in clouds.</li> <li>Production users can leverage cloud provider Kubernetes variants like Amazon EKS or Azure AKS to quickly create highly-reliable and scalable k0rdent management clusters.</li> </ul>"},{"location":"admin-installation/#mixed-use-management-clusters","title":"Mixed-use management clusters","text":"<p>k0rdent management clusters can also be mixed-use. For example, a cloud Kubernetes k0rdent management cluster can also be used as a 'mothership' environment for k0smotron hosted control planes managed by k0rdent, integrated with workers bootstrapped (also by k0rdent) on adjacent cloud VMs or on remote VMs, bare metal servers, or edge nodes.</p>"},{"location":"admin-installation/#where-k0rdent-management-clusters-can-live","title":"Where k0rdent management clusters can live","text":"<p>k0rdent management clusters can live anywhere. Unlike prior generations of Kubernetes cluster managers, there's no technical need to co-locate a k0rdent manager with managed clusters on a single infrastructure. k0rdent provides a single point of control and visibility across any cloud, any infrastructure, anywhere, on-premise or off. Deciding where to put a management cluster (or multiple clusters) is best done by assessing the requirements of your use case. Considered in the abstract, a k0rdent management cluster should be:</p> <ul> <li>Resilient and available</li> <li>Accessible and secure</li> <li>Easy to network</li> <li>Scalable (particularly for mixed-use implementations)</li> <li>Easy to operate with minimum overhead</li> <li>Monitored and observable</li> <li>Equipped for backup and disaster recovery</li> </ul>"},{"location":"admin-installation/#minimum-requirements-for-single-node-k0rdent-management-clusters-for-testing","title":"Minimum requirements for single-node k0rdent management clusters for testing","text":"<p>There isn't a strict minimum system requirement for k0rdent, but the following are recommended for a single node:</p> <ul> <li>A minimum of 32GB RAM</li> <li>8 cores</li> <li>100GB SSD</li> </ul> <p>This configuration is only sufficient for the base case.  If you will run k0rdent Observability and FinOps (KOF) you will need significantly more resources and in particular, much more storage.</p>"},{"location":"admin-installation/#recommended-requirements-for-single-node-k0rdent-management-clusters-for-production","title":"Recommended requirements for single-node k0rdent management clusters for production","text":"<p>We do not recommend running k0rdent in production on a single node.  If you are running in production you will want a scale-out architecture.  These documents do not currently cover this case.</p> <p>Important</p> <p> Detailed instructions for deploying k0rdent management clusters into a multi-node configuration and scaling them as needed are COMING SOON!</p>"},{"location":"admin-installation/#install-k0rdent","title":"Install k0rdent","text":"<p>This assumes that you already have a kubernetes cluster installed, if you need to setup a cluster you can follow the Create and prepare a Kubernetes cluster with k0s guide</p> <p>The actual management cluster is a Kubernetes cluster with the k0rdent application installed. The simplest way to install k0rdent is through its Helm chart.  You can find the latest release here, and from there you can deploy the Helm chart, as in:</p> <pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm --version 0.1.0 -n kcm-system --create-namespace\n</code></pre> <pre><code>Pulled: ghcr.io/k0rdent/kcm/charts/kcm:0.1.0\nDigest: sha256:1f75e8e55c44d10381d7b539454c63b751f9a2ec6c663e2ab118d34c5a21087f\nNAME: kcm\nLAST DEPLOYED: Mon Dec  9 00:32:14 2024\nNAMESPACE: kcm-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>Note</p> <p> Make sure to specify the correct release version number.</p> <p>The helm chart deploys the KCM operator and prepares the environment, KCM then proceeds to deploy the various sub components, including CAPI, the entire process takes a few minutes.</p>"},{"location":"admin-installation/#confirming-the-deployment","title":"Confirming the deployment","text":"<p>To understand whether installation is complete, start by making sure all pods are ready in the <code>kcm-system</code> namespace. There should be 15:</p> <pre><code>kubectl get pods -n kcm-system\n</code></pre> <pre><code>NAME                                                          READY   STATUS    RESTARTS   AGE\nazureserviceoperator-controller-manager-6b4dd86894-2m2wh      0/1     Running   0          57s\ncapa-controller-manager-64bbcb9f8-kx6wr                       1/1     Running   0          2m3s\ncapi-controller-manager-66f8998ff5-2m5m2                      1/1     Running   0          2m32s\ncapo-controller-manager-588f45c7cf-lmkgz                      1/1     Running   0          50s\ncapv-controller-manager-69f7fc65d8-wbqh8                      1/1     Running   0          46s\ncapz-controller-manager-544845f786-q8rzn                      1/1     Running   0          57s\nhelm-controller-7644c4d5c4-t6sjm                              1/1     Running   0          5m1s\nk0smotron-controller-manager-bootstrap-9fc48d76f-hppzs        2/2     Running   0          2m9s\nk0smotron-controller-manager-control-plane-7df9bc7bf-74vcs    2/2     Running   0          2m4s\nk0smotron-controller-manager-infrastructure-f7f94dd76-ppzq4   2/2     Running   0          116s\nkcm-cert-manager-895954d88-jplf6                              1/1     Running   0          5m1s\nkcm-cert-manager-cainjector-685ffdf549-qlj8m                  1/1     Running   0          5m1s\nkcm-cert-manager-webhook-59ddc6b56-8hfml                      1/1     Running   0          5m1s\nkcm-cluster-api-operator-8487958779-l9lvf                     1/1     Running   0          3m12s\nkcm-controller-manager-7998cdb69-x7kd9                        1/1     Running   0          3m12s\nkcm-velero-b68fd5957-fwg2l                                    1/1     Running   0          5m1s\nsource-controller-6cd7676f7f-7kpjn                            1/1     Running   0          5m1s\n</code></pre> <p>State management is handled by Project Sveltos, so you'll want to make sure that all 9 pods are running in the <code>projectsveltos</code> namespace:</p> <pre><code>kubectl get pods -n projectsveltos\n</code></pre> <pre><code>AME                                     READY   STATUS      RESTARTS   AGE\naccess-manager-56696cc7f-5txlb           1/1     Running     0          4m1s\naddon-controller-7c98776c79-dn9jm        1/1     Running     0          4m1s\nclassifier-manager-7b85f96469-666jx       1/1     Running     0          4m1s\nevent-manager-67f6db7f44-hsnnj           1/1     Running     0          4m1s\nhc-manager-6d579d675f-fgvk2              1/1     Running     0          4m1s\nregister-mgmt-cluster-job-rfkdh          0/1     Completed   0          4m1s\nsc-manager-55c99d494b-c8wrl              1/1     Running     0          4m1s\nshard-controller-5ff9cd796d-tlg79        1/1     Running     0          4m1s\nsveltos-agent-manager-7467959f4f-lsnd5   1/1     Running     0          3m34s\n</code></pre> <p>If any of these pods are missing, simply give k0rdent more time. If there's a problem, you'll see pods crashing and restarting, and you can see what's happening by describing the pod, as in:</p> <pre><code>kubectl describe pod classifier-manager-7b85f96469-666jx -n projectsveltos\n</code></pre> <p>As long as you're not seeing pod restarts, you just need to wait a few minutes.</p> <p>Next verify whether the kcm templates have been successfully installed and reconciled.  Start with the <code>ProviderTemplate</code> objects:</p> <pre><code>kubectl get providertemplate -n kcm-system\n</code></pre> <pre><code>NAME                                   VALID\ncluster-api-0-1-0                      true\ncluster-api-provider-aws-0-1-0         true\ncluster-api-provider-azure-0-1-0       true\ncluster-api-provider-openstack-0-1-0   true\ncluster-api-provider-vsphere-0-1-0     true\nk0smotron-0-1-0                        true\nkcm-0-1-0                              true\nprojectsveltos-0-45-0                  true\n</code></pre> <p>Make sure that all templates are not just installed, but valid. Again, this may take a few minutes.</p> <p>You'll also want to make sure the <code>ClusterTemplate</code> objects are installed and valid:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-0-1-0           true\naws-eks-0-1-0                   true\naws-hosted-cp-0-1-0             true\naws-standalone-cp-0-1-0         true\nazure-aks-0-1-0                 true\nazure-hosted-cp-0-1-0           true\nazure-standalone-cp-0-1-0       true\nopenstack-standalone-cp-0-1-0   true\nvsphere-hosted-cp-0-1-0         true\nvsphere-standalone-cp-0-1-0     true\n</code></pre> <p>Finally, make sure the <code>ServiceTemplate</code> objects are installed and valid:</p> <pre><code>kubectl get servicetemplate -n kcm-system\n</code></pre> <pre><code>NAME                      VALID\ncert-manager-1-16-2       true\ndex-0-19-1                true\nexternal-secrets-0-11-0   true\ningress-nginx-4-11-0      true\ningress-nginx-4-11-3      true\nkyverno-3-2-6             true\nvelero-8-1-0              true\n</code></pre>"},{"location":"admin-installation/#backing-up-a-k0rdent-management-cluster","title":"Backing up a k0rdent management cluster","text":"<p>In a production environment, you will always want to ensure that your management cluster is backed up. There are a few caveats and things you need to take into account when backing up k0rdent more info can be found in the guid at use Velero as a backup provider.</p>"},{"location":"admin-installation/#create-and-prepare-a-kubernetes-cluster-with-k0s","title":"Create and prepare a Kubernetes cluster with k0s","text":"<p>follow these steps to install and prepare a k0s kubernetes management cluster:</p> <ol> <li> <p>Deploy a Kubernetes cluster</p> <p>The first step is to create the actual cluster itself. Again, the actual distribution used for the management cluster isn't important, as long as it's a CNCF-compliant distribution. That means you can use an existing EKS cluster, or whatever is your normal corporate standard. To make things simple this guide uses k0s, a small, convenient, and fully-functional distribution:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh\nsudo k0s install controller --single\nsudo k0s start\n</code></pre> <p>k0s includes its own preconfigured version of <code>kubectl</code> so make sure the cluster is running:</p> <pre><code>sudo k0s kubectl get nodes\n</code></pre> <p>You should see a single node with a status of <code>Ready</code>, as in:</p> <pre><code>NAME              STATUS   ROLES    AGE   VERSION\nip-172-31-29-61   Ready    &lt;none&gt;   46s   v1.31.2+k0s\n</code></pre> </li> <li> <p>Install kubectl</p> <p>Everything you do in k0rdent is done by creating and manipulating Kubernetes objects, so you'll need to have <code>kubectl</code> installed. You can find the full install docs here, or just follow these instructions:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly\nsudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre> </li> <li> <p>Get the kubeconfig</p> <p>In order to access the management cluster you will, of course, need the kubeconfig. Again, if you're using another Kubernetes distribution follow those instructions to get the kubeconfig, but for k0s, the process involves simply copying the existing file and adding it to an environment variable so <code>kubectl</code> knows where to find it.</p> <pre><code>sudo cp /var/lib/k0s/pki/admin.conf KUBECONFIG\nsudo chmod +r KUBECONFIG\nexport KUBECONFIG=./KUBECONFIG\n</code></pre> <p>Now you should be able to use the non-k0s <code>kubectl</code> to see the status of the cluster:</p> <pre><code>kubectl get nodes\n</code></pre> <p>Again, you should see the single k0s node, but by this time it should have had its role assigned, as in:</p> <pre><code>NAME              STATUS   ROLES           AGE   VERSION\nip-172-31-29-61   Ready    control-plane   25m   v1.31.2+k0s\n</code></pre> <p>Now the cluster is ready for installation, which we'll do using Helm.</p> </li> <li> <p>Install Helm</p> <p>The easiest way to install k0rdent is through its Helm chart, so let's get Helm installed. You can find the full instructions here, or use these instructions:</p> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre> <p>Helm will be installed into <code>/usr/local/bin/helm</code></p> </li> </ol>"},{"location":"admin-kof/","title":"k0rdent Observability and FinOps (kof)","text":"<p>k0rdent Observability and FinOps (kof) provides enterprise-grade observability and FinOps capabilities for k0rdent-managed Kubernetes clusters. It enables centralized metrics, logging, and cost management through a unified OpenTelemetry-based architecture.</p>"},{"location":"admin-kof/#architecture","title":"Architecture","text":"<p>The general idea of kof is to collect relevant information about what's happening in the system, store it, and make it available for management and analysis.</p>"},{"location":"admin-kof/#high-level","title":"High-level","text":"<p>kof consists of 3 layers: Management, Storage, and Collection:</p> <pre><code>           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502   Management   \u2502\n           \u2502   UI, promxy   \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n             \u2502             \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n        \u2502 Storage  \u2502 \u2502 Storage  \u2502\n        \u2502 region 1 \u2502 \u2502 region 2 \u2502\n        \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n             \u2502             \u2502\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510     ...\n      \u2502             \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Collect   \u2502 \u2502 Collect   \u2502\n\u2502 managed 1 \u2502 \u2502 managed 2 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"admin-kof/#mid-level","title":"Mid-level","text":"<p>Data flows up, from observed resources to centralized Grafana:</p> <pre><code>management cluster_____________________\n\u2502                                     \u2502\n\u2502  kof-mothership chart_____________  \u2502\n\u2502  \u2502                               \u2502  \u2502\n\u2502  \u2502 centralized Grafana &amp; storage \u2502  \u2502\n\u2502  \u2502                               \u2502  \u2502\n\u2502  \u2502 promxy                        \u2502  \u2502\n\u2502  \u2502_______________________________\u2502  \u2502\n\u2502_____________________________________\u2502\n\n\ncloud 1...\n\u2502\n\u2502  region 1__________________________________________  region 2...\n\u2502  \u2502                                                \u2502  \u2502\n.  \u2502  storage cluster_____________________          \u2502  \u2502\n.  \u2502  \u2502                                  \u2502          \u2502  \u2502\n.  \u2502  \u2502  kof-storage chart_____________  \u2502          \u2502  .\n   \u2502  \u2502  \u2502                            \u2502  \u2502          \u2502  .\n   \u2502  \u2502  \u2502 regional Grafana &amp; storage \u2502  \u2502          \u2502  .\n   \u2502  \u2502  \u2502____________________________\u2502  \u2502          \u2502\n   \u2502  \u2502__________________________________\u2502          \u2502\n   \u2502                                                \u2502\n   \u2502                                                \u2502\n   \u2502  managed cluster 1_____________________  2...  \u2502\n   \u2502  \u2502                                    \u2502  \u2502     \u2502\n   \u2502  \u2502  kof-operators chart_____________  \u2502  \u2502     \u2502\n   \u2502  \u2502  \u2502                              \u2502  \u2502  \u2502     \u2502\n   \u2502  \u2502  \u2502  opentelemetry-operator____  \u2502  \u2502  .     \u2502\n   \u2502  \u2502  \u2502  \u2502                        \u2502  \u2502  \u2502  .     \u2502\n   \u2502  \u2502  \u2502  \u2502 OpenTelemetryCollector \u2502  \u2502  \u2502  .     \u2502\n   \u2502  \u2502  \u2502  \u2502________________________\u2502  \u2502  \u2502        \u2502\n   \u2502  \u2502  \u2502                              \u2502  \u2502        \u2502\n   \u2502  \u2502  \u2502  prometheus-operator-crds    \u2502  \u2502        \u2502\n   \u2502  \u2502  \u2502______________________________\u2502  \u2502        \u2502\n   \u2502  \u2502                                    \u2502        \u2502\n   \u2502  \u2502                                    \u2502        \u2502\n   \u2502  \u2502  kof-collectors chart________      \u2502        \u2502\n   \u2502  \u2502  \u2502                          \u2502      \u2502        \u2502\n   \u2502  \u2502  \u2502 opencost                 \u2502      \u2502        \u2502\n   \u2502  \u2502  \u2502 kube-state-metrics       \u2502      \u2502        \u2502\n   \u2502  \u2502  \u2502 prometheus-node-exporter \u2502      \u2502        \u2502\n   \u2502  \u2502  \u2502__________________________\u2502      \u2502        \u2502\n   \u2502  \u2502                                    \u2502        \u2502\n   \u2502  \u2502                                    \u2502        \u2502\n   \u2502  \u2502  observed resources                \u2502        \u2502\n   \u2502  \u2502____________________________________\u2502        \u2502\n   \u2502________________________________________________\u2502\n</code></pre>"},{"location":"admin-kof/#low-level","title":"Low-level","text":"<p>kof is tightly integrated with the k0rdent management cluster and with managed clusters:</p> <p></p>"},{"location":"admin-kof/#helm-charts","title":"Helm Charts","text":"<p>kof can be installed through a series of Helm charts.</p>"},{"location":"admin-kof/#kof-mothership","title":"kof-mothership","text":"<ul> <li>Centralized Grafana dashboard, managed by grafana-operator</li> <li>Local VictoriaMetrics storage for alerting rules only, managed by victoria-metrics-operator</li> <li>Sveltos dashboard, automatic secret distribution</li> <li>cluster-api-visualizer for insight into multicluster configuration</li> <li>k0rdent service templates to deploy other charts to regional clusters</li> <li>Promxy for aggregating Prometheus metrics from regional clusters</li> </ul>"},{"location":"admin-kof/#kof-storage","title":"kof-storage","text":"<ul> <li>Regional Grafana dashboard, managed by grafana-operator</li> <li>Regional VictoriaMetrics storage with main data, managed by victoria-metrics-operator</li> <li>vmauth entrypoint proxy for VictoriaMetrics components</li> <li>vmcluster for high-available fault-tolerant version of VictoriaMetrics database</li> <li>victoria-logs-single for high-performance, cost-effective, scalable logs storage</li> <li>external-dns to communicate with other clusters</li> </ul>"},{"location":"admin-kof/#kof-operators","title":"kof-operators","text":"<ul> <li>prometheus-operator-crds required to create OpenTelemetry collectors</li> <li>OpenTelemetry collectors below, managed by opentelemetry-operator</li> </ul>"},{"location":"admin-kof/#kof-collectors","title":"kof-collectors","text":"<ul> <li>prometheus-node-exporter for hardware and OS metrics</li> <li>kube-state-metrics (KSM) for metrics about the state of Kubernetes objects</li> <li>OpenCost \"shines a light into the black box of Kubernetes spend\"</li> </ul>"},{"location":"admin-kof/#demo","title":"Demo","text":"<p>You can see kof at work by watching these demos.</p>"},{"location":"admin-kof/#grafana","title":"Grafana","text":""},{"location":"admin-kof/#sveltos","title":"Sveltos","text":""},{"location":"admin-kof/#implementation-guide","title":"Implementation Guide","text":"<p>This implementation guide is still a work in progress, but you can get a feel for how to use it in your own k0rdent implementation.</p>"},{"location":"admin-kof/#prerequisites","title":"Prerequisites","text":"<p>Before beginning, make sure that you have:</p> <ul> <li>A working k0rdent management cluster</li> <li>Infrastructure provider credentials</li> <li>A domain for service endpoints</li> <li>cert-manager for SSL certificates</li> <li>The ingress-nginx controller installed in your management cluster</li> </ul> <p>Follow these steps:</p> <ol> <li> <p>Storage Cluster Deployment</p> <p>Create a <code>ClusterDeployment</code> for storage of data. </p> <p><code>yaml apiVersion: kcm.mirantis.com/v1alpha1 kind: ClusterDeployment metadata:     name: kof-storage-cluster     namespace: kof spec:     template: kof-storage     config:         clusterLabels: {}         ingress:             vmauth:                 host: vmauth.storage.example.com         grafana:             host: grafana.storage.example.com         retention:             metrics: 30d             logs: 14d         storage:             class: standard             size: 100Gi</code></p> </li> <li> <p>Collector Deployment</p> <p>Create a second <code>ClusterDeployment</code> for the kof-collector components:</p> <pre><code>apiVersion: kcm.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n    name: kof-collector\n    namespace: kof\nspec:\n    template: kof-collectors\n    config:\n        clusterLabels: {}\n        storage:\n            endpoint: vmauth.storage.example.com\n    collection:\n        logLevel: info\n        metrics:\n            scrapeInterval: 30s\n        logs:\n            excludePaths: \n                - /var/log/lastlog\n                - /var/log/tallylog\n</code></pre> </li> <li> <p>Grafana Setup</p> <p>To set up Grafana, first retrieve access credentials from the cluster:</p> <pre><code>kubectl get secret -n kof grafana-admin-credentials -o jsonpath=\"{.data.GF_SECURITY_ADMIN_USER}\" | base64 -d\nkubectl get secret -n kof grafana-admin-credentials -o jsonpath=\"{.data.GF_SECURITY_ADMIN_PASSWORD}\" | base64 -d\n</code></pre> <p>Then configure data sources:</p> <pre><code>apiVersion: grafana.integreatly.org/v1beta1\nkind: GrafanaDatasource\nmetadata:\n    name: metrics\nspec:\n    name: VictoriaMetrics\n    type: prometheus\n    access: proxy\n    url: http://vmcluster-vmselect.victoria-metrics:8481/select/0/prometheus\n</code></pre> </li> </ol>"},{"location":"admin-kof/#dashboard-organization","title":"Dashboard Organization","text":"<p>Once kof is installed and configured you can configure your dashboard. Follow these steps:</p> <ol> <li> <p>Login to\u00a0http://127.0.0.1:3000/dashboards\u00a0with user/pass printed above.</p> </li> <li> <p>Open a dashboard.</p> </li> </ol> Pastedimage20250122181247.png <p>Review the following:</p>"},{"location":"admin-kof/#1-cluster-overview","title":"1. Cluster Overview","text":"<ul> <li>Health metrics</li> <li>Resource utilization</li> <li>Performance trends</li> <li>Cost analysis</li> </ul>"},{"location":"admin-kof/#2-logging-interface","title":"2. Logging Interface","text":"<ul> <li>Real-time log streaming</li> <li>Full-text search</li> <li>Log aggregation</li> <li>Alert correlation</li> </ul>"},{"location":"admin-kof/#3-cost-management","title":"3. Cost Management","text":"<ul> <li>Resource cost tracking</li> <li>Usage analysis</li> <li>Budget monitoring</li> <li>Optimization recommendations</li> </ul>"},{"location":"admin-kof/#scaling-guidelines","title":"Scaling Guidelines","text":"<p>To keep k0rdent Observability and FinOps properly resourced, consider the following:</p>"},{"location":"admin-kof/#regional-expansion","title":"Regional Expansion","text":"<p>When managed clusters expand to new regions, you will want to expand kof as well:</p> <ol> <li>Deploy storage clusters in new regions</li> <li>Update Promxy configuration</li> <li>Configure collector routing</li> </ol>"},{"location":"admin-kof/#cluster-addition","title":"Cluster Addition","text":"<p>When you add additional managed clusters, make sure to update kof's configuration:</p> <ol> <li>Apply collector template</li> <li>Verify data flow</li> <li>Configure custom dashboards</li> </ol>"},{"location":"admin-kof/#maintenance","title":"Maintenance","text":""},{"location":"admin-kof/#backup-requirements","title":"Backup Requirements","text":"<ul> <li>Grafana configurations</li> <li>Alert definitions</li> <li>Custom dashboards</li> <li>Retention policies</li> </ul>"},{"location":"admin-kof/#health-monitoring","title":"Health Monitoring","text":"<pre><code># Check collector status\nkubectl get pods -n opentelemetry-system\n\n# Verify storage components  \nkubectl get pods -n victoria-metrics\n\n# Monitor Grafana\nkubectl get pods -n grafana-system\n</code></pre>"},{"location":"admin-kof/#resource-limits","title":"Resource Limits","text":""},{"location":"admin-kof/#storage-clusters","title":"Storage Clusters","text":"<pre><code>resources:\n  vmcluster:\n    limits:\n      cpu: 4\n      memory: 8Gi\n    requests:\n      cpu: 2\n      memory: 4Gi\n</code></pre>"},{"location":"admin-kof/#collectors","title":"Collectors","text":"<pre><code>resources:\n  collector:\n    limits:\n      cpu: 1\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 512Mi\n</code></pre>"},{"location":"admin-kof/#version-compatibility","title":"Version Compatibility","text":"Component Version Notes k0rdent \u2265 1.0.0 Required for template support Kubernetes \u2265 1.24 Earlier versions untested OpenTelemetry \u2265 0.81.0 Recommended minimum VictoriaMetrics \u2265 1.91.0 Required for clustering"},{"location":"admin-prepare/","title":"Prepare k0rdent to create managed clusters on multiple providers","text":"<p>Managed clusters can be hosted on a number of different platforms. At the time of this writing, those platforms include:</p> <ul> <li>Amazon Web Services</li> <li>Microsoft Azure</li> <li>OpenStack</li> <li>VMware</li> </ul>"},{"location":"admin-prepare/#aws","title":"AWS","text":"<p>k0rdent is able to deploy managed clusters as both EC2-based Kubernetes clusters and EKS clusters. In both cases, you'll need to create the relevant credentials, and to do that you'll need to configure an IAM user. Follow these steps to make it possible to deploy to AWS:</p> <ol> <li> <p>Install k0rdent</p> <p>Follow the instructions in Install k0rdent to create a management cluster with k0rdent running.</p> </li> <li> <p>Install <code>clusterawsadm</code></p> <p>k0rdent uses the Cluster API (CAPI) to marshal clouds and infrastructures. For AWS, this means using the components from the Cluster API Provider AWS (CAPA) project. clusterawsadm, a CLI tool created by CAPA project, helps with AWS-specific tasks such as creating IAM roles and policies, as well as credential configuration. To install clusterawsadm on Ubuntu on x86 hardware, execute these commands:</p> <pre><code>curl -LO https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.7.1/clusterawsadm-linux-amd64\nsudo install -o root -g root -m 0755 clusterawsadm-linux-amd64 /usr/local/bin/clusterawsadm\n</code></pre> </li> <li> <p>Configure AWS IAM</p> <p>Next you'll need to create the IAM policies and service account k0rdent will use to take action within the AWS infrastructure. (Note that you only need to do this once.)</p> <p>The first step is to crete the IAM CloudFormation stack based on your admin user. Start by specifying the environment variables clusterawsadm will use as AWS credentials:</p> <pre><code>export AWS_REGION=&lt;EXAMPLE_AWS_REGION&gt;\nexport AWS_ACCESS_KEY_ID=&lt;EXAMPLE_ACCESS_KEY_ID&gt;\nexport AWS_SECRET_ACCESS_KEY=&lt;EXAMPLE_SECRET_ACCESS_KEY&gt;\nexport AWS_SESSION_TOKEN=&lt;YOUR_SESSION_TOKEN&gt; # Optional. If you are using Multi-Factor Auth.\n</code></pre> </li> <li> <p>Create the IAM CloudFormation stack</p> <p>Now use <code>clusterawsadm</code> to create the IAM CloudFormation stack:</p> <pre><code>clusterawsadm bootstrap iam create-cloudformation-stack\n</code></pre> </li> <li> <p>Install the AWS CLI</p> <p>With the stack in place you can create the AWS IAM user. You can do this in the UI, but it's also possible to do it from the command line using the aws CLI tool.  Start by installing it, if you haven't already:</p> <pre><code>sudo apt install unzip\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" \nunzip awscliv2.zip \nsudo ./aws/install\n</code></pre> <p>The tool recognizes the environment variables you created earlier, so there's no need to login.</p> </li> <li> <p>Create the IAM user. </p> <p>The actual <code>user-name</code> parameter is arbitrary; you can specify it as anything you like:</p> <p><pre><code>aws iam create-user --user-name k0rdentQuickstart\n</code></pre> <pre><code>{\n  \"User\": {\n      \"Path\": \"/\",\n      \"UserName\": \"k0rdentQuickstart\",\n      \"UserId\": \"EXAMPLE_USER_ID\",\n      \"Arn\": \"arn:aws:iam::FAKE_ARN_123:user/k0rdentQuickstart\",\n      \"CreateDate\": \"2025-01-18T08:15:27+00:00\"\n  }\n}\n</code></pre></p> </li> <li> <p>Assign the relevant policies</p> <p>You'll need to assign the following policies to the user you just created:</p> <p><pre><code>control-plane.cluster-api-provider-aws.sigs.k8s.io\ncontrollers.cluster-api-provider-aws.sigs.k8s.io\nnodes.cluster-api-provider-aws.sigs.k8s.io\n</code></pre> To do that, you'll need the ARNs for each policy.  You can get them with the <code>list-policies</code> command, as in:</p> <p><pre><code>aws iam list-policies --scope Local\n</code></pre> <pre><code>{\n  \"Policies\": [\n      {\n          \"PolicyName\": \"controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n          \"PolicyId\": \"ANPA22CF4NNF3VUDTMH3N\",\n          \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n          \"Path\": \"/\",\n          \"DefaultVersionId\": \"v1\",\n          \"AttachmentCount\": 2,\n          \"PermissionsBoundaryUsageCount\": 0,\n          \"IsAttachable\": true,\n          \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n          \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n      },\n      {\n          \"PolicyName\": \"nodes.cluster-api-provider-aws.sigs.k8s.io\",\n          \"PolicyId\": \"ANPA22CF4NNF5TAKL44PU\",\n          \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\",\n          \"Path\": \"/\",\n          \"DefaultVersionId\": \"v1\",\n          \"AttachmentCount\": 3,\n          \"PermissionsBoundaryUsageCount\": 0,\n          \"IsAttachable\": true,\n          \"CreateDate\": \"2025-01-01T18:47:44+00:00\",\n          \"UpdateDate\": \"2025-01-01T18:47:44+00:00\"\n      },\n      {\n          \"PolicyName\": \"controllers.cluster-api-provider-aws.sigs.k8s.io\",\n          \"PolicyId\": \"ANPA22CF4NNFVO6OHIQOE\",\n          \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\",\n          \"Path\": \"/\",\n          \"DefaultVersionId\": \"v1\",\n          \"AttachmentCount\": 3,\n          \"PermissionsBoundaryUsageCount\": 0,\n          \"IsAttachable\": true,\n          \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n          \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n      },\n      {\n          \"PolicyName\": \"control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n          \"PolicyId\": \"ANPA22CF4NNFY4FJ3DA2E\",\n          \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n          \"Path\": \"/\",\n          \"DefaultVersionId\": \"v1\",\n          \"AttachmentCount\": 2,\n          \"PermissionsBoundaryUsageCount\": 0,\n          \"IsAttachable\": true,\n          \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n          \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n      }\n  ]\n}\n</code></pre></p> <p>Now you can add the policies using the <code>attach-user-policy</code> command and the ARNs you retrieved in the previous step:</p> <pre><code>aws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\n</code></pre> </li> <li> <p>Create an access key and secret</p> <p>To access AWS as this new user, you'll need to create an access key:</p> <p><pre><code>aws iam create-access-key --user-name k0rdentQuickstart \n</code></pre> <pre><code>{\n  \"AccessKey\": {\n      \"UserName\": \"k0rdentQuickstart\",\n      \"AccessKeyId\": \"EXAMPLE_ACCESS_KEY_ID\",\n      \"Status\": \"Active\",\n      \"SecretAccessKey\": \"EXAMPLE_SECRET_ACCESS_KEY\",\n      \"CreateDate\": \"2025-01-18T08:33:35+00:00\"\n  }\n}\n</code></pre></p> </li> <li> <p>Create the IAM Credentials Secret on the k0rdent Management Cluster</p> <p>Create a YAML file called aws-cluster-identity-secret.yaml and add the following text, including the <code>AccessKeyId</code> and <code>SecretAccessKey</code> you created in the previous step:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-cluster-identity-secret\n  namespace: kcm-system\ntype: Opaque\nstringData:\n  AccessKeyID: EXAMPLE_ACCESS_KEY_ID\n  SecretAccessKey: EXAMPLE_SECRET_ACCESS_KEY\n</code></pre> <p>Apply the YAML to your cluster, making sure to add it to the namespace where the CAPA provider is running (currently <code>kcm-system</code>) so the controller can read it:</p> <pre><code>kubectl apply -f aws-cluster-identity-secret.yaml -n kcm-system\n</code></pre> </li> <li> <p>Create the <code>AWSClusterStaticIdentity</code></p> <p>Create the <code>AWSClusterStaticIdentity</code> object in a file named <code>aws-cluster-identity.yaml</code>:</p> <pre><code>kind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Notice that the <code>secretRef</code> references the <code>Secret</code> you created in the previous step.</p> <p>Apply the YAML to your cluster, again adding it to the <code>kcm-system</code> namespace.</p> <pre><code>kubectl apply -f aws-cluster-identity.yaml  -n kcm-system\n</code></pre> </li> <li> <p>Create the <code>Credential</code></p> <p>Finally, create the kcm <code>Credential</code> object, making sure to reference the <code>AWSClusterStaticIdentity</code> you just created:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: aws-cluster-identity-cred\n  namespace: kcm-system\nspec:\ndescription: \"Credential Example\"\nidentityRef:\n  apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n  kind: AWSClusterStaticIdentity\n  name: aws-cluster-identity\n</code></pre> Apply the YAML to your cluster, again keeping it in the <code>kcm-system</code> namespace:</p> <pre><code>kubectl apply -f aws-cluster-identity-cred.yaml -n kcm-system\n</code></pre> </li> <li> <p>Deploy a cluster</p> <p>Make sure everything is configured properly by creating a <code>ClusterDeployment</code>. Start with a YAML file specifying the <code>ClusterDeployment</code>, as in:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-aws-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-5\n  credential: aws-cluster-identity-cred\nconfig:\n  clusterLabels: {}\n  region: us-east-2\n  controlPlane:\n    instanceType: t3.small\n  worker:\n    instanceType: t3.small\n</code></pre> </p> <p>Note</p> <ul> <li>You're giving it an arbitrary name in <code>.metadata.name</code> (<code>my-aws-clusterdeployment1</code>)</li> <li>You're referencing the credential you created in the previous step, <code>aws-cluster-identity-cred</code>. This enables you to set up a system where users can take advantage of having access to the credentials to the AWS account without actually having those credentials in hand.</li> <li>You need to choose a template to use for the cluster, in this case <code>aws-standalone-cp-0-0-5</code>. You can get a list of available templates using:</li> </ul> </li> </ol> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n</code></pre>     Apply the YAML to your management cluster:     <pre><code>kubectl apply -f my-aws-clusterdeployment1.yaml\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-aws-clusterdeployment1 created\n</code></pre>   As before, there will be a delay as the cluster finishes provisioning. Follow the provisioning process with:     <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-aws-clusterdeployment1 --watch\n</code></pre> <pre><code>NAME                        READY   STATUS\nmy-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre>     When the cluster is <code>Ready</code>, you can access it via the kubeconfig, as in:     <pre><code>kubectl -n kcm-system get secret my-aws-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-aws-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-aws-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> 14. Cleanup</p> <p>When you've established that it's working properly. you can delete the managed cluster and its AWS objects:</p> <pre><code>kubectl delete ClusterDeployment my-aws-clusterdeployment1 \n</code></pre>"},{"location":"admin-prepare/#azure","title":"Azure","text":"<p>Standalone clusters can be deployed on Azure instances. Follow these steps to make Azure clusters available to your users:</p> <ol> <li> <p>Install k0rdent</p> <p>Follow the instructions in Install k0rdent to create a management cluster with k0rdent running.</p> </li> <li> <p>The Azure CLI</p> <p>The Azure CLI (az) is required to interact with Azure resources. You can install it on Ubuntu as follows:</p> <pre><code>curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n</code></pre> </li> <li> <p>Log in to Azure</p> <p>Run the <code>az login</code> command to authenticate your session with Azure:</p> <pre><code>az login\n</code></pre> <p>Make sure that the account you're using has at least one active subscription.</p> </li> <li> <p>Register resource providers</p> <p>In order for k0rdent to deploy and manage clusters, it needs to be able to work with Azure resources such as  compute, network, and identity. Make sure the subscription you're using has the following resource providers registered:</p> <pre><code>Microsoft.Compute\nMicrosoft.Network\nMicrosoft.ContainerService\nMicrosoft.ManagedIdentity\nMicrosoft.Authorization\n</code></pre> <p>To register these providers, run the following commands in the Azure CLI:</p> <pre><code>az provider register --namespace Microsoft.Compute\naz provider register --namespace Microsoft.Network\naz provider register --namespace Microsoft.ContainerService\naz provider register --namespace Microsoft.ManagedIdentity\naz provider register --namespace Microsoft.Authorization\n</code></pre> </li> <li> <p>Find Your Subscription ID</p> <p>Creating a managed cluster requires a structure of credentials that link to user identities on the provider system without exposing the actual username and password to users. You can find more information on k0rdent  Credentials, but for Azure, this involves creating an <code>AzureClusterIdentity</code> and a  Service Principal (SP) to let CAPZ (Cluster API Azure) communicate with the cloud provider. </p> <p>On Azure, the lowest level of this hierarchy is the subscription, which ties to your billing information Azure.Your Azure must have at least one subscription for you to use it with k0rdent, so if you're working with a new account make sure to create a new subscription with billing information before you start.</p> <p>To get the information you need, list all your Azure subscriptions: </p> <p><pre><code>az account list -o table\n</code></pre> <pre><code>Name                     SubscriptionId                        TenantId\n-----------------------  -------------------------------------  --------------------------------\nMy Azure Subscription    SUBSCRIPTION_ID_SUBSCRIPTION_ID        TENANT_ID_TENANT_ID_TENANT_ID\n</code></pre></p> <p>Make note of the <code>SubscriptionId</code> for the subscription you want to use.</p> </li> <li> <p>Create a Service Principal (SP)</p> <p>The Service Principal is like a password-protected user that CAPZ will use to manage resources on Azure. Create the Service Principal, making sure to replace  with the <code>SubscriptionId</code> from step 1. <p><pre><code>az ad sp create-for-rbac --role contributor --scopes=\"/subscriptions/&lt;SUBSCRIPTION_ID_SUBSCRIPTION_ID&gt;\"\n</code></pre> <pre><code>{\n\"appId\": \"SP_APP_ID_SP_APP_ID\",\n\"displayName\": \"azure-cli-2024-10-24-17-36-47\",\n\"password\": \"SP_PASSWORD_SP_PASSWORD\",\n\"tenant\": \"SP_TENANT_SP_TENANT\"\n}\n</code></pre> Note that this information gives you access to your Azure account, so make sure to treat these strings  like passwords. Do not share them or check them into a repository.</p> <li> <p>Use the password to create a <code>Secret</code> object</p> <p>The <code>Secret</code> stores the <code>clientSecret</code> (password) from the Service Principal. Save the <code>Secret</code> YAML in a file called <code>azure-cluster-identity-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: azure-cluster-identity-secret\n  namespace: kcm-system\nstringData:\n  clientSecret: &lt;SP_PASSWORD_SP_PASSWORD&gt; # Password retrieved from the Service Principal\ntype: Opaque\n</code></pre> <p>You can then apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity-secret.yaml\n</code></pre> </li> <li> <p>Create the <code>AzureClusterIdentity</code> objects</p> <p>The <code>AzureClusterIdentity</code> object defines the credentials CAPZ uses to manage Azure resources.  It references the <code>Secret</code> you just created, so make sure that <code>.spec.clientSecret.name</code> matches  the name of that <code>Secret</code>.</p> <p>Save the following YAML into a file named <code>azure-cluster-identity.yaml</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: AzureClusterIdentity\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/move-hierarchy: \"true\"\n  name: azure-cluster-identity\n  namespace: kcm-system\nspec:\n  allowedNamespaces: {}\n  clientID: &lt;SP_APP_ID_SP_APP_ID&gt; # The App ID retrieved from the Service Principal above in Step 2\n  clientSecret:\n    name: azure-cluster-identity-secret\n    namespace: kcm-system\n  tenantID: &lt;SP_TENANT_SP_TENANT&gt; # The Tenant ID retrieved from the Service Principal above in Step 2\n  type: ServicePrincipal\n</code></pre> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f azure-cluster-identity.yaml\n</code></pre> <pre><code>azureclusteridentity.infrastructure.cluster.x-k8s.io/azure-cluster-identity created\n</code></pre></p> </li> <li> <p>Create the k0rdent <code>Credential</code> Object</p> <p>Create the YAML for the specification of the <code>Credential</code> and save it as <code>azure-cluster-identity-cred.yaml</code>.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: azure-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: AzureClusterIdentity\n    name: azure-cluster-identity\n    namespace: kcm-system\n</code></pre> <p>You're referencing the <code>AzureClusterIdentity</code> object you just created, so make sure that <code>.spec.name</code> matches  <code>.metadata.name</code> of that object. Also, note that while the overall object's <code>kind</code> is <code>Credential</code>, the  <code>.spec.identityRef.kind</code> must be <code>AzureClusterIdentity</code> to match that object.</p> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f azure-cluster-identity-cred.yaml\n</code></pre> <pre><code>credential.k0rdent.mirantis.com/azure-cluster-identity-cred created\n</code></pre></p> </li> <p>Now you're ready to deploy the cluster.</p> <ol> <li> <p>Create a <code>ClusterDeployment</code></p> <p>To test the configuration, deploy a managed cluster by following these steps:</p> <p>First get a list of available locations/regions:</p> <p><pre><code>az account list-locations -o table\n</code></pre> <pre><code>DisplayName               Name                 RegionalDisplayName\n------------------------  -------------------  -------------------------------------\nEast US                   eastus               (US) East US\nSouth Central US          southcentralus       (US) South Central US\nWest US 2                 westus2              (US) West US 2\nWest US 3                 westus3              (US) West US 3\nAustralia East            australiaeast        (Asia Pacific) Australia East\n\u2026\n</code></pre></p> <p>Make note of the location you want to use, such as <code>eastus</code>.</p> <p>To create the actual managed cluster, create a <code>ClusterDeployment</code> that references the appropriate template as well as the location, credentials, and <code>subscriptionId</code>.</p> <p>You can see the available templates by listing them:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>Create the yaml:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-azure-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: azure-standalone-cp-0-0-5\n  credential: azure-cluster-identity-cred\n  config:\n    location: \"westus\" # Select your desired Azure Location (find it via `az account list-locations -o table`)\n    subscriptionID: &lt;SUBSCRIPTION_ID_SUBSCRIPTION_ID&gt; # Enter the Subscription ID used earlier\n    controlPlane:\n      vmSize: Standard_A4_v2\n    worker:\n      vmSize: Standard_A4_v2\n</code></pre> <p>Apply the YAML to your management cluster:</p> <p><pre><code>kubectl apply -f my-azure-clusterdeployment1.yaml\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-azure-clusterdeployment1 created\n</code></pre></p> <p>Note that although the <code>ClusterDeployment</code> object has been created, there will be a delay as actual Azure instances are provisioned and added to the cluster. You can follow the provisioning process:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-azure-clusterdeployment1 --watch\n</code></pre> <p>If the provisioning process continues for a more than a few minutes, check to make sure k0rdent isn't trying to exceed your quotas. If you are near the top of your quotas, requesting an increase can \"unstick\" the provisioning process.</p> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-azure-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-azure-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-azure-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>To clean up Azure resources, delete the managed cluster by deleting the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl get ClusterDeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-azure-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre> <pre><code>kubectl delete ClusterDeployment my-azure-clusterdeployment1 -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-azure-clusterdeployment1\" deleted\n</code></pre></p> </li> </ol>"},{"location":"admin-prepare/#openstack","title":"OpenStack","text":"<p>k0rdent is able to deploy managed clusters on OpenStack virtual machines. Follow these steps to make it possible to deploy to OpenStack:</p> <ol> <li> <p>Install k0rdent</p> <p>Follow the instructions in Install k0rdent to create a management cluster with k0rdent running.</p> </li> <li> <p>OpenStack CLI (optional)</p> <p>If you plan to access OpenStack directly, go ahead and  install the OpenStack CLI.</p> </li> <li> <p>Configure the OpenStack Application Credential</p> <p>This credential should include:</p> <pre><code>OS_AUTH_URL\nOS_APPLICATION_CREDENTIAL_ID\nOS_APPLICATION_CREDENTIAL_SECRET\nOS_REGION_NAME\nOS_INTERFACE\nOS_IDENTITY_API_VERSION\nOS_AUTH_TYPE (e.g. v3applicationcredential)\n</code></pre> <p>While it's possible to use a username and password instead of the Application Credential \u2014 adjust your YAML accordingly \u2014 an  Application Credential is strongly recommended because it limits scope and improves security over a raw username/password approach.</p> </li> <li> <p>Create the OpenStack Credentials Secret</p> <p>Create a Kubernetes <code>Secret</code> containing the <code>clouds.yaml</code> that defines your OpenStack environment, substituting real values where appropriate. Save this as <code>openstack-cloud-config.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: openstack-cloud-config\n  namespace: kcm-system\nstringData:\n  clouds.yaml: |\n    clouds:\n      openstack:\n        auth:\n          auth_url: &lt;OS_AUTH_URL&gt;\n          application_credential_id: &lt;OS_APPLICATION_CREDENTIAL_ID&gt;\n          application_credential_secret: &lt;OS_APPLICATION_CREDENTIAL_SECRET&gt;\n        region_name: &lt;OS_REGION_NAME&gt;\n        interface: &lt;OS_INTERFACE&gt;\n        identity_api_version: &lt;OS_IDENTITY_API_VERSION&gt;\n        auth_type: &lt;OS_AUTH_TYPE&gt;\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f openstack-cloud-config.yaml\n</code></pre> </li> <li> <p>Create the k0rdent Credential Object</p> <p>Next, define a <code>Credential</code> that references the <code>Secret</code> from the previous step. Save this as <code>openstack-cluster-identity-cred.yaml</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: openstack-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  description: \"OpenStack credentials\"\n  identityRef:\n    apiVersion: v1\n    kind: Secret\n    name: openstack-cloud-config\n    namespace: kcm-system\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f openstack-cluster-identity-cred.yaml\n</code></pre> <p>Note that <code>.spec.identityRef.name</code> must match the <code>Secret</code> you created in the previous step, and  <code>.spec.identityRef.namespace</code> must be the same as the <code>Secret</code>\u2019s namespace (<code>kcm-system</code>).</p> </li> <li> <p>Create Your First Managed Cluster</p> <p>To test the configuration, create a YAML file with the specification of your Managed Cluster and save it as <code>my-openstack-cluster-deployment.yaml</code>.  Note that you can see the available templates by listing them:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n</code></pre></p> <p>The <code>ClusterDeployment</code> should look something like this:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-openstack-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: openstack-standalone-cp-0-0-1\n  credential: openstack-cluster-identity-cred\n  config:\n    controlPlaneNumber: 1\n    workersNumber: 1\n    controlPlane:\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    worker:\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    authURL: &lt;OS_AUTH_URL&gt;\n</code></pre> You can adjust <code>flavor</code>, <code>image</code> name, and <code>authURL</code> to match your OpenStack environment. For more information about the configuration options, see the OpenStack Template Parameters Reference.</p> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f my-openstack-cluster-deployment.yaml\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. You can follow the provisioning process:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-openstack-cluster-deployment --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig, just like any other Kubernetes cluster:</p> <pre><code>kubectl -n kcm-system get secret my-openstack-cluster-deployment-kubeconfig -o jsonpath='{.data.value&gt;' | base64 -d &gt; my-openstack-cluster-deployment-kubeconfig.kubeconfig\nKUBECONFIG=\"my-openstack-cluster-deployment-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>To clean up OpenStack resources, delete the managed cluster by deleting the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl get ClusterDeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-openstack-cluster-deployment   True    ClusterDeployment is ready\n</code></pre> <pre><code>kubectl delete ClusterDeployment my-openstack-cluster-deployment -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-openstack-cluster-deployment\" deleted\n</code></pre></p> </li> </ol>"},{"location":"admin-prepare/#vsphere","title":"vSphere","text":"<p>To enable users to deploy managed clusers on vSphere, follow these steps:</p> <ol> <li> <p>Create a k0rdent management cluster</p> <p>Follow the instructions in Install k0rdent to create a management cluster with k0rdent running, as well as a local install of <code>kubectl</code>.</p> </li> <li> <p>Install a vSphere instance version <code>6.7.0</code> or higher.</p> </li> <li> <p>Create a vSphere account with appropriate privileges</p> <p>To function properly, the user assigned to the vSphere Provider should be able to manipulate vSphere resources. The user should have the following  required privileges:</p> <pre><code>Virtual machine: Full permissions are required\nNetwork: Assign network is sufficient\nDatastore: The user should be able to manipulate virtual machine files and metadata\n</code></pre> <p>In addition to that, specific CSI driver permissions are required. See the official doc for more information on CSI-specific permissions.</p> </li> <li> <p>Image template</p> <p>You can use pre-built image templates from the CAPV project or build your own.</p> <p>When building your own image, make sure that VMware tools and cloud-init are installed and properly configured.</p> <p>You can follow the official open-vm-tools guide on how to correctly install VMware tools.</p> <p>When setting up cloud-init, you can refer to the official docs and specifically the VMware datasource docs for extended information regarding cloud-init on vSphere.</p> </li> <li> <p>vSphere network</p> <p>When creating a network, make sure that it has the DHCP service.</p> <p>Also, ensure that part of your network is out of the DHCP range (for example, the network <code>172.16.0.0/24</code> should have a DHCP range of <code>172.16.0.100-172.16.0.254</code> only) so that LoadBalancer services will not create any IP conflicts in the network.</p> </li> <li> <p>vSphere Credentials</p> <p>To enable k0rdent to access vSphere resources, create the appropriate credentials objects. For a full explanation of how Credentials work, see the main Credentials chapter but for now, follow these steps:</p> <p>Create a <code>Secret</code> Object with the username and password</p> <p>The <code>Secret</code> stores the username and password for your vSphere instance. Save the <code>Secret</code> YAML in a file named <code>vsphere-cluster-identity-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: vsphere-cluster-identity-secret\n  namespace: kcm-system\nstringData:\n  username: &lt;USERNAME&gt;\n  password: &lt;PASSWORD&gt;\ntype: Opaque\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity-secret.yaml\n</code></pre> </li> <li> <p>Create the <code>VSphereClusterIdentity</code> Object</p> <p>The <code>VSphereClusterIdentity</code> object defines the credentials CAPV will use to manage vSphere resources.</p> <p>Save the <code>VSphereClusterIdentity</code> YAML into a file named <code>vsphere-cluster-identity.yaml</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: VSphereClusterIdentity\nmetadata:\n  name: vsphere-cluster-identity\nspec:\n  secretName: vsphere-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>The <code>VSphereClusterIdentity</code> object references the <code>Secret</code> you created in the previous step, so <code>.spec.secretName</code>  needs to match the <code>.metadata.name</code> for the <code>Secret</code>.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity.yaml\n</code></pre> </li> <li> <p>Create the <code>Credential</code> Object</p> <p>Create a YAML with the specification of our credential and save it as <code>vsphere-cluster-identity-cred.yaml</code></p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: vsphere-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: VSphereClusterIdentity\n    name: vsphere-cluster-identity\n</code></pre> Again, <code>.spec.identityRef.name</code> must match the <code>.metadata.name</code> of the <code>VSphereClusterIdentity</code> object you just created.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity-cred.yaml\n</code></pre> </li> <li> <p>Create your first Cluster Deployment</p> <p>Test the configuration by deploying a cluster. Create a YAML document with the specification of your Cluster Deployment and save it as <code>my-vsphere-clusterdeployment1.yaml</code>.</p> <p>You can get a list of available templates:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n</code></pre></p> <p>The <code>ClusterDeployment</code> YAML file should look something like this. Make sure the replace the placeholders with your specific information:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-vsphere-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: vsphere-standalone-cp-0-0-5\n  credential: vsphere-cluster-identity-cred\n  config:\n    vsphere:\n      server: &lt;VSPHERE_SERVER&gt;\n      thumbprint: &lt;VSPHERE_THUMBPRINT&gt;\n      datacenter: &lt;VSPHERE_DATACENTER&gt;\n      datastore: &lt;VSPHERE_DATASTORE&gt;\n      resourcePool: &lt;VSPHERE_RESOURCEPOOL&gt;\n      folder: &lt;VSPHERE_FOLDER&gt;\n    controlPlaneEndpointIP: &lt;VSPHERE_CONTROL_PLANE_ENDPOINT&gt;\n    controlPlane:\n      ssh:\n        user: ubuntu\n        publicKey: &lt;VSPHERE_SSH_KEY&gt;\n      vmTemplate: &lt;VSPHERE_VM_TEMPLATE&gt;\n      network: &lt;VSPHERE_NETWORK&gt;\n    worker:\n      ssh:\n        user: ubuntu\n        publicKey: &lt;VSPHERE_SSH_KEY&gt;\n      vmTemplate: &lt;VSPHERE_VM_TEMPLATE&gt;\n      network: &lt;VSPHERE_NETWORK&gt;\n</code></pre> <p>For more information about the available configuration options, see the vSphere Template Parameters.</p> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f my-vsphere-clusterdeployment1.yaml\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-vsphere-clusterdeployment1 --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig, like this:</p> <pre><code>kubectl -n kcm-system get secret my-vsphere-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-vsphere-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-vsphere-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>To delete the provisioned cluster and free consumed vSphere resources run:</p> <pre><code>kubectl -n kcm-system delete cluster my-vsphere-clusterdeployment1\n</code></pre> </li> </ol>"},{"location":"admin-rbac/","title":"Role Based Access Control","text":"<p>k0rdent provides the opportunity to use Role Based Access Control in order to try to use the principle of least privilege and only give users access to the objects and resources they absolutely have to have.</p>"},{"location":"admin-rbac/#what-roles-do","title":"What roles do","text":"<p>k0rdent leverages the Kubernetes RBAC system and provides a set of standard <code>ClusterRoles</code> with associated permissions. These standard <code>ClusterRoles</code> are created as part of the k0rdent helm chart. k0rdent roles are based on labels and aggregated permissions, meaning they automatically collect rules from other <code>ClusterRoles</code> with specific labels.</p> <p>The following table outlines the roles available in k0rdent, along with their respective read/write or read-only permissions:</p> Roles Global Admin Global Viewer Namespace Admin Namespace Editor Namespace Viewer Scope Global Global Namespace Namespace Namespace k0rdent management r/w r/o - - - Namespaces management r/w r/o - - - Provider Templates r/w r/o - - - Global Template Management r/w r/o - - - Multi Cluster Service Management r/w r/o - - - Template Chain Management r/w r/o r/w r/o r/o Cluster and Service Templates r/w r/o r/w r/o r/o Credentials r/w r/o r/w r/o r/o Flux Helm objects r/w r/o r/w r/o r/o Cluster Deployments r/w r/o r/w r/w r/o <p>This section provides an overview of all <code>ClusterRoles</code> available in k0rdent.</p>"},{"location":"admin-rbac/#roles-summary","title":"Roles summary","text":"<p>Note</p> <p> The names of the <code>ClusterRoles</code> may have different prefix depending on the name of the k0rdent Helm chart. The <code>ClusterRoles</code> definitions below use the <code>kcm</code> prefix, which is the default name of the k0rdent Helm chart.</p>"},{"location":"admin-rbac/#global-admin","title":"Global Admin","text":"<p>The <code>Global Admin</code> role provides full administrative access across all the k0rdent system.</p> <p>Name: <code>kcm-global-admin-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRoles</code> with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-global-admin: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-admin: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-editor: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Full access to k0rdent API</li> <li>Full access to Flux Helm repositories and Helm charts</li> <li>Full access to Cluster API identities</li> <li>Full access to namespaces and secrets</li> </ol> <p>Use case</p> <p>A user with the <code>Global Admin</code> role is authorized to perform the following actions:</p> <ol> <li>Manage the k0rdent configuration</li> <li>Manage namespaces in the management cluster</li> <li>Manage <code>Provider Templates</code>: add new templates or remove unneeded ones</li> <li>Manage <code>Cluster</code> and <code>Service Templates</code> in any namespace, including adding and removing templates</li> <li>Manage Flux <code>HelmRepositories</code> and <code>HelmCharts</code> in any namespace</li> <li>Manage access rules for <code>Cluster</code> and <code>Service Templates</code>, including distributing templates across namespaces using    <code>Template Chains</code></li> <li>Manage upgrade sequences for <code>Cluster</code> and <code>Service Templates</code></li> <li>Manage and deploy Services across multiple clusters in any namespace by modifying <code>MultiClusterService</code> resources</li> <li>Manage <code>ClusterDeployments</code> in any namespace</li> <li>Manage <code>Credentials</code> and <code>secrets</code> in any namespace</li> <li>Upgrade k0rdent</li> <li>Uninstall k0rdent</li> </ol>"},{"location":"admin-rbac/#global-viewer","title":"Global Viewer","text":"<p>The <code>Global Viewer</code> role grants read-only access across the k0rdent system. It does not permit any modifications, including the creation of clusters.</p> <p>Name: <code>kcm-global-viewer-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRoles</code> with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-global-viewer: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-viewer: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Read access to k0rdent API</li> <li>Read access to Flux Helm repositories and Helm charts</li> <li>Read access to Cluster API identities</li> <li>Read access to namespaces and secrets</li> </ol> <p>Use case</p> <p>A user with the <code>Global Viewer</code> role is authorized to perform the following actions:</p> <ol> <li>View the k0rdent configuration</li> <li>List namespaces available in the management cluster</li> <li>List and get the detailed information about available <code>Provider Templates</code></li> <li>List available <code>Cluster</code> and <code>Service Templates</code> in any namespace</li> <li>List and view detailed information about Flux <code>HelmRepositories</code> and <code>HelmCharts</code> in any namespace</li> <li>View access rules for <code>Cluster</code> and <code>Service Templates</code>, including <code>Template Chains</code> in any namespace</li> <li>View full details about the created <code>MultiClusterService</code> objects</li> <li>List and view detailed information about <code>ClusterDeployments</code> in any namespace</li> <li>List and view detailed information about created <code>Credentials</code> and <code>secrets</code> in any namespace</li> </ol>"},{"location":"admin-rbac/#namespace-admin","title":"Namespace Admin","text":"<p>The <code>Namespace Admin</code> role provides full administrative access within a namespace.</p> <p>Name: <code>kcm-namespace-admin-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRoles</code> with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-admin: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-editor: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Full access to <code>ClusterDeployments</code>, <code>Credentials</code>, <code>Cluster</code> and <code>Service Templates</code> in the namespace</li> <li>Full access to <code>Template Chains</code> in the namespace</li> <li>Full access to Flux <code>HelmRepositories</code> and <code>HelmCharts</code> in the namespace</li> </ol> <p>Use case</p> <p>A user with the <code>Namespace Admin</code> role is authorized to perform the following actions within the namespace:</p> <ol> <li>Create and manage all <code>ClusterDeployments</code> in the namespace</li> <li>Create and manage <code>Cluster</code> and <code>Service Templates</code> in the namespace</li> <li>Manage the distribution and upgrade sequences of Templates within the namespace</li> <li>Create and manage Flux <code>HelmRepositories</code> and <code>HelmCharts</code> in the namespace</li> <li>Manage <code>Credentials</code> created by any user in the namespace</li> </ol>"},{"location":"admin-rbac/#namespace-editor","title":"Namespace Editor","text":"<p>The <code>Namespace Editor</code> role allows users to create and modify <code>ClusterDeployments</code> within namespace using predefined <code>Credentials</code> and <code>Templates</code>.</p> <p>Name: <code>kcm-namespace-editor-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRoles</code> with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-editor: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Full access to <code>ClusterDeployments</code> in the allowed namespace</li> <li>Read access to <code>Credentials</code>, <code>Cluster</code> and <code>Service Templates</code>, and <code>TemplateChains</code> in the namespace</li> <li>Read access to Flux <code>HelmRepositories</code> and <code>HelmCharts</code> in the namespace</li> </ol> <p>Use case</p> <p>A user with the <code>Namespace Editor</code> role has the following permissions in the namespace:</p> <ol> <li>Can create and manage <code>ClusterDeployment</code> objects in the namespace using existing <code>Credentials</code> and <code>Templates</code></li> <li>Can list and view detailed information about the <code>Credentials</code> available in the namespace</li> <li>Can list and view detailed information about the available <code>Cluster</code> and <code>Service Templates</code> and the <code>Templates'</code>    upgrade sequences</li> <li>Can list and view detailed information about the Flux <code>HelmRepositories</code> and <code>HelmCharts</code></li> </ol>"},{"location":"admin-rbac/#namespace-viewer","title":"Namespace Viewer","text":"<p>The <code>Namespace Viewer</code> role grants read-only access to resources within a namespace.</p> <p>Name: <code>kcm-namespace-viewer-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRoles</code> with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-viewer: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Read access to <code>ClusterDeployments</code> in the namespace</li> <li>Read access to <code>Credentials</code>, <code>Cluster</code> and <code>Service Templates</code>, and <code>TemplateChains</code> in the namespace</li> <li>Read access to Flux <code>HelmRepositories</code> and <code>HelmCharts</code> in the namespace</li> </ol> <p>Use case</p> <p>A user with the <code>Namespace Viewer</code> role has the following permissions in the namespace:</p> <ol> <li>Can list and view detailed information about all the <code>ClusterDeployment</code> objects in the allowed namespace</li> <li>Can list and view detailed information about <code>Credentials</code> available in the specific namespace</li> <li>Can list and view detailed information about available <code>Cluster</code> and <code>Service Templates</code> and the <code>Templates'</code>    upgrade sequences</li> <li>Can list and view detailed information about Flux <code>HelmRepositories</code> and <code>HelmCharts</code></li> </ol>"},{"location":"admin-service-templates/","title":"Creating and Deploying Applications","text":"<p>Deploying an application, like deploying a cluster, involves applying a template to the management cluster. Rather than a <code>ClusterTemplate</code>, however, applications are deployed using a <code>ServiceTemplate</code>.</p> <p>You can find more information on Bringing Your Own Templates in theTemplate Guide, but this section gives you an idea of how to create a <code>ServiceTemplate</code> and use it to deploy an application to a k0rdent managed cluster.</p> <p>The basic sequence looks like this:</p> <ol> <li> <p>Define the source of the Helm chart that defines the service.  For example, this YAML describes a custom <code>Source</code> object of <code>kind</code> <code>HelmRepository</code>:</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: custom-templates-repo\n  namespace: kcm-system\nspec:\n  insecure: true\n  interval: 10m0s\n  provider: generic\n  type: oci\n  url: oci://ghcr.io/external-templates-repo/charts\n</code></pre> </li> <li> <p>Create the <code>ServiceTemplate</code></p> <p>A template can either define a Helm chart directly using the template's <code>spec.helm.chartSpec</code> field or reference its location using the <code>spec.helm.chartRef</code> field.</p> <p>For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ServiceTemplate\nmetadata:\n  name: project-ingress-nginx-4.11.3\n  namespace: my-target-namespace\nspec:\n  helm:\n    chartSpec:\n      chart: ingress-nginx\n      version: 4.11.3\n      interval: 10m0s\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-templates\n</code></pre> <p>In this case, we're creating a <code>ServiceTemplate</code> called <code>ingress-nginx-4.11.3</code> in the <code>my-target-namespace</code> namespace.  It references version 4.11.3 of the <code>ingress-nginx</code> chart located in the <code>k0rdent-templates</code> Helm repository.</p> <p>For more information on creating templates, see the Template Guide.</p> </li> <li> <p>Create a <code>ServiceTemplateChain</code></p> <p>To let k0rdent know where this <code>ServiceTemplate</code> can and can't be used, create a <code>ServiceTemplateChain</code> object, as in:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ServiceTemplateChain\nmetadata:\n  name: project-ingress-nginx-4.11.3\n  namespace: my-target-namespace\nspec:\n  supportedTemplates:\n    - name: project-ingress-nginx-4.11.3\n    - name: project-ingress-nginx-4.10.0\n      availableUpgrades:\n        - name: project-ingress-nginx-4.11.3\n</code></pre> <p>Here you see a template called <code>project-ingress-nginx-4.11.3</code> that is meant to be deployed in the <code>my-target-namespace</code> namespace. The <code>.spec.helm.chartSpec</code> specifies the name of the Helm chart and where to find it, as well as the version and other  important information. The <code>ServiceTempateChain</code> shows that this template is also an upgrade path from version 4.10.0.</p> <p>If you wanted to deploy this as an application, you would first go ahead and add it to the cluster in which you were working, so if you were to save this YAML to a file called <code>project-ingress.yaml</code> you could run this command on the management cluster:</p> <pre><code>kubectl apply -f project-ingress.yaml -n my-target-namespace\n</code></pre> </li> <li> <p>Adding a <code>Service</code> to a <code>ClusterDeployment</code></p> <p>To add the service defined by this template to a cluster, you would simply add it to the <code>ClusterDeployment</code> object when you create it, as in:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-managed-cluster\n  namespace: tenant42\nspec:\n  config:\n    clusterLabels: {}\n  template: aws-standalone-cp-0-0-3\n  credential: aws-credential\n  services:\n    - template: project-ingress-nginx-4.11.3\n      name: ingress-nginx\n      namespace: my-target-namespace\n  servicesPriority: 100\n  stopOnConflict: false\n</code></pre> As you can see, you're simply referencing the template in the <code>.spec.services.template</code> field of the <code>ClusterDeployment</code> to tell k0rdent that you want this service to be part of this cluster.</p> <p>If you wanted to add this service to an existing cluster, you would simply patch the definition of the <code>ClusterDeployment</code>, as in:</p> <p><pre><code>kubectl patch clusterdeployment my-managed-cluster -n my-target-namespace --type='merge' -p '{\"spec\":{\"services\":[{\"template\":\"project-ingress-nginx-4.11.3\",\"name\":\"ingress-nginx\",\"namespace\":\"my-target-namespace\"}]}}'\n</code></pre> For more information on creating and using <code>ServiceTemplate</code>s, see the User Guide.</p> </li> </ol>"},{"location":"admin-troubleshooting-aws-vpcs/","title":"AWS VPC Not Removed When Deleting EKS Cluster","text":"<p>A bug has been fixed in CAPA (Cluster API Provider AWS) for VPC removal: kubernetes-sigs/cluster-api-provider-aws#5192</p> <p>If you find that a VPC has not been deleted, you can deal with it in three different ways:</p>"},{"location":"admin-troubleshooting-aws-vpcs/#applying-ownership-information-on-vpcs","title":"Applying ownership information on VPCs","text":"<p>When VPCs have owner information, all AWS resources will be removed when the k0rdent EKS cluster is deleted. So after provisioning an EKS cluster, the operator can go and set tags (i.e. <code>tag:Owner</code>) and it will be  sufficient for CAPA to manage them.</p>"},{"location":"admin-troubleshooting-aws-vpcs/#guardduty-vpce","title":"GuardDuty VPCE","text":"<p>Another way to prevent an issue with non-deleted VPCs is to disable GuardDuty. GuardDuty creates an extra VPCE (VPC Endpoint) not managed by CAPA and when CAPA  starts EKS cluster removal, this VPCE does not get removed.</p>"},{"location":"admin-troubleshooting-aws-vpcs/#manual-removal-of-vpcs","title":"Manual removal of VPCs","text":"<p>When it is impossible to turn off GuardDuty or applying ownership tags is not permitted, you need to remove VPCs manually. Follow these steps.</p> <ol> <li> <p>Look for the affected VPC. The sign of \u201cstuck\u201d VPC looks like a hidden \u201cDelete\u201d button. </p> </li> <li> <p>Open \u201cNetwork Interfaces\u201d and attempt to detach an interface. You will see a disabled \u201cDetach\u201d button: </p> </li> <li> <p>It is required to get to VPC endpoints screen and remove the end-point:  </p> </li> <li> <p>OK Endpoint deletion </p> </li> <li> <p>Wait until VPCE is completely removed, all network interfaces disappear. </p> </li> <li> <p>Now VPC can be finally removed: </p> </li> </ol>"},{"location":"admin-upgrading-k0rdent/","title":"Upgrading k0rdent","text":"<p>Upgrading k0rdent involves making upgrades to the <code>Management</code> object. To do that, you must have the <code>Global Admin</code> role. (For detailed information about k0rdent RBAC roles and permissions, refer to the RBAC documentation.) Follow these steps to upgrade k0rdent:</p> <ol> <li> <p>Create a new <code>Release</code> object</p> <p>Start by creating a <code>Release</code> object in the management cluster the points to the desired version. You can see available versions at https://github.com/k0rdent/kcm/releases.  The actual <code>Release</code> object includes information on the templates and resources that are available, as well as the version of the Kubernetes Cluster API.  For example, the v0.0.7 <code>Release</code> object looks like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Release\nmetadata:\n  name: kcm-0-0-7\nannotations:\n  helm.sh/resource-policy: keep\nlabels:\n  k0rdent.mirantis.com/component: kcm\nspec:\n  version: 0.0.7\n  kcm:\n    template: kcm-0-0-7\n  capi:\n    template: cluster-api-0-0-6\n  providers:\n    - name: k0smotron\n      template: k0smotron-0-0-6\n    - name: cluster-api-provider-azure\n      template: cluster-api-provider-azure-0-0-4\n    - name: cluster-api-provider-vsphere\n      template: cluster-api-provider-vsphere-0-0-5\n    - name: cluster-api-provider-aws\n      template: cluster-api-provider-aws-0-0-4\n    - name: cluster-api-provider-openstack\n      template: cluster-api-provider-openstack-0-0-1\n    - name: projectsveltos\n      template: projectsveltos-0-45-0\n</code></pre> <p>Thankfully, you don't have to build these YAML files yourself. Once you've chosen a release, you can go ahead and create the release object by referencing the YAML file online, as in:</p> <pre><code>VERSION=v0.0.7\nkubectl create -f https://github.com/k0rdent/kcm/releases/download/${VERSION}/release.yaml\n</code></pre> </li> <li> <p>List Available <code>Releases</code></p> <p>Once you've created the new <code>Release</code> you need to update the <code>Management</code> object to use it. Start by viewing all available <code>Release</code>s:</p> <pre><code>kubectl get releases\n</code></pre> <pre><code>NAME        AGE\nkcm-0-0-6   71m\nkcm-0-0-7   65m\n</code></pre> </li> <li> <p>Patch the <code>Management</code> object with the new <code>Release</code></p> <p>Update the <code>spec.release</code> field in the <code>Management</code> object to point to the new release. Replace <code>&lt;release-name&gt;</code> with the name of your desired release:</p> <pre><code>RELEASE_NAME=kcm-0-0-7\nkubectl patch management.kcm kcm --patch \"{\\\"spec\\\":{\\\"release\\\":\\\"${RELEASE_NAME}\\\"}}\" --type=merge\n</code></pre> </li> <li> <p>Verify the Upgrade</p> <p>Although the change will be made immediately, it will take some time for k0rdent to update the components it should be using. Monitor the readiness of the <code>Management</code> object to ensure the upgrade was successful. For example:</p> <pre><code>kubectl get management.kcm kcm\nNAME   READY   RELEASE     AGE\nkcm    True    kcm-0-0-7   4m34s\n</code></pre> </li> </ol>"},{"location":"appendix-airgap/","title":"Air-gapped Installation Guide","text":"<p>Warning</p> <p> Currently only the vSphere infrastructure provider supports full air-gapped installation.</p>"},{"location":"appendix-airgap/#prerequisites","title":"Prerequisites","text":"<p>In order to install k0rdent in an air-gapped environment, you will need the following:</p> <ul> <li>An installed k0s cluster that will be used as the management cluster.  If you   do not yet have a k0s cluster, you can follow the Airgapped Installation   documentation.  While k0rdent supports any certified Kubernetes distribution,    we reccomend k0s for airgapped installations because it   implements an OCI image bundle watcher that enables k0s to easily use a bundle   of management cluster images. </li> <li>The <code>KUBECONFIG</code> of the management cluster that will be the target for the k0rdent   installation.</li> <li> <p>A registry that is accessible from the airgapped hosts to store the k0rdent images.   If you do not have a registry you can deploy a local Docker registry   or use mindthegap</p> <p>Warning</p> <p> If using a local Docker registry, ensure the registry URL is added to the <code>insecure-registries</code> key within the Docker <code>/etc/docker/daemon.json</code> file. <pre><code>{\n  \"insecure-registries\": [\"&lt;registry-url&gt;\"]\n}\n</code></pre></p> </li> <li> <p>A registry and associated chart repository for hosting k0rdent charts.  At this   time all k0rdent charts MUST be hosted in a single OCI chart repository.  See   Use OCI-based registries in the   Helm documentation for more information.</p> </li> <li>jq, Helm and Docker binaries   installed on the machine where the <code>airgap-push.sh</code> script will be run.</li> </ul>"},{"location":"appendix-airgap/#installation","title":"Installation","text":"<p>Follow these steps to perform an airgapped installation:</p> <ol> <li> <p>Download the kcm airgap bundle. This bundle contains the following: </p> <ul> <li><code>images/kcm-images-&lt;version&gt;.tgz</code> - The image bundle tarball for the   management cluster, this bundle will be loaded into the management   cluster.</li> <li><code>images/kcm-extension-images-&lt;version&gt;.tgz</code> - The image bundle tarball for   the managed clusters, this bundle will be pushed to a registry where the   images can be accessed by the managed clusters.</li> <li><code>charts</code> - Contains the kcm Helm chart, dependency charts and k0s   extensions charts within the <code>extensions</code> directory.  All of these charts   will be pushed to a chart repository within a registry.</li> <li><code>scripts/airgap-push.sh</code> - A script that will aid in re-tagging and   pushing the <code>ClusterDeployment</code> required charts and images to a desired   registry.</li> </ul> </li> <li> <p>Extract and use the <code>airgap-push.sh</code> script to push the <code>extensions</code> images    and <code>charts</code> contents to the registry.  Ensure you have logged into the    registry using both <code>docker login</code> and <code>helm registry login</code> before running    the script.</p> <pre><code>tar xvf kcm-airgap-&lt;version&gt;.tgz scripts/airgap-push.sh\n./scripts/airgap-push.sh -r &lt;registry&gt; -c &lt;chart-repo&gt; -a kcm-airgap-&lt;version&gt;.tgz\n</code></pre> </li> <li> <p>Next, extract the <code>management</code> bundle tarball and sync the images to the    k0s cluster that will host the management cluster.  See Sync the Bundle File    for more information.</p> <p>Note</p> <p> Multiple image bundles can be placed in the <code>/var/lib/k0s/images</code> directory for k0s to use, so the existing <code>k0s</code> airgap bundle does not need to be merged into the <code>kcm-images-&lt;version&gt;.tgz</code> bundle.</p> <pre><code>tar -C /var/lib/k0s -xvf kcm-airgap-&lt;version&gt;.tgz \"images/kcm-images-&lt;version&gt;.tgz\"\n</code></pre> </li> <li> <p>Install the kcm Helm chart on the management cluster from the registry where    the kcm charts were pushed.  The kcm controller image is loaded as part of    the airgap <code>management</code> bundle and does not need to be customized within the    Helm chart, but the default chart repository configured via    <code>controller.defaultRegistryURL</code> should be set to reference the repository    where charts have been pushed.</p> <pre><code>helm install kcm oci://&lt;chart-repository&gt;/kcm \\\n  --version &lt;version&gt; \\\n  -n kcm-system \\\n  --create-namespace \\\n  --set controller.defaultRegistryURL=oci://&lt;chart-repository&gt;\n</code></pre> </li> <li> <p>Edit the <code>Management</code> object to add the airgap parameters.</p> <p>Note</p> <p> Use <code>insecureRegistry</code> parameter only if you have a plain HTTP registry.</p> <p>The resulting yaml will look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  core:\n    capi:\n      config:\n        airgap: true\n    kcm:\n      config:\n        controller:\n          defaultRegistryURL: oci://&lt;registry-url&gt;\n          insecureRegistry: true\n  providers:\n  - config:\n      airgap: true\n    name: k0smotron\n  - config:\n      airgap: true\n    name: cluster-api-provider-vsphere\n  - name: projectsveltos\n  release: &lt;release name&gt;\n</code></pre> </li> <li> <p>Place the k0s binary and airgap bundle on an internal server so they're    available over HTTP. This is required for the airgap provisioning process,    because k0s components must be downloaded at each node upon creation.    Alternatively, you can create the following example deployment using the k0s    image provided in the bundle.</p> <p>Note</p> <p> The k0s image version is the same that is defined in the vSphere template by default.</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k0s-ag-image\n  labels:\n    app: k0s-ag-image\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k0s-ag-image\n  template:\n    metadata:\n      labels:\n        app: k0s-ag-image\n    spec:\n      containers:\n      - name: k0s-ag-image\n        image: k0s-ag-image:v1.31.1-k0s.1\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: k0s-ag-image\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: k0s-ag-image\n  type: NodePort\n</code></pre> </li> </ol>"},{"location":"appendix-airgap/#creation-of-the-clusterdeployment","title":"Creation of the ClusterDeployment","text":"<p>In order to successfully deploy a cluster in an airgapped configuration, several configuration options must be properly defined in the <code>.spec.config</code> of the `ClusterDeployment.</p> <p>You must specify the custom image registry and chart repository to be used (the registry and chart repository where the <code>extensions</code> bundle and charts were pushed).</p> <p>Apart from that, you must provide an endpoint where the k0s binary and airgap bundle can be downloaded (step <code>6</code> of the installation procedure).</p> <pre><code>spec:\n config:\n   airgap: true\n   k0s:\n     downloadURL: \"http://&lt;k0s binary endpoint&gt;/k0s\"\n     bundleURL: \"http://&lt;k0s binary endpoint&gt;/k0s-airgap-bundle\"\n   extensions:\n    imageRepository: ${IMAGE_REPOSITORY}\n    chartRepository: ${CHART_REPOSITORY}\n</code></pre>"},{"location":"appendix-dryrun/","title":"Understanding Dry Run mode","text":"<p>The <code>ClusterDeployment</code> process includes a \"dry run\" mode, which enables you to validate your configuration without actually provisioning resources. By default, <code>.spec.dryRun</code> is set to <code>false</code>, but enabling it can help identify potential issues early.</p> <p>Note that if no configuration (<code>.spec.config</code>) is provided, default values from the selected template will populate the object, and <code>.spec.dryRun</code> will automatically be enabled.</p> <p>Example: Dry Run with default configuration:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-managed-cluster\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-3\n  credential: aws-credential\n  dryRun: true\n</code></pre> <p>After validation (this is, you see <code>TemplateReady</code> as a condition in <code>.status.conditions</code>), remove or disable <code>.spec.dryRun</code> to proceed with deployment.</p> <p>Example: Validated <code>ClusterDeployment</code> object:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-managed-cluster\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-3\n  credential: aws-credential\n  config:\n    clusterLabels: {}\n    region: us-east-2\n    publicIP: true\n    controlPlaneNumber: 1\n    workersNumber: 1\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n  status:\n    conditions:\n    - type: TemplateReady\n      status: \"True\"\n      reason: Succeeded\n      message: Template is valid\n    - type: Ready\n      status: \"True\"\n      reason: Succeeded\n      message: ClusterDeployment is ready\n</code></pre>"},{"location":"appendix-extend-mgmt/","title":"Extended management configuration","text":""},{"location":"appendix-extend-mgmt/#extended-management-configuration","title":"Extended Management Configuration","text":"<p>k0rdent is deployed with the following default configuration, which may vary depending on the release version:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  core:\n    capi: {}\n    kcm: {}\n  providers:\n  - name: k0smotron\n  - name: cluster-api-provider-aws\n  - name: cluster-api-provider-azure\n  - name: cluster-api-provider-openstack\n  - name: cluster-api-provider-vsphere\n  - name: projectsveltos\nrelease: kcm-0-0-7\n</code></pre> <p>As you can see, the <code>Management</code> object defines the providers that are available from within k0rdent. Some of these are providers directly used by the user, such as aws, azure, and so on, and others are used internally by k0rdent, such as Sveltos.</p> <p>To see what is included in a specific release, look at the <code>release.yaml</code> file in the tagged release. For example, here is the v0.0.7 release.yaml.</p> <p>k0rdent allows you to customize its default configuration by modifying the spec of the <code>Management</code> object. This enables you to manage the list of providers to deploy and adjust the default settings for core components.</p> <p>For detailed examples and use cases, refer to Examples and Use Cases</p>"},{"location":"appendix-extend-mgmt/#configuration-guide","title":"Configuration Guide","text":"<p>There are two options to override the default management configuration of k0rdent:</p> <ol> <li> <p>Update the <code>Management</code> object after the k0rdent installation using <code>kubectl</code>:</p> <p><code>kubectl --kubeconfig &lt;path-to-management-kubeconfig&gt; edit management</code></p> </li> <li> <p>Deploy k0rdent skipping the default <code>Management</code> object creation and provide your    own <code>Management</code> configuration:</p> <ul> <li> <p>Create <code>management.yaml</code> file and configure core components and providers.   For example:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  core:\n    capi: {}\n    kcm:\n      config:\n        controller:\n          defaultRegistryURL: \"oci://ghcr.io/my-oci-registry-name/kcm/charts\"\n  providers:\n  - name: k0smotron\n  - name: cluster-api-provider-aws\n  - name: projectsveltos\nrelease: kcm-0-0-7\n</code></pre>       In the example above, the <code>Management</code> object is configured with custom registry settings for the KCM controller       and a reduced list of providers.</p> </li> <li> <p>Specify <code>--create-management=false</code> controller argument and install k0rdent:   If installing using <code>helm</code> add the following parameter to the <code>helm   install</code> command:</p> <pre><code>--set=\"controller.createManagement=false\"\n</code></pre> </li> <li> <p>Create <code>kcm</code> <code>Management</code> object after k0rdent installation:</p> <pre><code>kubectl --kubeconfig &lt;path-to-management-kubeconfig&gt; create -f management.yaml\n</code></pre> </li> </ul> </li> </ol> <p>You can customize the default configuration options for core components by updating the <code>.spec.core.&lt;core-component-name&gt;.config</code> section in the <code>Management</code> object. For example, to override the default settings for the KCM component, modify the <code>spec.core.kcm.config</code> section. To view the complete list of configuration options available for kcm, refer to: KCM Configuration Options for k0rdent v0.0.7 (Replace v0.0.7 with the relevant release tag for other k0rdent versions).</p> <p>To customize the list of providers to deploy, update the <code>.spec.providers</code> section. You can add or remove providers and configure custom templates for each provider. Each provider in the list must include the <code>name</code> field and may include the <code>template</code> and <code>config</code> fields:</p> <pre><code>- name: &lt;provider-name&gt; \n  template: &lt;provider-template&gt; # optional. If omitted, the default template from the `Release` object will be used\n  config: {} # optional provider configuration containing provider Helm Chart values in YAML format\n</code></pre>"},{"location":"appendix-extend-mgmt/#examples-and-use-cases","title":"Examples and Use Cases","text":""},{"location":"appendix-extend-mgmt/#configuring-a-custom-oci-registry-for-kcm-components","title":"Configuring a Custom OCI Registry for KCM components","text":"<p>You can override the default registry settings in k0rdent by specifying the <code>defaultRegistryURL</code>, <code>insecureRegistry</code>, and <code>registryCredsSecret</code> parameters under <code>spec.core.kcm.config.controller</code>:</p> <ul> <li><code>defaultRegistryURL</code>: Specifies the registry URL for downloading Helm charts representing templates.  Use the <code>oci://</code> prefix for OCI registries. Default: <code>oci://ghcr.io/k0rdent/kcm/charts</code>.</li> <li><code>insecureRegistry</code>: Allows connecting to an HTTP registry. Default: <code>false</code>.</li> <li><code>registryCredsSecret</code>: Specifies the name of a Kubernetes Secret containing authentication credentials for the  registry (optional). This Secret should exist in the system namespace (default: <code>kcm-system</code>).</li> </ul> <p>Example Configuration:</p> <pre><code>spec:\n  core:\n    kcm:\n      config:\n        controller:\n          defaultRegistryURL: \"oci://ghcr.io/my-private-oci-registry-name/kcm/charts\"\n          insecureRegistry: true\n          registryCredsSecret: my-private-oci-registry-creds\n</code></pre> <p>Example of a Secret with Registry Credentials:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-private-oci-registry-creds\n  namespace: kcm-system\nstringData:\n  username: \"my-user-123\"\n  password: \"my-password-123\"\n</code></pre> <p>The KCM controller will create the default HelmRepository using the provided configuration and fetch KCM components from this repository. For the example above, the following <code>HelmRepository</code> will be created:</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  labels:\n    k0rdent.mirantis.com/managed: \"true\"\n  name: my-private-oci-registry-name\n  namespace: kcm-system\nspec:\n  insecure: true\n  interval: 10m0s\n  provider: generic\n  type: oci\n  url: oci://ghcr.io/my-private-oci-registry-name/kcm/charts\n  secretRef:\n    name: my-private-oci-registry-creds\n</code></pre>"},{"location":"appendix-extend-mgmt/#configuring-a-custom-image-for-kcm-controllers","title":"Configuring a Custom Image for KCM controllers","text":"<p>You can override the default image for the KCM controllers by specifying the <code>repository</code>, <code>tag</code> and <code>pullPolicy</code> parameters under <code>spec.core.kcm.config.image</code>: </p> <p>Example Configuration:</p> <pre><code>spec:\n  core:\n    kcm:\n      config:\n        image:\n          repository: ghcr.io/my-custom-repo/kcm/controller\n          tag: v0.0.7\n          pullPolicy: IfNotPresent\n</code></pre>"},{"location":"appendix-providers/","title":"Cloud Provider Credentials Management in CAPI","text":"<p>Cloud provider credentials in Cluster API (CAPI) environments are managed through objects in the management cluster. Three objects handle credential storage and management, while a fourth object renders configuration into child clusters.</p> <p>The configuration follows two patterns:     - The first uses a ClusterIdentity resource that defines provider identity configuration and references a Secret with credentials. This approach is used by Azure and vSphere providers.     - The second uses only a Source Secret without ClusterIdentity, as used by OpenStack, where the Secret contains all configurations.</p> <p>The Credential resource provides an abstraction layer, referencing either a ClusterIdentity through identityRef or directly referencing a Secret, depending on the pattern used.</p> <p>The Template ConfigMap, marked with projectsveltos.io/template: \"true\", contains Go template code generating child cluster resources. Template processing accesses infrastructure cluster objects through built-in Sveltos variables, and/or through the getResource function for additionally exposed objects.</p> <p>The templating system uses Sprig functions for string manipulation, encoding/decoding, and type conversion. For details see Sprig docs at https://masterminds.github.io/sprig/ and Sveltos examples at https://projectsveltos.github.io/sveltos/template/intro_template/. Additional template examples can be found in the repository for each supported CAPI provider at https://github.com/k0rdent/kcm/tree/main/config/dev, in <code>*.credentials.yaml</code> files.</p>"},{"location":"appendix-providers/#provider-registration","title":"Provider Registration","text":"<p>New providers are registered through YAML configuration files mounted into the manager container at startup.</p> <p>The provider configuration files must be mounted into a predefined path in the manager container, typically done through a ConfigMap. Provider configuration examples can be found at https://github.com/k0rdent/kcm/tree/main/providers.</p>"},{"location":"glossary/","title":"k0rdent Glossary","text":"<p>This glossary is a collection of terms related to k0rdent. It clarifies some of the unique terms and concepts we use or explains more common ones that may need a little clarity in the way we use them.</p>"},{"location":"glossary/#beach-head-services","title":"Beach-head Services","text":"<p>We use the term to refer to those Kubernetes services that need to be installed on a Kubernetes cluster to make it actually useful, for example: an ingress controller, CNI, and/or CSI. While from the perspective of how they are deployed they are no different from other Kubernetes services, we define them as distinct from the apps and services deployed as part of the applications.</p>"},{"location":"glossary/#cluster-api-capi","title":"Cluster API (CAPI)","text":"<p>CAPI is a Kubernetes project that provides a declarative way to manage the lifecycle of  Kubernetes clusters. It abstracts the underlying infrastructure, allowing users to  create, scale, upgrade, and delete clusters using a consistent API. CAPI is extensible  via providers that offer infrastructure-specific functionality, such as AWS, Azure, and  vSphere.</p>"},{"location":"glossary/#capi-provider-see-also-infrastructure-provider","title":"CAPI provider (see also Infrastructure provider)","text":"<p>A CAPI provider is a Kubernetes CAPI extension that allows k0rdent to manage and drive  the creation of clusters on a specific infrastructure via API calls.</p>"},{"location":"glossary/#capa","title":"CAPA","text":"<p>CAPA stands for Cluster API Provider for AWS.</p>"},{"location":"glossary/#capg","title":"CAPG","text":"<p>CAPG stands for Cluster API Provider for Google Cloud.</p>"},{"location":"glossary/#capo","title":"CAPO","text":"<p>CAPO stands for Cluster API Provider for OpenStack.</p>"},{"location":"glossary/#capv","title":"CAPV","text":"<p>CAPV stands for Cluster API Provider for vSphere.</p>"},{"location":"glossary/#capz","title":"CAPZ","text":"<p>CAPZ stands for Cluster API Provider for Azure.</p>"},{"location":"glossary/#cloud-controller-manager","title":"Cloud Controller Manager","text":"<p>Cloud Controller Manager (CCM) is a Kubernetes component that embeds logic to manage a  specific infrastructure provider.</p>"},{"location":"glossary/#cluster-deployment","title":"Cluster Deployment","text":"<p>A Kubernetes cluster created and managed by k0rdent.</p>"},{"location":"glossary/#clusteridentity","title":"ClusterIdentity","text":"<p>ClusterIdentity is a Kubernetes object that references a Secret object containing  credentials for a specific infrastructure provider.</p>"},{"location":"glossary/#credential","title":"Credential","text":"<p>A <code>Credential</code> is a custom resource (CR) in kcm that supplies k0rdent with the necessary  credentials to manage a specific infrastructure. The credential object references other  CRs with infrastructure-specific credentials such as access keys, passwords,  certificates, etc. This means that a credential is specific to the CAPI provider that  uses it.</p>"},{"location":"glossary/#k0rdent-cluster-manager-kcm","title":"k0rdent Cluster Manager (kcm)","text":"<p>Deployment and life-cycle management of Kubernetes clusters, including configuration,  updates, and other CRUD operations.</p>"},{"location":"glossary/#k0rdent-observability-and-finops-kof","title":"k0rdent Observability and FinOps (kof)","text":"<p>Cluster and beach-head services monitoring, events and log management.</p>"},{"location":"glossary/#k0rdent-state-manager-ksm","title":"k0rdent State Manager (ksm)","text":"<p>Installation and life-cycle management of beach-head services, policy, Kubernetes API  configurations and more.</p>"},{"location":"glossary/#hosted-control-plane-hcp","title":"Hosted Control Plane (HCP)","text":"<p>An HCP is a Kubernetes control plane that runs outside of the clusters it manages.  Instead of running the control plane components (like the API server, controller  manager, and etcd) within the same cluster as the worker nodes, the control plane is  hosted on a separate, often centralized, infrastructure. This approach can provide  benefits such as easier management, improved security, and better resource utilization,  as the control plane can be scaled independently of the worker nodes.</p>"},{"location":"glossary/#infrastructure-provider-see-also-capi-provider","title":"Infrastructure provider (see also CAPI provider)","text":"<p>An infrastructure provider (aka <code>InfrastructureProvider</code>) is a Kubernetes custom  resource (CR) that defines the infrastructure-specific configuration needed for managing  Kubernetes clusters. It enables Cluster API (CAPI) to provision and manage clusters on  a specific infrastructure platform (e.g., AWS, Azure, VMware, OpenStack, etc.).</p>"},{"location":"glossary/#multi-cluster-service","title":"Multi-Cluster Service","text":"<p>The <code>MultiClusterService</code> is a custom resource used to manage deployment of beach-head  services across multiple clusters.</p>"},{"location":"glossary/#management-cluster","title":"Management Cluster","text":"<p>The Kubernetes cluster where k0rdent is installed and from which all other managed  clusters are managed.</p>"},{"location":"guide-to-quickstarts/","title":"Guide to QuickStarts","text":"<p>The following QuickStart chapters provide a recipe for quickly installing and trying k0rdent. Setting up k0rdent for production is detailed in the Administrator Guide.</p>"},{"location":"guide-to-quickstarts/#what-the-quickstart-covers","title":"What the QuickStart covers","text":"<p>The goal of the QuickStart is:</p> <ul> <li>To get a working environment set up for managing k0rdent.</li> <li>To get a Kubernetes cluster and other tools set up for hosting k0rdent itself.</li> <li>To select a cloud environment (AWS or Azure) and configure k0rdent to lifecycle manage clusters on this substrate.</li> <li>To use k0rdent to deploy a managed cluster.</li> <li>(Optional stretch goal) It's also possible to set up the k0rdent management cluster to lifecycle manage clusters on both cloud environments.</li> </ul>"},{"location":"guide-to-quickstarts/#where-the-quickstart-leads","title":"Where the QuickStart leads","text":"<p>The QuickStart shows and briefly explains the hows, whys, and wherefores of manually setting up k0rdent for use. Once built and validated, the QuickStart setup can be leveraged to begin an expanding sequence of demos that let you explore k0rdent's many features. The demos presently use makefiles to speed and simplify setup and operations. We strongly recommend exploring this fast-evolving resource.</p>"},{"location":"guide-to-quickstarts/#quickstart-prerequisites","title":"QuickStart Prerequisites","text":"<p>QuickStart prerequisites are simple \u2014 you'll need:</p> <ul> <li>A desktop or virtual machine running a supported version of linux \u2014 This machine will be used to install a basic Kubernetes working environment, and to host a single-node k0s Kubernetes management cluster to host k0rdent components. For simplest setup, configure this machine as follows:</li> <li>A minimum of 32GB RAM, 8 vCPUs, 100GB SSD (e.g., AWS <code>t3.2xlarge</code> or equivalent)</li> <li>Set up for SSH access using keys (standard for cloud VMs)</li> <li>Set up for passwordless sudo (i.e., edit /etc/sudoers to configure your user to issue sudo commands without a password challenge)</li> <li>Inbound traffic - SSH (port 22) and ping from your laptop's IP address</li> <li>Outbound traffic - All to any IP address</li> <li>Apply all recent updates and upgrade local applications (sudo apt update/sudo apt upgrade)</li> <li>(Optional) snapshot the machine in its virgin state</li> <li>Administrative-level access to an AWS or Azure cloud account - Depending on which cloud environment you prefer. k0rdent will leverage this cloud to provide infrastructure for hosting managed clusters.</li> </ul>"},{"location":"guide-to-quickstarts/#supported-operating-systems","title":"Supported Operating Systems","text":"<p>Any linux based os that supports deploying k0s will work, you may need to adjust the suggested commands.</p> OS Package Manager Link Ubuntu Server <code>apt</code> 22.04.5 LTS, Jammy Jellyfish <p>Note</p> <p>Other recent versions of 'enterprise' Linux should work with the following instructions as well, though you will need to adapt for different package managers and perhaps use slightly-different provider-recommended methods for installing required dependencies (e.g., Helm). Once you've installed k0rdent in the management cluster and have kubectl, Helm, and other resources connected, you'll mostly be dealing with Kubernetes, and everything should work the same way on any host OS.</p>"},{"location":"guide-to-quickstarts/#limitations","title":"Limitations","text":"<p>This QuickStart guides you in quickly creating a minimal k0rdent working environment. Setting up k0rdent for production is detailed in the Administrator Guide.</p> <p>The current QuickStart focuses on AWS and Azure cloud environments, and guides in creating 'standalone' clusters \u2014 in k0rdent parlance, that means 'CNCF-certified Kubernetes clusters with control planes and workers hosted on cloud virtual machines.' The 'CNCF-certified Kubernetes cluster' is the k0s Kubernetes distro.</p> <p>k0rdent can do so much more today. Let's take a look at what's possible.</p>"},{"location":"guide-to-quickstarts/#coming-soon","title":"Coming soon","text":"<p>QuickStarts for other Kubernetes distros, clouds, and environments will appear in the near future (short-term roadmap below):</p> <ul> <li>AWS EKS hosted \u2014 Amazon Elastic Kubernetes Service managed clusters </li> <li>Azure AKS hosted \u2014 Azure Kubernetes Service</li> <li>vSphere standalone \u2014 k0s Kubernetes on vSphere virtual machines</li> <li>OpenStack standalone \u2014 k0s Kubernetes on OpenStack virtual machines</li> </ul> <p>Plus (intermediate-term roadmap) tutorials for using k0rdent to create and manage hybrid, edge, and distributed platforms with Kubernetes-hosted control planes and workers on local or remote substrates.</p> <p>Demo/Tutorials: We will also be converting the demos gradually into tutorials that explain how to use k0rdent for:</p> <ul> <li>Adding services to individual managed clusters, enabling management of complete platforms/IDPs</li> <li>Adding services to multiple managed clusters, enabling at-scale implementation and lifecycle management of standardized environments</li> <li>(As a Platform Architect) Authorizing cluster and service templates for use by others, and constraining their use within guardrails (enabling self-service)</li> <li>(As an authorized user) Leveraging shared cluster and service templates to lifecycle manage platforms (performing self-service)</li> <li>... and more</li> </ul> <p>Next you'll learn how to use k0rdent.</p>"},{"location":"k0rdent-architecture/","title":"k0rdent architecture","text":"<p>The k0rdent architecture follows a declarative approach to cluster management using Kubernetes principles. The modular extensible architecture provides for a repeatable template-driven solution to interact with sub components such as the Cluster API (CAPI) and other Kubernetes components.</p> <p>The key principles of the architecture include:</p> <ul> <li>Leveraging Kubernetes core principles</li> <li>Its highly aligned but loosely coupled architecture</li> <li>A pluggable and extensible architecture</li> <li>A template-driven approach for repeatability</li> <li>A Standards-driven API</li> <li>Leveraging unmodified upstream components (e.g.CAPI)</li> <li>Supporting integration with custom components downstream</li> </ul> <p>Note</p> <p> This document is a ongoing work in progress, and we would welcome suggestions and questions. </p>"},{"location":"k0rdent-architecture/#summary","title":"Summary","text":"<p>The Management Cluster can orchestrate the provisioning and lifecycle of multiple child clusters on multiple clouds and infrastructures, keeping you from having to directly interact with individual infrastructure providers. By abstracting infrastructure, k0rdent promotes reusability (reducing, for example, the effort required to implement an IDP on a particular cloud), encourages standardization where practical, and lets you use the clouds and technologies you want, while also minimizing the cost of switching components (for example, open source subsystems, cloud substrates, and so on) if you need to.</p> <p>The k0rdent architecture comprises the following highlevel components:</p> <ul> <li>Cluster Management: Tools and controllers for defining, provisioning, and managing clusters.</li> <li>State Management: Controllers and systems for monitoring, updating, and managing the state of child clusters and their workloads.</li> <li>Infrastructure Providers: Services and APIs responsible for provisioning resources such as virtual machines, networking, and storage for clusters.</li> <li>Templates: Templates that can be used to define and create managed child clusters or the workloads that run on them.</li> </ul> <p></p>"},{"location":"k0rdent-architecture/#management-cluster","title":"Management cluster","text":"<p>The management cluster is the core of the k0rdent architecture. It hosts all of the controllers needed to make k0rdent work. This includes:</p> <ul> <li>k0rdent Cluster Manager (kcm) Controller:  kcm provides a wrapper for k0rdent\u2019s CAPI-related capabilities. It orchestrates:<ul> <li>Cluster API (CAPI) Controllers: CAPI controllers are designed to work with specific infrastructure providers. For example, one CAPI controller manages the creation and lifecycle of Kubernetes clusters running on Amazon Web Services, while another manages those on Azure. It\u2019s also possible to create custom CAPI controllers to integrate with internal systems.</li> <li>k0smotron Controller: k0smotron extends CAPI with additional functionality, including control plane and worker node bootstrap providers for k0s Kubernetes, and a control plane provider that supports Hosted Control Plane creation (for example, k0s control planes in pods on a host Kubernetes cluster, which can be the same cluster that hosts k0rdent). The k0smotron project has also provided a so-called \u2018RemoteMachine\u2019 infrastructure provider for CAPI, enabling deployment and cluster operations via SSH on arbitrary remote Linux servers (including small-scale edge devices).</li> </ul> </li> <li>k0rdent Service Manager (ksm) Controller: ksm is responsible for lifecycle managing (deploy, scale, update, upgrade, teardown) services and applications on clusters, and for doing continuous state management of these services and applications. This is currently part of the kcm code base; we may split it out in the future. It orchestrates:<ul> <li>Services Controller: Responsible for coordinating kubernetes services such as combinations of services (and infrastructure provisioning dependencies) that add capabilities to the platform. For example, Nginx, with its dependencies, can be packaged as a service. Artifacts for services are stored locally or in an OCI repository, and are referenced as kubernetes CRD objects.</li> </ul> </li> <li>k0rdent Observability &amp; FinOps (kof) Controller (not depicted in above diagram): k0rdent Observability and FinOps provides enterprise-grade observability and FinOps capabilities for k0rdent-managed Kubernetes clusters. It enables centralized metrics, logging, and cost management through a unified OpenTelemetry-based architecture.</li> </ul> <p>We\u2019ll take a closer look at these pieces under Roles and Responsibilities.</p>"},{"location":"k0rdent-architecture/#cluster-deployments","title":"Cluster Deployments","text":"<p>A cluster deployment is also known as a child cluster, or a workload cluster. It\u2019s a Kubernetes cluster provisioned and managed by the management cluster, and it\u2019s where developers run their applications and workloads. These are \u201cregular\u201d Kubernetes clusters, and don\u2019t host any management components. Clusters are fully isolated from the management cluster via namespaces, and also from each other, making it possible to create multi-tenant environments. </p> <p>You can tailor a child cluster to specific use cases, with customized addons such as ingress controllers, monitoring tools, and logging solutions. You can also define specific Kubernetes configurations (for example, network policies, storage classes, and security policies) so they work for you and your applications or environments.</p> <p>Simply put, child clusters are where applications and workloads run.</p>"},{"location":"k0rdent-architecture/#templates","title":"Templates","text":"<p>One of the important tenets of the platform engineering philosophy is the use of Infrastructure as Code, but k0rdent takes that one step further through the use of templates. Templates are re-usable text definitions of components that can be used to create and manage clusters. Templates provide a declarative way for users and developers to deploy and manage complex clusters or components while massively reducing the number of parameters they need to configure. Considered generally, k0rdent templates are:</p> <ul> <li>Formatted using YAML: Templates use YAML as an abstraction to represent the target state, so they\u2019re human-readable and editable.</li> <li>Designed to be used in multiple contexts using runtime parameterization: Through the use of placeholders, you can customize templates at runtime without having to directly edit the template.</li> <li>Used for both cluster creation and addon management: Users can define a cluster using YAML, or they can define addons, such as an ingress operator or monitoring tools, to be added to those clusters.</li> <li>Of limited scope: k0rdent lets you set restrictions over what templates can be deployed by whom. For example, as the platform manager (see Roles and Responsibilities), you can specify that non-admin users can only execute templates that deploy a particular set of controllers.</li> </ul> <p>Major template types used in k0rdent include:</p> <ul> <li>Cluster Templates: <code>ClusterTemplate</code>s define clusters in coordination with the clouds and infrastructures they run on. They're designed to be immutable \u2014 they get invoked by k0rdent objects like <code>ClusterDeployment</code>s to create and manage individual clusters and groups of clusters.</li> <li>Service Templates: <code>ServiceTemplate</code>s define services, addons, and workloads that run on clusters. They're also designed to be immutable, and get invoked by 'ClusterDeployment's and other k0rdent objects so that IDPs/platforms can be declared and managed as units. </li> </ul>"},{"location":"k0rdent-architecture/#roles-and-responsibilities","title":"Roles and responsibilities","text":"<p>k0rdent was designed to be used by several groups of people, with hierarchical and complementary roles and responsibilities. You may have your own names for them, but we\u2019ll refer to them as:</p> <ul> <li>Platform Architect: This person or team has global responsibility to the business and technical stakeholders for designing IDPs/platforms for later adaptation to particular clouds and infrastructures, workloads, performance and cost objectives, security and regulatory regimes, and operational requirements. k0rdent enables Platform Architects to create sets of reusable <code>ClusterTemplate</code>s and <code>ServiceTemplate</code>s, closely defining IDPs/platforms in the abstract.</li> <li>Platform Lead: This person or team (sometimes referred to as 'CloudOps') is primarily responsible for actions corresponding to k0rdent Cluster Manager (kcm). They adapt <code>ClusterTemplate</code>s to the correct cloud, and they make sure that everything is working properly. They\u2019re also responsible for limiting the Project Team\u2019s access to <code>Cluster</code> and <code>Service</code> templates necessary to do their jobs. For example, they might limit the templates that can be deployed to an approved set, or provide CAPI operators for only the clouds on which the company wants applications to run, helping to eliminate shadow IT. </li> <li>Platform Engineer: This person or team is responsible for the day-to-day management of the environment. They use <code>ClusterTemplate</code>s and <code>ServiceTemplate</code>s provided by the Platform Lead (as authorized to do so) and may create additional <code>ServiceTemplate</code>s to customize their own Kubernetes cluster so that it\u2019s appropriate for their application.</li> </ul>"},{"location":"k0rdent-architecture/#credentials","title":"Credentials","text":"<p>Creating and managing Kubernetes clusters requires having the proper permissions on the target infrastructure, but you certainly wouldn\u2019t want to give out your AWS account information to every single one of your developers.</p> <p>To solve this problem, k0rdent lets you create a <code>Credential</code> object that provides the access your developers need. It works like this:</p> <ol> <li>The platform lead creates a provider-specific <code>ClusterIdentity</code> and <code>Secret</code> that include all of the information necessary to perform various actions.</li> <li>The platform lead then creates a <code>Credential</code> object that references the <code>ClusterIdentity</code>.</li> <li>Developers reference the <code>Credential</code> object, which gives the cluster the ability to access these credentials (little \u201cc\u201d) without having to expose them to developers directly.</li> </ol>"},{"location":"k0rdent-architecture/#tldr-conclusion","title":"TL;DR - Conclusion","text":"<p>k0rdent provides a comprehensive Kubernetes lifecycle management framework through its three core components:</p> <ul> <li>kcm: Cluster provisioning and management.</li> <li>ksm: Application and runtime state management.</li> <li>kof: Observability, logging, and cost optimization.</li> </ul> <p>With multi-provider support, templated deployments, and strong security controls, k0rdent is being built to enable scalable, efficient, and consistent Kubernetes operations.</p>"},{"location":"k0rdent-architecture/#terms-and-clarification","title":"Terms and Clarification","text":"<p>Declarative approach: We define define the declarative approach to cluster management using the Kubernetes principles as the process where you define the state you want within custom resource objects and the controllers or customer operators ensure that the system moves toward that desired state.</p>"},{"location":"k0rdent-documentation-style-guide/","title":"Macro Syntax Error","text":"<p>File: <code>k0rdent-documentation-style-guide.md</code></p> <p>Line 176 in Markdown file: Missing end of comment tag <pre><code>### Use code style for inline code, commands {#code-style-inline-code}\n</code></pre></p>"},{"location":"known-issues-eks/","title":"EKS Machines Are not Created: ControlPlaneIsStable Preflight Check Failed","text":"<p>Related issue: KCM #907</p> <p>The deployment of the EKS cluster is stuck waiting for the machines to be provisioned. The <code>MachineDeployment</code> resource is showing the following conditions:</p> <pre><code>Type: MachineSetReady\nStatus: False\nReason: PreflightCheckFailed\nMessage: ekaz-eks-dev-eks-md: AWSManagedControlPlane kcm-system/ekaz-eks-dev-eks-cp is provisioning (\"ControlPlaneIsStable\" preflight check failed)\nType: Available\nStatus: False\nReason: WaitingForAvailableMachines\nMessage: ekaz-eks-dev-eks-md: Minimum availability requires 1 replicas, current 0 available\nType: Ready\nStatus: False\nReason: WaitingForAvailableMachines\nMessage: ekaz-eks-dev-eks-md: Minimum availability requires 1 replicas, current 0 available\n</code></pre> <p>As a result, the cluster was successfully created in EKS but no nodes are available.</p> <p>Workaround</p> <ol> <li> <p>Edit the <code>MachineDeployment</code> object: <pre><code>kubectl --kubeconfig &lt;management-kubeconfig&gt; edit MachineDeployment -n &lt;cluster-namespace&gt; &lt;cluster-name&gt;-md\n</code></pre></p> </li> <li> <p>Add <code>machineset.cluster.x-k8s.io/skip-preflight-checks: \"ControlPlaneIsStable\"</code> annotation to skip the <code>ControlPlaneIsStable</code> preflight check: <pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: MachineDeployment\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: aws-eks-dev\n    meta.helm.sh/release-namespace: kcm-system\n    machineset.cluster.x-k8s.io/skip-preflight-checks: \"ControlPlaneIsStable\" # add new annotation\n  name: aws-eks-dev-md\n</code></pre></p> </li> <li> <p>Save and exit</p> </li> </ol>"},{"location":"quickstart-1-mgmt-node-and-cluster/","title":"QuckStart 1 - Set up Management Node and Cluster","text":"<p>Please review the Guide to QuickStarts for preliminaries. This QuickStart unit details setting up a single-VM environment for managing and interacting with k0rdent, and for hosting k0rdent components on a single-node local Kubernetes management cluster. Once k0rdent is installed on the management cluster, you can drive k0rdent by SSHing into the management node (kubectl is there and will be provisioned with the appropriate kubeconfig) or remotely by various means (e.g., install the management cluster kubeconfig in Lens or another Kubernetes dashboard on your laptop, tunnel across from your own local kubectl, etc.)</p>"},{"location":"quickstart-1-mgmt-node-and-cluster/#install-a-single-node-k0s-cluster-locally-to-work-as-k0rdents-management-cluster","title":"Install a single-node k0s cluster locally to work as k0rdent's management cluster","text":"<p>k0s Kubernetes is a CNCF-certified minimal single-binary Kubernetes that installs with one command, and brings along its own CLI. We're using it to quickly set up a single-node management cluster on our manager node. However, k0rdent works on any CNCF-certified Kubernetes. If you choose to use something else, Team k0rdent would love to hear how you set things up to work for you.</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh\nsudo k0s install controller --single\nsudo k0s start\n</code></pre> <p>You can check to see if the cluster is working by leveraging kubectl (installed and configured automatically by k0s) via the k0s CLI:</p> <pre><code>sudo k0s kubectl get nodes\n</code></pre> <p>You should see something like this:</p> <pre><code>NAME              STATUS   ROLES    AGE   VERSION\nip-172-31-29-61   Ready    &lt;none&gt;   46s   v1.31.2+k0s\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#install-kubectl","title":"Install kubectl","text":"<p>k0s installs a compatible kubectl and makes it accessible via its own client. But to make your environment easier to configure, we advise installing kubectl the normal way on the manager node and using it to control the local k0s management cluster.</p> <pre><code>sudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly\nsudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#get-the-local-k0s-clusters-kubeconfig-for-kubectl","title":"Get the local k0s cluster's kubeconfig for kubectl","text":"<p>On startup, k0s stores the administrator's kubeconfig in a local directory, making it easy to access:</p> <pre><code>sudo cp /var/lib/k0s/pki/admin.conf KUBECONFIG\nsudo chmod +r KUBECONFIG\nexport KUBECONFIG=./KUBECONFIG\n</code></pre> <p>At this point, your newly-installed kubectl should be able to interoperate with the k0s management cluster with administrative privileges. Test to see that the cluster is ready (usually takes about one minute):</p> <pre><code>kubectl get nodes\n</code></pre> <p>You should see something like this:</p> <pre><code>NAME              STATUS   ROLES           AGE   VERSION\nip-172-31-29-61   Ready    control-plane   25m   v1.31.2+k0s\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#install-helm","title":"Install Helm","text":"<p>The Helm Kubernetes package manager is used to install k0rdent services. We'll install Helm as follows:</p> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre> <p>Issuing these commands should produce something very much like the following output:</p> <pre><code>Downloading https://get.helm.sh/helm-v3.16.3-linux-amd64.tar.gz\nVerifying checksum... Done.\nPreparing to install helm into /usr/local/bin\nhelm installed into /usr/local/bin/helm\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#install-k0rdent-into-the-k0s-management-cluster","title":"Install k0rdent into the k0s management cluster","text":"<p>Now we'll install k0rdent itself into the k0s management cluster:</p> <pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm --version 0.1.0 -n kcm-system --create-namespace\n</code></pre> <p>You'll see something like the following. Ignore the warnings, since this is an ephemeral, non-production, non-shared environment:</p> <pre><code>WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: ./KUBECONFIG\nWARNING: Kubernetes configuration file is world-readable. This is insecure. Location: ./KUBECONFIG\nPulled: ghcr.io/k0rdent/kcm/charts/kcm:0.1.0\nDigest: sha256:1f75e8e55c44d10381d7b539454c63b751f9a2ec6c663e2ab118d34c5a21087f\nNAME: kcm\nLAST DEPLOYED: Mon Dec  9 00:32:14 2024\nNAMESPACE: kcm-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>k0rdent startup takes several minutes.</p>"},{"location":"quickstart-1-mgmt-node-and-cluster/#check-that-k0rdent-cluster-management-pods-are-running","title":"Check that k0rdent cluster management pods are running","text":"<p>One fundamental k0rdent subsystem, k0rdent Cluster Manager (KCM), handles cluster lifecycle management on clouds and infrastructures: i.e., it helps you configure and compose clusters and manages infrastructure via Cluster API (CAPI). Before continuing, check that KCM pods are ready:</p> <pre><code>kubectl get pods -n kcm-system   # check pods in the kcm-system namespace\n</code></pre> <p>You should see something like:</p> <pre><code>NAME                                                           READY   STATUS\nazureserviceoperator-controller-manager-86d566cdbc-rqkt9       1/1     Running\ncapa-controller-manager-7cd699df45-28hth                       1/1     Running\ncapi-controller-manager-6bc5fc5f88-hd8pv                       1/1     Running\ncapv-controller-manager-bb5ff9bd5-7dsr9                        1/1     Running\ncapz-controller-manager-5dd988768-qjdbl                        1/1     Running\nhelm-controller-76f675f6b7-4d47l                               1/1     Running\nkcm-cert-manager-7c8bd964b4-nhxnq                              1/1     Running\nkcm-cert-manager-cainjector-56476c46f9-xvqhh                   1/1     Running\nkcm-cert-manager-webhook-69d7fccf68-s46w8                      1/1     Running\nkcm-cluster-api-operator-79459d8575-2s9jc                      1/1     Running\nkcm-controller-manager-64869d9f9d-zktgw                        1/1     Running\nk0smotron-controller-manager-bootstrap-6c5f6c7884-d2fqs        2/2     Running\nk0smotron-controller-manager-control-plane-857b8bffd4-zxkx2    2/2     Running\nk0smotron-controller-manager-infrastructure-7f77f55675-tv8vb   2/2     Running\nsource-controller-5f648d6f5d-7mhz5                             1/1     Running\n</code></pre> <p>Pods reported in states other than Running should become ready momentarily.</p>"},{"location":"quickstart-1-mgmt-node-and-cluster/#check-that-the-projectsveltos-pods-are-running","title":"Check that the projectsveltos pods are running","text":"<p>The other fundamental k0rdent subsystem, k0rdent Service Manager (KSM), handles services configuration and lifecycle management on clusters. This utilizes the projectsveltos Kubernetes Add-On Controller and other open source projects. Before continuing, check that KSM pods are ready:</p> <pre><code>kubectl get pods -n projectsveltos   # check pods in the projectsveltos namespace\n</code></pre> <p>You should see something like:</p> <pre><code>NAME                                     READY   STATUS    RESTARTS   AGE\naccess-manager-cd49cffc9-c4q97           1/1     Running   0          16m\naddon-controller-64c7f69796-whw25        1/1     Running   0          16m\nclassifier-manager-574c9d794d-j8852      1/1     Running   0          16m\nconversion-webhook-5d78b6c648-p6pxd      1/1     Running   0          16m\nevent-manager-6df545b4d7-mbjh5           1/1     Running   0          16m\nhc-manager-7b749c57d-5phkb               1/1     Running   0          16m\nsc-manager-f5797c4f8-ptmvh               1/1     Running   0          16m\nshard-controller-767975966-v5qqn         1/1     Running   0          16m\nsveltos-agent-manager-56bbf5fb94-9lskd   1/1     Running   0          15m\n</code></pre> <p>If you have fewer pods than shown above, just wait a little while for all the pods to reconcile and start running.</p>"},{"location":"quickstart-1-mgmt-node-and-cluster/#verify-that-kcm-provider-and-related-templates-are-available","title":"Verify that KCM provider and related templates are available","text":"<p>k0rdent KCM leverages CAPI to manage Kubernetes cluster assembly and host infrastructure. CAPI requires infrastructure providers for different clouds and infrastructure types. These are delivered and referenced within k0rdent using templates, instantiated in the management cluster as objects. Before continuing, verify that default provider template objects are installed and verified. Other templates are also stored as provider templates in this namespace \u2014 for example, the templates that determine setup of KCM itself and other parts of the k0rdent system (e.g., projectsveltos, which is a component of k0rdent Service Manager (KSM, see below)) as well as the k0smotron subsystem, which enables creation and lifecycle management of managed clusters that use Kubernetes-hosted control planes (i.e., control planes as pods):</p> <pre><code>kubectl get providertemplate -n kcm-system   # list providertemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to what's shown below. The placeholder X-Y-Z represents the current version number of the template, and will be replaced in the listing with digits:</p> <pre><code>NAME                                 VALID\ncluster-api-X-Y-Z                    true\ncluster-api-provider-aws-X-Y-Z       true\ncluster-api-provider-azure-X-Y-Z     true\ncluster-api-provider-vsphere-X-Y-Z   true\nkcm-X-Y-Z                            true\nk0smotron-X-Y-Z                      true\nprojectsveltos-X-Y-Z                 true\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#verify-that-kcm-clustertemplate-objects-are-available","title":"Verify that KCM ClusterTemplate objects are available","text":"<p>CAPI also requires control plane and bootstrap (worker node) providers to construct and/or manage different Kubernetes cluster distros and variants. Again, these providers are delivered and referenced within k0rdent using templates, instantiated in the management cluster as <code>ClusterTemplate</code> objects. Before continuing, verify that default ClusterTemplate objects are installed and verified:</p> <pre><code>kubectl get clustertemplate -n kcm-system   # list clustertemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to what's shown below:</p> <pre><code>NAME                                VALID\naws-eks-X-Y-Z                       true\naws-hosted-cp-X-Y-Z                 true\naws-standalone-cp-X-Y-Z             true\nazure-hosted-cp-X-Y-Z               true\nazure-standalone-cp-X-Y-Z           true\nvsphere-hosted-cp-X-Y-Z             true\nvsphere-standalone-cp-X-Y-Z         true\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#verify-that-ksm-servicetemplate-objects-are-available","title":"Verify that KSM ServiceTemplate objects are available","text":"<p>k0rdent Service Manager (KSM) uses Service Templates to lifecycle manage services and applications installed on clusters. These, too, are represented as declarative templates, instantiated as ServiceTemplate objects. Check that default ServiceTemplate objects have been created and validated:</p> <pre><code>kubectl get servicetemplate -n kcm-system   # list servicetemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to what's shown below:</p> <pre><code>NAME                      VALID\ncert-manager-1-16-2       true\ndex-0-19-1                true\nexternal-secrets-0-11-0   true\ningress-nginx-4-11-0      true\ningress-nginx-4-11-3      true\nkyverno-3-2-6             true\nvelero-8-1-0              true\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#next-steps","title":"Next steps","text":"<p>Your QuickStart management node is now complete, and k0rdent is installed and operational. Next, it's time to select AWS or Azure as an environment for hosting managed clusters.</p>"},{"location":"quickstart-2-aws/","title":"QuickStart 2 - AWS target environment","text":"<p>In this QuickStart unit, we'll be gathering information and performing preparatory steps to enable k0rdent (running on your management node) to manage clusters on Amazon Web Services (AWS), and deploying our first managed cluster.</p> <p>As noted in the Guide to QuickStarts, you'll need administrative access to AWS account to complete this step. If you haven't yet created a management node and installed k0rdent, go back to QuickStart 1 - Management node and cluster.</p> <p>Note that if you have already done our Azure QuickStart (QuickStart 2 - Azure target environment) you can continue here with steps to add the ability to manage clusters on AWS. The k0rdent management cluster can accommodate multiple provider and credential setups, enabling management of multiple infrastructures. And even if your management node is external to AWS (for example, it could be on an Azure virtual machine), as long as you permit outbound traffic to all IP addresses from the management node, this should work fine. A big benefit of k0rdent is that it provides a single point of control and visibility across multiple clusters on multiple clouds and infrastructures.</p> <p>Note</p> <p> CLOUD SECURITY 101: k0rdent requires some but not all permissions to manage AWS \u2014 doing so via the CAPA (ClusterAPI for AWS) provider. So a best practice for using k0rdent with AWS (this pattern is repeated with other clouds and infrastructures) is to create a new 'k0rdent user' on your account with the particular permissions k0rdent and CAPA require.</p> <p>In this section, we'll create and configure IAM for that user, and perform other steps to make that k0rdent user's credentials accessible to k0rdent in the management node.</p> <p>Note</p> <p> If you're working on a shared AWS account, please ensure that the k0rdent user is not already set up before creating a new one.</p> <p>Creating a k0rdent user with minimal required permissions is one of several principle-of-least-privilege mechanisms used to help ensure security as organizations work with k0rdent at progressively greater scales. For more on k0rdent security best practices, please see the Administrator Guide.</p>"},{"location":"quickstart-2-aws/#install-the-aws-cli","title":"Install the AWS CLI","text":"<p>We'll use the AWS CLI to create and set IAM permissions for the k0rdent user, so we'll install it on our management node:</p> <pre><code>sudo apt install unzip\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" \nunzip awscliv2.zip \nsudo ./aws/install\n</code></pre>"},{"location":"quickstart-2-aws/#install-clusterawsadm","title":"Install clusterawsadm","text":"<p>k0rdent uses Cluster API (CAPI) to marshal clouds and infrastructures. For AWS, this means using the components from the Cluster API Provider AWS (CAPA) project. This QuickStart leverages clusterawsadm, a CLI tool created by CAPA project that helps with AWS-specific tasks like IAM role, policy, and credential configuration.</p> <p>To install clusterawsadm on Ubuntu on x86 hardware:</p> <pre><code>curl -LO https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.7.1/clusterawsadm-linux-amd64\nsudo install -o root -g root -m 0755 clusterawsadm-linux-amd64 /usr/local/bin/clusterawsadm\n</code></pre>"},{"location":"quickstart-2-aws/#export-your-administrative-credentials","title":"Export your administrative credentials","text":"<p>You should have these already, preserved somewhere safe. If not, you can visit the AWS webUI (Access Management &gt; Users) and generate new credentials (Access Key ID, Secret Access Key, and Session Token (if using multi-factor authentication)).</p> <p>Export the credentials to the management node environment:</p> <pre><code>export AWS_REGION=EXAMPLE_AWS_REGION\nexport AWS_ACCESS_KEY_ID=EXAMPLE_ACCESS_KEY_ID\nexport AWS_SECRET_ACCESS_KEY=EXAMPLE_SECRET_ACCESS_KEY\nexport AWS_SESSION_TOKEN=EXAMPLE_SESSION_TOKEN # Optional. If you are using Multi-Factor Auth.\n</code></pre> <p>These credentials will be used both by the AWS CLI (to create your k0rdent user) and by clusterawsadm (to create a CloudFormation template used by CAPA within k0rdent).</p>"},{"location":"quickstart-2-aws/#create-the-k0rdent-aws-user","title":"Create the k0rdent AWS user","text":"<p>Now we can use the AWS CLI to create a new k0rdent user:</p> <pre><code> aws iam create-user --user-name k0rdentQuickstart\n</code></pre> <p>You'll see something like what's shown below. You should save this data securely. Note the Amazon Resource Name (ARN) because we'll be using it right away:</p> <pre><code>{\n    \"User\": {\n        \"Path\": \"/\",\n        \"UserName\": \"k0rdentQuickstart\",\n        \"UserId\": \"EXAMPLE_USER_ID\",\n        \"Arn\": \"arn:aws:iam::FAKE_ARN_123:user/k0rdentQuickstart\",\n        \"CreateDate\": \"2025-01-18T08:15:27+00:00\"\n    }\n}\n</code></pre>"},{"location":"quickstart-2-aws/#attach-iam-policies-to-the-k0rdent-user","title":"Attach IAM policies to the k0rdent user","text":"<p>Next, we'll attach appropriate policies to the k0rdent user. These are:</p> <ul> <li><code>control-plane.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>controllers.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>nodes.cluster-api-provider-aws.sigs.k8s.io</code></li> </ul> <p>We use the AWS CLI to attach them. To do this, you will need to extract the Amazon Resource Name (ARN) for the newly-created user. In the above example of content returned from the AWS CLI on user creation (see above) that's marked with the placeholder <code>FAKE_ARN_123</code>:</p> <p>Given this, you can assemble and execute the following commands to implement the required policies:</p> <pre><code>aws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\n</code></pre> <p>We can check to see that policies were assigned:</p> <p><pre><code>aws iam list-policies --scope Local\n</code></pre> And you'll see output that looks like this (this is non-valid example text):</p> <pre><code>{\n    \"Policies\": [\n        {\n            \"PolicyName\": \"controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNF3VUDTMH3N\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 2,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n        },\n        {\n            \"PolicyName\": \"nodes.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNF5TAKL44PU\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 3,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:44+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:44+00:00\"\n        },\n        {\n            \"PolicyName\": \"controllers.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNFVO6OHIQOE\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 3,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n        },\n        {\n            \"PolicyName\": \"control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNFY4FJ3DA2E\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 2,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n        }\n    ]\n}\n</code></pre>"},{"location":"quickstart-2-aws/#create-aws-credentials-for-the-k0rdent-user","title":"Create AWS credentials for the k0rdent user","text":"<p>In the AWS IAM Console, you can now create the Access Key ID and Secret Access Key for the k0rdent user and download them. You can also do this via the AWS CLI:</p> <pre><code>aws iam create-access-key --user-name k0rdentQuickstart\n</code></pre> <p>You should see something like this. It's important to save these credentials securely somewhere other than the management node, since the management node may end up being ephemeral. Again, this is non-valid example text:</p> <pre><code>{\n    \"AccessKey\": {\n        \"UserName\": \"k0rdentQuickstart\",\n        \"AccessKeyId\": \"EXAMPLE_ACCESS_KEY_ID\",\n        \"Status\": \"Active\",\n        \"SecretAccessKey\": \"EXAMPLE_SECRET_ACCESS_KEY\",\n        \"CreateDate\": \"2025-01-18T08:33:35+00:00\"\n    }\n}\n</code></pre>"},{"location":"quickstart-2-aws/#configure-aws-iam-for-k0rdent","title":"Configure AWS IAM for k0rdent","text":"<p>Before k0rdent CAPI can manage resources on AWS, you need to prepare for this by using <code>clusterawsadm</code> to create a bootstrap CloudFormation stack with additional IAM policies and a service account. You do this under the administrative account credentials you earlier exported to the management node environment:</p> <pre><code>clusterawsadm bootstrap iam create-cloudformation-stack\n</code></pre>"},{"location":"quickstart-2-aws/#create-iam-credentials-secret-on-the-management-cluster","title":"Create IAM credentials secret on the management cluster","text":"<p>Next, we create a secret containing credentials for the k0rdent user and apply this to the management cluster running k0rdent, in the kcm-system namespace (important: if you use another namespace, k0rdent will be unable to read the credentials). To do this, create the following YAML in a file called `aws-cluster-identity-secret.yaml':</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\ntype: Opaque\nstringData:\n  AccessKeyID: \"EXAMPLE_ACCESS_KEY_ID\"\n  SecretAccessKey: \"EXAMPLE_SECRET_ACCESS_KEY\"\n</code></pre> <p>Remember: the Access Key ID and Secret Access Key are the ones you generated for the k0rdent user, <code>k0rdentQuickStart</code>.</p> <p>Then apply this YAML to the management cluster as follows:</p> <pre><code>kubectl apply -f aws-cluster-identity-secret.yaml -n kcm-system\n</code></pre>"},{"location":"quickstart-2-aws/#create-the-awsclusterstaticidentity-object","title":"Create the AWSClusterStaticIdentity object","text":"<p>Next, we need to create an <code>AWSClusterStaticIdentity</code> object that uses the secret.</p> <p>To do this, create a YAML file named <code>aws-cluster-identity</code> as follows:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Note that the <code>spec.secretRef</code> is the same as the <code>metadata.name</code> of the secret we just created.</p> <p>Create the object as follows:</p> <pre><code>kubectl apply -f aws-cluster-identity.yaml  -n kcm-system\n</code></pre>"},{"location":"quickstart-2-aws/#create-the-k0rdent-cluster-manager-credential-object","title":"Create the k0rdent Cluster Manager credential object","text":"<p>Now we create the k0rdent Cluster Manager credential object. As in prior steps, create a YAML file called `aws-cluster-identity-cred.yaml':</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: aws-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  description: \"Credential Example\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: aws-cluster-identity\n</code></pre> <p>Note that <code>.spec.identityRef.kind</code> must be <code>AWSClusterStaticIdentity</code> and <code>.spec.identityRef.name</code> must match the <code>.metadata.name</code> of the <code>AWSClusterStaticIdentity</code> object.</p> <p>Now apply this YAML to your management cluster:</p> <pre><code>kubectl apply -f aws-cluster-identity-cred.yaml -n kcm-system\n</code></pre>"},{"location":"quickstart-2-aws/#list-available-cluster-templates","title":"List available cluster templates","text":"<p>k0rdent is now fully configured to manage AWS. To create a cluster, begin by listing the available <code>ClusterTemplate</code>s provided with k0rdent:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>You'll see output resembling what's below. Grab the name of the AWS standalone cluster template in its present version (in the below example, that's <code>aws-standalone-cp-0-0-5</code>):</p> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n</code></pre>"},{"location":"quickstart-2-aws/#create-your-clusterdeployment","title":"Create your ClusterDeployment","text":"<p>Now, to deploy a cluster, create a YAML file called <code>my-aws-clusterdeployment1.yaml</code>. We'll use this to create a <code>ClusterDeployment</code> object in k0rdent, representing the deployed cluster. The <code>ClusterDeployment</code> identifies for k0rdent the <code>ClusterTemplate</code> you wish to use for cluster creation, the identity credential object you wish to create it under (that of your k0rdent user), plus the region and instance types you want to use to host control plane and worker nodes:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-aws-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-5\n  credential: aws-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    region: us-east-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre>"},{"location":"quickstart-2-aws/#apply-the-clusterdeployment-to-deploy-the-cluster","title":"Apply the <code>ClusterDeployment</code> to deploy the cluster","text":"<p>Finally, we'll apply the <code>ClusterDeployment</code> YAML (<code>my-aws-clusterdeployment1.yaml</code>) to instruct k0rdent to deploy the cluster:</p> <pre><code>kubectl apply -f my-aws-clusterdeployment1.yaml\n</code></pre> <p>Kubernetes should confirm this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-aws-clusterdeployment1 created\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. You can watch the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-aws-clusterdeployment1 --watch\n</code></pre> <p>In a short while, you'll see output such as:</p> <pre><code>NAME                        READY   STATUS\nmy-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre>"},{"location":"quickstart-2-aws/#obtain-the-clusters-kubeconfig","title":"Obtain the cluster's kubeconfig","text":"<p>Now you can retrieve the cluster's <code>kubeconfig</code>:</p> <pre><code>kubectl -n kcm-system get secret my-aws-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-aws-clusterdeployment1-kubeconfig.kubeconfig\n</code></pre> <p>And you can use the <code>kubeconfig</code> to see what's running on the cluster:</p> <pre><code>KUBECONFIG=\"my-aws-clusterdeployment1-kubeconfig.kubeconfig\"\nkubectl get pods -A\n</code></pre>"},{"location":"quickstart-2-aws/#list-managed-clusters","title":"List managed clusters","text":"<p>To verify the presence of the managed cluster, list the available <code>ClusterDeployment</code>s:</p> <pre><code>kubectl get ClusterDeployments -A\n</code></pre> <p>You'll see output something like this:</p> <pre><code>NAMESPACE    NAME                        READY   STATUS\nkcm-system   my-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre>"},{"location":"quickstart-2-aws/#tear-down-the-managed-cluster","title":"Tear down the managed cluster","text":"<p>To tear down the managed cluster, delete the <code>ClusterDeployment</code>:</p> <pre><code>kubectl delete ClusterDeployment my-aws-clusterdeployment1 -n kcm-system\n</code></pre> <p>You'll see confirmation like this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-aws-clusterdeployment1\" deleted\n</code></pre>"},{"location":"quickstart-2-aws/#next-steps","title":"Next Steps","text":"<p>Now that you've finished the k0rdent QuickStart, we have some suggestions for what to do next:</p> <p>Check out the Administrator Guide ...</p> <ul> <li>For a more detailed view of k0rdent setup for production</li> <li>For details about setting up k0rdent to manage clusters on VMware and OpenStack</li> <li>For details about using k0rdent with cloud Kubernetes distros: AWS EKS and Azure AKS</li> </ul> <p>Or check out the Demos Repository for fast, makefile-driven demos of k0rdent's key features!</p>"},{"location":"quickstart-2-azure/","title":"QuickStart 2 - Azure target environment","text":"<p>In this QuickStart unit, we'll be gathering information and performing preparatory steps to enable k0rdent (running on your management node) to manage clusters on Azure, and deploying a managed cluster.</p> <p>As noted in the Guide to QuickStarts, you'll need administrative access to an Azure account to complete this step. If you haven't yet created a management node and installed k0rdent, go back to QuickStart 1 - Management node and cluster.</p> <p>Note that if you have already done our AWS QuickStart (QuickStart 2 - AWS target environment) you can continue here with steps to add the ability to manage clusters on Azure. The k0rdent management cluster can accommodate multiple provider and credential setups, enabling management of multiple infrastructures. And even if your management node is external to Azure (for example, it could be on an AWS EC2 virtual machine), as long as you permit outbound traffic to all IP addresses from the management node, this should work fine. A big benefit of k0rdent is that it provides a single point of control and visibility across multiple clusters on multiple clouds and infrastructures.</p> <p>Note</p> <p> Cloud Security 101: k0rdent requires some but not all permissions to manage Azure resources \u2014 doing so via the CAPZ (ClusterAPI for Azure) provider. So a best practice for using k0rdent with Azure (this pattern is repeated with other clouds and infrastructures) is to create a new k0rdent Azure Cluster Identity and Service Principal (SP) on your account with the particular permissions k0rdent and CAPZ require.</p> <p>In this section, we'll create and configure those identity abstractions, and perform other steps to make required credentials accessible to k0rdent in the management node.</p> <p>Note</p> <p> If you're working on a shared Azure account, please ensure that the Azure Cluster Identity and Service Principal are not already set up before creating new abstractions.</p> <p>Creating user identity abstractions with minimal required permissions is one of several principle-of-least-privilege mechanisms used to help ensure security as organizations work with k0rdent at progressively greater scales. For more on k0rdent security best practices, please see the Administrator Guide.</p>"},{"location":"quickstart-2-azure/#install-the-azure-cli-az","title":"Install the Azure CLI (az)","text":"<p>The Azure CLI (az) is required to interact with Azure resources. Install it according to instructions in How to install the Azure CLI. For Linux/Debian (i.e., Ubuntu Server), it's one command:</p> <pre><code>curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n</code></pre>"},{"location":"quickstart-2-azure/#log-in-with-azure-cli","title":"Log in with Azure CLI","text":"<p>Run the az login command to authenticate your session with Azure.</p> <pre><code>az login\n</code></pre>"},{"location":"quickstart-2-azure/#register-resource-providers","title":"Register resource providers","text":"<p>Azure Resource Manager uses resource providers to manage resources of all different kinds, and required providers must be registered with an Azure account before k0rdent and CAPZ can work with them.</p> <p>You can list resources registered with your account using Azure CLI:</p> <pre><code>az provider list --query \"[?registrationState=='Registered']\" --output table\n</code></pre> <p>And see a listing like this:</p> <pre><code>Namespace                             RegistrationState\n-----------------------------------   -----------------\nMicrosoft.Compute                     Registered\nMicrosoft.Network                     Registered\n</code></pre> <p>You can then select from the commands below (or enter all of them) to register any unregistered resources that k0rdent and CAPZ require:</p> <pre><code>az provider register --namespace Microsoft.Compute\naz provider register --namespace Microsoft.Network\naz provider register --namespace Microsoft.ContainerService\naz provider register --namespace Microsoft.ManagedIdentity\naz provider register --namespace Microsoft.Authorization\n</code></pre>"},{"location":"quickstart-2-azure/#get-your-azure-subscription-id","title":"Get your Azure Subscription ID","text":"<p>Use the following command to list Azure subscriptions and their IDs:</p> <pre><code>az account list -o table\n</code></pre> <p>The output will look like this:</p> <pre><code>Name                     SubscriptionId                    TenantId\n-----------------------  -------------------------------   -----------------------------\nMy Azure Subscription    SUBSCRIPTION_ID_SUBSCRIPTION_ID   TENANT_ID_TENANT_ID_TENANT_ID\n</code></pre> <p>The Subcription ID is in the second column.</p>"},{"location":"quickstart-2-azure/#create-a-service-principal-for-k0rdent","title":"Create a Service Principal for k0rdent","text":"<p>The Service Principal is like a password-protected user that CAPZ will use to manage resources on Azure. To create it, run the following command with the Azure CLI, replacing  with the ID you copied earlier. <pre><code>az ad sp create-for-rbac --role contributor --scopes=\"/subscriptions/&lt;subscription-id&gt;\"\n</code></pre> <p>You'll see output that resembles what's below:</p> <pre><code>{\n \"appId\": \"SP_APP_ID_SP_APP_ID\",\n \"displayName\": \"azure-cli-2024-10-24-17-36-47\",\n \"password\": \"SP_PASSWORD_SP_PASSWORD\",\n \"tenant\": \"SP_TENANT_SP_TENANT\"\n}\n</code></pre> <p>Capture this output and secure the values it contains. We'll need several of these in a moment.</p>"},{"location":"quickstart-2-azure/#create-a-secret-object-with-the-azure-credentials","title":"Create a Secret object with the Azure credentials","text":"<p>For self-managed Azure clusters (non-AKS) create a Secret object that stores the <code>clientSecret</code> (password) from the Service Principal. Create a YAML file called <code>azure-cluster-identity-secret.yaml</code>, as follows, inserting the password for the Service Principal (represented by the placeholder <code>SP_PASSWORD_SP_PASSWORD</code> above):</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: azure-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nstringData:\n  clientSecret: SP_PASSWORD_SP_PASSWORD # Password retrieved from the Service Principal\ntype: Opaque\n</code></pre> <p>For managed (AKS) clusters on Azure create the secret with the <code>AZURE_CLIENT_ID</code>, <code>AZURE_CLIENT_SECRET</code>, <code>AZURE_SUBSCRIPTION_ID</code> and <code>AZURE_TENANT_ID</code> keys set:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: azure-aks-credential\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nstringData:\n  AZURE_CLIENT_ID: &lt;SP_APP_ID_SP_APP_ID&gt; # AppId retrieved from the Service Principal\n  AZURE_CLIENT_SECRET: &lt;SP_PASSWORD_SP_PASSWORD&gt; # Password retrieved from the Service Principal\n  AZURE_SUBSCRIPTION_ID: &lt;SUBSCRIPTION_ID_SUBSCRIPTION_ID&gt; # The ID of the Subscription\n  AZURE_TENANT_ID: &lt;TENANT_ID_TENANT_ID_TENANT_ID&gt; # TenantID retrieved from the Service Principal\ntype: Opaque\n</code></pre> <p>Apply the YAML to the k0rdent management cluster using the following command:</p> <pre><code>kubectl apply -f azure-cluster-identity-secret.yaml\n</code></pre>"},{"location":"quickstart-2-azure/#create-the-azureclusteridentity-object","title":"Create the AzureClusterIdentity Object","text":"<p>Info</p> <p> Skip this step for managed (AKS) clusters.</p> <p>This object defines the credentials k0rdent and CAPZ will use to manage Azure resources. It references the Secret you just created above.</p> <p>Create a YAML file called <code>azure-cluster-identity.yaml</code>. Make sure that <code>.spec.clientSecret.name</code> matches the <code>metadata.name</code> in the file you created above.</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: AzureClusterIdentity\nmetadata:\n  name: azure-cluster-identity\n  namespace: kcm-system\n  labels:\n    clusterctl.cluster.x-k8s.io/move-hierarchy: \"true\"\n    k0rdent.mirantis.com/component: \"kcm\"\nspec:\n  allowedNamespaces: {}\n  clientID: SP_APP_ID_SP_APP_ID # The App ID retrieved from the Service Principal above in Step 2\n  clientSecret:\n    name: azure-cluster-identity-secret\n    namespace: kcm-system\n  tenantID: SP_TENANT_SP_TENANT # The Tenant ID retrieved from the Service Principal above in Step 2\n  type: ServicePrincipal\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity.yaml\n</code></pre> <p>You should see output resembling this:</p> <pre><code>azureclusteridentity.infrastructure.cluster.x-k8s.io/azure-cluster-identity created\n</code></pre>"},{"location":"quickstart-2-azure/#create-the-kcm-credential-object","title":"Create the KCM Credential Object","text":"<p>Create a YAML with the specification of our credential and save it as <code>azure-cluster-identity-cred.yaml</code>.</p> <p>Note that for non-AKS clusters <code>.spec.kind</code> must be <code>AzureClusterIdentity</code>, and <code>.spec.name</code> must match <code>.metadata.name</code> of the AzureClusterIdentity object created in the previous step.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: azure-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: AzureClusterIdentity\n    name: azure-cluster-identity\n    namespace: kcm-system\n</code></pre> <p>For AKS clusters, the <code>.spec.identityRef.kind</code> must be set to <code>Secret</code>, and <code>.spec.name</code> must match <code>.metadata.name</code> of the <code>Secret</code> object.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: azure-aks-credential\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: v1\n    kind: Secret\n    name: azure-aks-credential\n    namespace: kcm-system\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity-cred.yaml\n</code></pre> <p>You should see output resembling this:</p> <pre><code>credential.k0rdent.mirantis.com/azure-cluster-identity-cred created\n</code></pre>"},{"location":"quickstart-2-azure/#find-your-locationregion","title":"Find your location/region","text":"<p>To determine where to deploy your cluster, you may wish to begin by listing your Azure location/regions:</p> <pre><code>az account list-locations -o table\n</code></pre> <p>You'll see output like this:</p> <pre><code>DisplayName               Name                 RegionalDisplayName\n------------------------  -------------------  -------------------------------------\nEast US                   eastus               (US) East US\nSouth Central US          southcentralus       (US) South Central US\nWest US 2                 westus2              (US) West US 2\nWest US 3                 westus3              (US) West US 3\nAustralia East            australiaeast        (Asia Pacific) Australia East\n</code></pre> <p>What you'll need to insert in your ClusterDeployment is the name (center column) of the region you wish to deploy to.</p>"},{"location":"quickstart-2-azure/#list-available-cluster-templates","title":"List available cluster templates","text":"<p>k0rdent is now fully configured to manage Azure. To create a cluster, begin by listing the available ClusterTemplates provided with k0rdent:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>You'll see output resembling what's below. Grab the name of the AWS standalone cluster template in its present version (in the below example, that's <code>azure-standalone-cp-0-0-5</code>):</p> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n</code></pre>"},{"location":"quickstart-2-azure/#create-your-clusterdeployment","title":"Create your ClusterDeployment","text":"<p>Now, to deploy a cluster, create a YAML file called <code>my-azure-clusterdeployment1.yaml</code>. We'll use this to create a ClusterDeployment object in k0rdent, representing the deployed cluster. The ClusterDeployment identifies for k0rdent the ClusterTemplate you wish to use for cluster creation, the identity credential object you wish to create it under, plus the location/region and instance types you want to use to host control plane and worker nodes:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-azure-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: azure-standalone-cp-0-0-5 # name of the clustertemplate\n  credential: azure-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    location: \"AZURE_LOCATION\" # Select your desired Azure Location\n    subscriptionID: SUBSCRIPTION_ID_SUBSCRIPTION_ID # Enter the Subscription ID used earlier\n    controlPlane:\n      vmSize: Standard_A4_v2\n    worker:\n      vmSize: Standard_A4_v2\n</code></pre> <p>For AKS clusters, the <code>ClusterDeployment</code> looks like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-azure-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: azure-aks-0-0-2\n  credential: azure-aks-credential\n  propagateCredentials: false # Should be set to `false`\n  config:\n    clusterLabels: {}\n    location: \"westus\" # Select your desired Azure Location (find it via `az account list-locations -o table`)\n    machinePools:\n      system:\n        vmSize: Standard_A4_v2\n      user:\n        vmSize: Standard_A4_v2\n</code></pre>"},{"location":"quickstart-2-azure/#apply-the-clusterdeployment-to-deploy-the-cluster","title":"Apply the ClusterDeployment to deploy the cluster","text":"<p>Finally, we'll apply the ClusterDeployment YAML (<code>my-azure-clusterdeployment1.yaml</code>) to instruct k0rdent to deploy the cluster:</p> <pre><code>kubectl apply -f my-azure-clusterdeployment1.yaml\n</code></pre> <p>Kubernetes should confirm this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-azure-clusterdeployment1 created\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-azure-clusterdeployment1 --watch\n</code></pre>"},{"location":"quickstart-2-azure/#obtain-the-clusters-kubeconfig","title":"Obtain the cluster's kubeconfig","text":"<p>Now you can retrieve the cluster's kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-azure-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-azure-clusterdeployment1-kubeconfig.kubeconfig\n</code></pre> <p>And you can use the kubeconfig to see what's running on the cluster:</p> <pre><code>KUBECONFIG=\"my-azure-clusterdeployment1-kubeconfig.kubeconfig\"\nkubectl get pods -A\n</code></pre>"},{"location":"quickstart-2-azure/#list-managed-clusters","title":"List managed clusters","text":"<p>To verify the presence of the managed cluster, list the available ClusterDeployments:</p> <pre><code>kubectl get ClusterDeployments -A\n</code></pre> <p>You'll see output something like this:</p> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-azure-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre>"},{"location":"quickstart-2-azure/#tear-down-the-managed-cluster","title":"Tear down the managed cluster","text":"<p>To tear down the managed cluster, delete the ClusterDeployment:</p> <pre><code>kubectl delete ClusterDeployment my-azure-clusterdeployment1 -n kcm-system\n</code></pre> <p>You'll see confirmation like this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-azure-clusterdeployment1\" deleted\n</code></pre>"},{"location":"quickstart-2-azure/#next-steps","title":"Next Steps","text":"<p>Now that you've finished the k0rdent QuickStart, we have some suggestions for what to do next:</p> <p>Check out the Administrator Guide ...</p> <ul> <li>For a more detailed view of k0rdent setup for production</li> <li>For details about setting up k0rdent to manage clusters on VMware and OpenStack</li> <li>For details about using k0rdent with cloud Kubernetes distros: AWS EKS and Azure AKS</li> </ul> <p>Or check out the Demos Repository for fast, makefile-driven demos of k0rdent's key features!</p>"},{"location":"template-aws/","title":"AWS template parameters","text":""},{"location":"template-aws/#aws-ami","title":"AWS AMI","text":"<p>By default k0rdent looks up the AMI ID automatically, using the latest Amazon Linux 2 image.</p> <p>You can override lookup parameters to search your desired image automatically or you can use a specific AMI ID directly. If both the AMI ID and lookup parameters are defined, the AMI ID will have higher precedence.</p>"},{"location":"template-aws/#image-lookup","title":"Image lookup","text":"<p>To configure automatic AMI lookup, k0rdent uses three parameters:</p> <ul> <li> <p><code>.imageLookup.format</code> - Used directly as a value for the <code>name</code> filter (see the describe-images filters). This field supports substitutions for <code>{{.BaseOS}}</code> and <code>{{.K8sVersion}}</code> with the base OS and kubernetes version, respectively.</p> </li> <li> <p><code>.imageLookup.org</code> - The AWS org ID that will be used as value for the <code>owner-id</code> filter.</p> </li> <li> <p><code>.imageLookup.baseOS</code> - The string to be used as a value for the <code>{{.BaseOS}}</code> substitution in the <code>.imageLookup.format</code> string.</p> </li> </ul>"},{"location":"template-aws/#ami-id","title":"AMI ID","text":"<p>The AMI ID can be directly used in the <code>.amiID</code> parameter.</p>"},{"location":"template-aws/#capa-prebuilt-amis","title":"CAPA prebuilt AMIs","text":"<p>Use <code>clusterawsadm</code> to get available AMIs to create a <code>ClusterDeployment</code>:</p> <pre><code>clusterawsadm ami list\n</code></pre> <p>For details, see Pre-built Kubernetes AMIs.</p>"},{"location":"template-aws/#ssh-access-to-cluster-nodes","title":"SSH access to cluster nodes","text":"<p>To access nodes using SSH you'll need to do two things:</p> <ul> <li>Add an SSH key added in the region where you want to deploy the cluster</li> <li>Enable Bastion host is enabled</li> </ul>"},{"location":"template-aws/#ssh-keys","title":"SSH keys","text":"<p>Only one SSH key is supported and it should be added in AWS prior to creating the <code>ClusterDeployment</code> object. The name of the key should then be placed under the <code>.spec.config.sshKeyName</code>.</p> <p>The same SSH key will be used for all machines and a bastion host.</p> <p>To enable the bastion, set the <code>.spec.config.bastion.enabled</code> option in the <code>ClusterDeployment</code> object to <code>true</code>.</p> <p>You can get a full list of the bastion configuration options in the CAPA docs.</p> <p>The resulting <code>ClusterDeployment</code> might look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: aws-standalone-cp-0-0-5\n  credential: aws-cred\n  config:\n    clusterLabels: {}\n    sshKeyName: foobar\n    bastion:\n      enabled: true\n...\n</code></pre>"},{"location":"template-aws/#eks-templates","title":"EKS templates","text":"<p>Warning</p> <p> When deploying EKS cluster please note that additional steps may be needed for proper VPC removal.</p> <p>Warning</p> <p> You may encounter an issue where EKS machines are not created due to the <code>ControlPlaneIsStable</code> preflight check failure during EKS cluster deployment. Please follow the instruction to apply the workaround.</p> <p>Warning</p> <p> You may encounter an issue where EKS machines are not created due to the <code>ControlPlaneIsStable</code> preflight check failure during EKS cluster deployment. Please follow the instruction to apply the workaround.</p> <p>EKS templates use parameters similar to AWS and the resulting EKS <code>ClusterDeployment</code> looks something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: aws-eks-0-0-3\n  credential: aws-cred\n  config:\n    clusterLabels: {}\n    sshKeyName: foobar\n    region: ${AWS_REGION}\n    workersNumber: 1\n...\n</code></pre>"},{"location":"template-aws/#aws-hosted-control-plane-deployment","title":"AWS Hosted control plane deployment","text":"<p>This section covers setting up for a k0smotron hosted control plane on AWS.</p>"},{"location":"template-aws/#prerequisites","title":"Prerequisites","text":"<p>Before starting you must have:</p> <ul> <li>A management Kubernetes cluster (v1.28+) deployed on AWS with kcm installed on it</li> <li>A default default storage class configured on the management cluster</li> <li>A VPC ID for the worker nodes</li> <li>A Subnet ID which will be used along with AZ information</li> <li>An AMI ID which will be used to deploy worker nodes</li> </ul> <p>Keep in mind that all control plane components for all cluster deployments will reside in the management cluster.</p>"},{"location":"template-aws/#networking","title":"Networking","text":"<p>The networking resources in AWS which are needed for a cluster deployment can be reused with a management cluster.</p> <p>If you deployed your AWS Kubernetes cluster using Cluster API Provider AWS (CAPA) you can obtain all the necessary data with the commands below or use the template found below in the kcm ClusterDeployment manifest generation section.</p> <p>If using the <code>aws-standalone-cp</code> template to deploy a hosted cluster it is recommended to use a <code>t3.large</code> or larger instance type as the <code>kcm-controller</code> and other provider controllers will need a large amount of resources to run.</p> <p>VPC ID</p> <pre><code>    kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{.spec.network.vpc.id}}'\n</code></pre> <p>Subnet ID</p> <pre><code>    kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{(index .spec.network.subnets 0).resourceID}}'\n</code></pre> <p>Availability zone</p> <pre><code>    kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{(index .spec.network.subnets 0).availabilityZone}}'\n</code></pre> <p>Security group <pre><code>    kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{.status.networkStatus.securityGroups.node.id}}'\n</code></pre></p> <p>AMI id</p> <pre><code>    kubectl get awsmachinetemplate &lt;cluster-name&gt;-worker-mt -o go-template='{{.spec.template.spec.ami.id}}'\n</code></pre> <p>If you want to use different VPCs/regions for your management or managed clusters you should setup additional connectivity rules like VPC peering.</p>"},{"location":"template-aws/#kcm-clusterdeployment-manifest","title":"kcm ClusterDeployment manifest","text":"<p>With all the collected data your <code>ClusterDeployment</code> manifest will look similar to this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: aws-hosted-cp\nspec:\n  template: aws-hosted-cp-0-0-4\n  credential: aws-credential\n  config:\n    clusterLabels: {}\n    vpcID: vpc-0a000000000000000\n    region: us-west-1\n    publicIP: true\n    subnets:\n      - id: subnet-0aaaaaaaaaaaaaaaa\n        availabilityZone: us-west-1b\n        isPublic: true\n        natGatewayID: xxxxxx\n        routeTableId: xxxxxx\n      - id: subnet-1aaaaaaaaaaaaaaaa\n        availabilityZone: us-west-1b\n        isPublic: false\n        routeTableId: xxxxxx\n    instanceType: t3.medium\n    securityGroupIDs:\n      - sg-0e000000000000000\n</code></pre> <p>Note</p> <p> In this example we're using the <code>us-west-1</code> region, but you should use the region of your VPC.</p>"},{"location":"template-aws/#kcm-clusterdeployment-manifest-generation","title":"kcm ClusterDeployment manifest generation","text":"<p>Grab the following <code>ClusterDeployment</code> manifest template and save it to a file named <code>clusterdeployment.yaml.tpl</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: aws-hosted\nspec:\n  template: aws-hosted-cp-0-0-4\n  credential: aws-credential\n  config:\n    clusterLabels: {}\n    vpcID: \"{{.spec.network.vpc.id}}\"\n    region: \"{{.spec.region}}\"\n    subnets:\n    {{- range $subnet := .spec.network.subnets }}\n      - id: \"{{ $subnet.resourceID }}\"\n        availabilityZone: \"{{ $subnet.availabilityZone }}\"\n        isPublic: {{ $subnet.isPublic }}\n        {{- if $subnet.isPublic }}\n        natGatewayId: \"{{ $subnet.natGatewayId }}\"\n        {{- end }}\n        routeTableId: \"{{ $subnet.routeTableId }}\"\n        zoneType: \"{{ $subnet.zoneType }}\"\n    {{- end }}\n    instanceType: t3.medium\n    securityGroupIDs:\n      - \"{{.status.networkStatus.securityGroups.node.id}}\"\n</code></pre> <p>Then run the following command to create the <code>clusterdeployment.yaml</code>:</p> <pre><code>kubectl get awscluster cluster -o go-template=\"$(cat clusterdeployment.yaml.tpl)\" &gt; clusterdeployment.yaml\n</code></pre>"},{"location":"template-aws/#deployment-tips","title":"Deployment Tips","text":"<ul> <li>Ensure kcm templates and the controller image are somewhere public and   fetchable.</li> <li>For installing the kcm charts and templates from a custom repository, load   the <code>kubeconfig</code> from the cluster and run the commands:</li> </ul> <pre><code>KUBECONFIG=kubeconfig IMG=\"ghcr.io/k0rdent/kcm/controller-ci:v0.0.1-179-ga5bdf29\" REGISTRY_REPO=\"oci://ghcr.io/k0rdent/kcm/charts-ci\" make dev-apply\nKUBECONFIG=kubeconfig make dev-templates\n</code></pre> <ul> <li>The infrastructure will need to manually be marked <code>Ready</code> to get the   <code>MachineDeployment</code> to scale up.  You can patch the <code>AWSCluster</code> kind using   the command:</li> </ul> <pre><code>KUBECONFIG=kubeconfig kubectl patch AWSCluster &lt;hosted-cluster-name&gt; --type=merge --subresource status --patch 'status: {ready: true}' -n kcm-system\n</code></pre> <p>For additional information on why this is required click here.</p>"},{"location":"template-azure/","title":"Azure machine parameters","text":""},{"location":"template-azure/#ssh","title":"SSH","text":"<p>The SSH public key can be passed to <code>.spec.config.sshPublicKey</code>  parameter (in the case of a hosted control plane) or <code>.spec.config.controlPlane.sshPublicKey</code> and <code>.spec.config.worker.sshPublicKey</code> parameters (in the case of a standalone control plane) of the <code>ClusterDeployment</code> object.</p> <p>It should be encoded in base64 format.</p>"},{"location":"template-azure/#vm-size","title":"VM size","text":"<p>Azure supports various VM sizes which can be retrieved with the following command:</p> <pre><code>az vm list-sizes --location \"&lt;location&gt;\" -o table\n</code></pre> <p>Then desired VM size could be passed to the:</p> <ul> <li><code>.spec.config.vmSize</code> - for hosted CP deployment.</li> <li><code>.spec.config.controlPlane.vmSize</code> - for control plane nodes in the standalone   deployment.</li> <li><code>.spec.config.worker.vmSize</code> - for worker nodes in the standalone deployment.</li> </ul> <p>Example: Standard_A4_v2</p>"},{"location":"template-azure/#root-volume-size","title":"Root Volume size","text":"<p>Root volume size of the VM (in GB) can be changed through the following parameters:</p> <ul> <li><code>.spec.config.rootVolumeSize</code> - for hosted CP deployment.</li> <li><code>.spec.config.controlPlane.rootVolumeSize</code> - for control plane nodes in the   standalone deployment.</li> <li><code>.spec.config.worker.rootVolumeSize</code> - for worker nodes in the standalone   deployment.</li> </ul> <p>Default value: 30</p> <p>Please note that this value can't be less than size of the root volume  defined in your image.</p>"},{"location":"template-azure/#vm-image","title":"VM Image","text":"<p>You can define the image which will be used for your machine using the following parameters:</p> <ul> <li><code>.spec.config.image</code> - for hosted CP deployment.</li> <li><code>.spec.config.controlPlane.image</code> - for control plane nodes in the standalone   deployment.</li> <li><code>.spec.config.worker.image</code> - for worker nodes in the standalone deployment.</li> </ul> <p>There are multiple self-excluding ways to define the image source (for example Azure Compute Gallery, Azure Marketplace, and so on).</p> <p>Detailed information regarding image can be found in CAPZ documentation</p> <p>By default, the latest official CAPZ Ubuntu based image is used.</p>"},{"location":"template-azure/#azure-hosted-control-plane-k0smotron-deployment","title":"Azure Hosted control plane (k0smotron) deployment","text":""},{"location":"template-azure/#prerequisites","title":"Prerequisites","text":"<ul> <li>Management Kubernetes cluster (v1.28+) deployed on Azure with kcm installed     on it</li> <li>Default storage class configured on the management cluster</li> </ul> <p>Keep in mind that all control plane components for all managed clusters will reside in the management cluster.</p>"},{"location":"template-azure/#pre-existing-resources","title":"Pre-existing resources","text":"<p>Certain resources will not be created automatically in a hosted control plane scenario, so you must create them in advance and provide them to  the <code>ClusterDeployment</code> object. You can reuse these resources with management cluster as described below.</p> <p>If you deployed your Azure Kubernetes cluster using Cluster API Provider Azure (CAPZ) you can obtain all the necessary data with the commands below:</p> <p>Location</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.location}}'\n</code></pre> <p>Subscription ID</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.subscriptionID}}'\n</code></pre> <p>Resource group</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.resourceGroup}}'\n</code></pre> <p>vnet name</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.networkSpec.vnet.name}}'\n</code></pre> <p>Subnet name</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).name}}'\n</code></pre> <p>Route table name</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).routeTable.name}}'\n</code></pre> <p>Security group name</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).securityGroup.name}}'\n</code></pre>"},{"location":"template-azure/#kcm-clusterdeployment-manifest","title":"kcm ClusterDeployment manifest","text":"<p>With all the collected data your <code>ClusterDeployment</code> manifest will look similar to this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp-0-0-2\n  credential: azure-credential\n  config:\n    clusterLabels: {}\n    location: \"westus\"\n    subscriptionID: SUBSCRIPTION_ID_SUBSCRIPTION_ID\n    vmSize: Standard_A4_v2\n    resourceGroup: mgmt-cluster\n    network:\n      vnetName: mgmt-cluster-vnet\n      nodeSubnetName: mgmt-cluster-node-subnet\n      routeTableName: mgmt-cluster-node-routetable\n      securityGroupName: mgmt-cluster-node-nsg\n</code></pre> <p>To simplify creation of the <code>ClusterDeployment</code> object you can use the template below:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp-0-0-2\n  credential: azure-credential\n  config:\n    clusterLabels: {}\n    location: \"{{.spec.location}}\"\n    subscriptionID: \"{{.spec.subscriptionID}}\"\n    vmSize: Standard_A4_v2\n    resourceGroup: \"{{.spec.resourceGroup}}\"\n    network:\n      vnetName: \"{{.spec.networkSpec.vnet.name}}\"\n      nodeSubnetName: \"{{(index .spec.networkSpec.subnets 1).name}}\"\n      routeTableName: \"{{(index .spec.networkSpec.subnets 1).routeTable.name}}\"\n      securityGroupName: \"{{(index .spec.networkSpec.subnets 1).securityGroup.name}}\"\n</code></pre> <p>Then you can render it using the command:</p> <pre><code>kubectl get azurecluster &lt;management-cluster-name&gt; -o go-template=\"$(cat template.yaml)\"\n</code></pre>"},{"location":"template-azure/#cluster-creation","title":"Cluster creation","text":"<p>After applying the <code>ClusterDeployment</code> object you must manually set the status of the <code>AzureCluster</code> object due to limitations in k0smotron (see k0sproject/k0smotron#668).</p> <p>Execute the following command:</p> <pre><code>kubectl patch azurecluster &lt;cluster-name&gt; --type=merge --subresource status --patch 'status: {ready: true}'\n</code></pre>"},{"location":"template-azure/#important-notes-on-the-cluster-deletion","title":"Important notes on the cluster deletion","text":"<p>Because of the aforementioned limitation you also need to take manual steps in order to properly delete an Azurecluster.</p> <p>Before removing the cluster, make sure to place a custom finalizer on the <code>AzureCluster</code> object. This is needed to prevent it from being deleted instantly, which will cause cluster deletion to stuck indefinitely.</p> <p>To place a finalizer you can execute the following command:</p> <pre><code>kubectl patch azurecluster &lt;cluster-name&gt; --type=merge --patch 'metadata: {finalizers: [manual]}'\n</code></pre> <p>When the finalizer is placed you can remove the <code>ClusterDeployment</code> as usual. Check that all <code>AzureMachine</code> objects are deleted successfully, and remove the finalizer you've placed to finish cluster deletion.</p> <p>If you have orphaned <code>AzureMachine</code>s left, you'll have to delete the finalizers on them manually after making sure that no VMs are present in Azure.</p> <p>Note</p> <p> Since Azure admission prohibits orphaned objects mutation, you'll have to disable it by deleting its <code>mutatingwebhookconfiguration</code>.</p>"},{"location":"template-byo/","title":"Bring your own Templates","text":"<p>In addition to the templates that ship with k0rdent, it's possible to make your own. These might represent different types of clusters, or they may represent additional services to add to a cluster. Follow these steps:</p>"},{"location":"template-byo/#create-a-source-object","title":"Create a Source Object","text":"<p>Info</p> <p> Skip this step if you're using an existing source.</p> <p>All templates are based on a Helm chart, so the first step is to define the source in which that Helm chart can be found.</p> <p>The source can be one of the following types:</p> <ul> <li>HelmRepository</li> <li>GitRepository</li> <li>Bucket</li> </ul> <p>Note that it's important to pay attention to where the source resides. Cluster-scoped <code>ProviderTemplates</code> must reside in the system namespace (<code>kcm-system</code> by default) but other template sources must reside in the same directory as the templates that will come from them.</p> <p>For example, this YAML describes a custom <code>Source</code> object of <code>kind</code> <code>HelmRepository</code>:</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: k0rdent-templates\n  namespace: kcm-system\nspec:\n  insecure: true\n  interval: 10m0s\n  provider: generic\n  type: oci\n  url: oci://ghcr.io/k0rdent/kcm/charts\n</code></pre>"},{"location":"template-byo/#create-the-template","title":"Create the Template","text":"<p>Once you have the Source, you can create the actual template. This template can be one of three types:</p> <ul> <li><code>ClusterTemplate</code></li> <li><code>ServiceTemplate</code></li> <li><code>ProviderTemplate</code></li> </ul> <p>For <code>ClusterTemplate</code>s and <code>ServiceTemplate</code>s, configure the namespace where this template should reside (<code>metadata.namespace</code>).</p> <p>For defining the Helm chart, you have two choices. You can either specify the actual helm chart definition in the <code>.spec.helm.chartSpec</code> field of the HelmChartSpec kind, or you can reference and existing <code>HelmChart</code> object in <code>.spec.helm.chartRef</code>.</p> <p>Note</p> <p> <code>spec.helm.chartSpec</code> and <code>spec.helm.chartRef</code> are mutually exclusive.</p> <p>To automatically create the <code>HelmChart</code> for the <code>Template</code>, configure the following custom helm chart parameters under <code>spec.helm.chartSpec</code>:</p> Field Description <code>sourceRef</code>LocalHelmChartSourceReference Reference to the source object (e.g., <code>HelmRepository</code>, <code>GitRepository</code>, or <code>Bucket</code>) in the same namespace as the Template. <code>chart</code>string The name of the Helm chart available in the source. <code>version</code>string Version is the chart version semver expression. Defaults to latest when omitted. <code>interval</code>Kubernetes meta/v1.Duration The frequency at which the <code>sourceRef</code> is checked for updates. Defaults to 10 minutes. <p>For the complete list of the <code>HelmChart</code> parameters, see: HelmChartSpec.</p> <p>The controller automatically creates the <code>HelmChart</code> object based on the chartSpec defined in <code>.spec.helm.chartSpec</code>.</p> <p>Note</p> <p> <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects should reside in the same namespace as the <code>ClusterDeployment</code> referencing them. The <code>ClusterDeployment</code> can't reference the Template from another namespace (the creation request will be declined by the admission webhook). All <code>ClusterTemplates</code> and <code>ServiceTemplates</code> shipped with kcm reside in the system namespace (defaults to <code>kcm-system</code>). To get the instructions on how to distribute Templates along multiple namespaces, read Template Life Cycle Management.</p>"},{"location":"template-byo/#examples","title":"Examples","text":"<p>Example</p> <p>Custom ClusterTemplate with the Chart Definition to Create a new HelmChart <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterTemplate\nmetadata:\n  name: custom-template\n  namespace: kcm-system\nspec:\n  providers:\n    - bootstrap-k0sproject-k0smotron\n    - control-plane-k0sproject-k0smotron\n    - infrastructure-openstack\n  helm:\n    chartSpec:\n      chart: os-k0sproject-k0smotron\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-templates\n</code></pre></p> <p>Example</p> <p>Custom ClusterTemplate Referencing an Existing HelmChart object</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterTemplate\nmetadata:\n  name: custom-template\n  namespace: kcm-system\nspec:\n  helm:\n    chartRef:\n      kind: HelmChart\n      name: custom-chart\n</code></pre>"},{"location":"template-byo/#required-and-exposed-providers-definition","title":"Required and exposed providers definition","text":"<p>The <code>*Template</code> object must specify the list of Cluster API providers that are either required (for <code>ClusterTemplates</code> and <code>ServiceTemplates</code>) or exposed (for <code>ProviderTemplates</code>). These providers include <code>infrastructure</code>, <code>bootstrap</code>, and <code>control-plane</code>. This can be achieved in two ways:</p> <ol> <li>By listing the providers explicitly in the <code>spec.providers</code> field.</li> <li>Alternatively, by including specific annotations in the <code>Chart.yaml</code> of the referenced Helm chart. The annotations should list the providers as a <code>comma-separated</code> value.</li> </ol> <p>For example:</p> <p>In a <code>Template</code> spec:</p> <pre><code>spec:\n  providers:\n  - bootstrap-k0sproject-k0smotron\n  - control-plane-k0sproject-k0smotron\n  - infrastructure-aws\n</code></pre> <p>In a <code>Chart.yaml</code>:</p> <pre><code>annotations:\n  cluster.x-k8s.io/provider: infrastructure-aws, control-plane-k0sproject-k0smotron, bootstrap-k0sproject-k0smotron\n</code></pre>"},{"location":"template-byo/#compatibility-attributes","title":"Compatibility attributes","text":"<p>Each of the <code>*Template</code> resources has compatibility versions attributes to constraint the core <code>CAPI</code>, <code>CAPI</code> provider or Kubernetes versions. CAPI-related version constraints must be set in the <code>CAPI</code> contract format. Kubernetes version constraints must be set in the Semantic Version format. Each attribute can be set either via the corresponding <code>.spec</code> fields or via the annotations. Values set via the <code>.spec</code> have precedence over the values set via the annotations.</p> <p>Note</p> <p> All of the compatibility attributes are optional, and validation checks only take place if both of the corresponding type attributes (e.g. provider contract versions in both <code>ProviderTemplate</code> and <code>ClusterTemplate</code>) are set.</p> <ol> <li> <p>The <code>ProviderTemplate</code> resource has dedicated fields to set compatible <code>CAPI</code> contract versions along with CRDs contract versions supported by the provider. Given contract versions will then be set accordingly in the <code>.status</code> field. Compatibility contract versions are key-value pairs, where the key is the core <code>CAPI</code> contract version, and the value is an underscore-delimited (_) list of provider contract versions supported by the core <code>CAPI</code>. For the core <code>CAPI</code> Template values should be empty.</p> <p>Example with the <code>.spec</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ProviderTemplate\n# ...\nspec:\n  providers:\n  - infrastructure-aws\n  capiContracts:\n    # commented is the example exclusively for the core CAPI Template\n    # v1alpha3: \"\"\n    # v1alpha4: \"\"\n    # v1beta1: \"\"\n    v1alpha3: v1alpha3\n    v1alpha4: v1alpha4\n    v1beta1: v1beta1_v1beta2\n</code></pre> <p>Example with the <code>annotations</code> in the <code>Chart.yaml</code> with the same logic as in the <code>.spec</code>:</p> <pre><code>annotations:\n  cluster.x-k8s.io/provider: infrastructure-aws\n  cluster.x-k8s.io/v1alpha3: v1alpha3\n  cluster.x-k8s.io/v1alpha4: v1alpha4\n  cluster.x-k8s.io/v1beta1: v1beta1_v1beta2\n</code></pre> </li> <li> <p>The <code>ClusterTemplate</code> resource has dedicated fields to set an exact compatible Kubernetes version in the Semantic Version format and required contract versions per each provider to match against the related <code>ProviderTemplate</code> objects. Given compatibility attributes will be then set accordingly in the <code>.status</code> field. Compatibility contract versions are key-value pairs, where the key is the name of the provider, and the value is the provider contract version required to be supported by the provider.</p> <p>Example with the <code>.spec</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterTemplate\n# ...\nspec:\n  k8sVersion: 1.30.0 # only exact semantic version is applicable\n  providers:\n  - bootstrap-k0sproject-k0smotron\n  - control-plane-k0sproject-k0smotron\n  - infrastructure-aws\n  providerContracts:\n    bootstrap-k0sproject-k0smotron: v1beta1 # only a single contract version is applicable\n    control-plane-k0sproject-k0smotron: v1beta1\n    infrastructure-aws: v1beta2\n</code></pre> <p>Example with the <code>.annotations</code> in the <code>Chart.yaml</code>:</p> <pre><code>annotations:\n  cluster.x-k8s.io/provider: infrastructure-aws, control-plane-k0sproject-k0smotron, bootstrap-k0sproject-k0smotron\n  cluster.x-k8s.io/bootstrap-k0sproject-k0smotron: v1beta1\n  cluster.x-k8s.io/control-plane-k0sproject-k0smotron: v1beta1\n  cluster.x-k8s.io/infrastructure-aws: v1beta2\n  k0rdent.mirantis.com/k8s-version: 1.30.0\n</code></pre> </li> <li> <p>The <code>ServiceTemplate</code> resource has dedicated fields to set an compatibility constrained Kubernetes version to match against the related <code>ClusterTemplate</code> objects. Given compatibility values will be then set accordingly in the <code>.status</code> field.</p> <p>Example with the <code>.spec</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ServiceTemplate\n# ...\nspec:\n  k8sConstraint: \"^1.30.0\" # only semantic version constraints are applicable\n</code></pre> <p>Example with the <code>annotations</code> in the <code>Chart.yaml</code>:</p> <pre><code>k0rdent.mirantis.com/k8s-version-constraint: ^1.30.0\n</code></pre> </li> </ol>"},{"location":"template-byo/#compatibility-attributes-enforcement","title":"Compatibility attributes enforcement","text":"<p>The aforedescribed attributes are checked for compliance with the following rules:</p> <ul> <li>Both the exact and constraint version of the same type (e.g. <code>k8sVersion</code> and <code>k8sConstraint</code>) must be set, otherwise no check is performed;</li> <li>If a <code>ClusterTemplate</code> object's provider's contract version does not satisfy contract versions from the related <code>ProviderTemplate</code> object, updates to the <code>ClusterDeployment</code> object will be blocked;</li> <li>If a <code>ProviderTemplate</code> object's <code>CAPI</code> contract version (for example, in a <code>v1beta1: v1beta1_v1beta2</code> key-value pair, the key <code>v1beta1</code> is the core <code>CAPI</code> contract version) is not listed in the core <code>CAPI</code> <code>ProviderTemplate</code> object, the updates to the <code>Management</code> object will be blocked;</li> <li>If a <code>ClusterTemplate</code> object's exact kubernetes version does not satisfy the kubernetes version constraint from the related <code>ServiceTemplate</code> object, the updates to the <code>ClusterDeployment</code> object will be blocked.</li> </ul>"},{"location":"template-intro/","title":"The Templates system","text":"<p>By default, k0rdent delivers a set of default <code>ProviderTemplate</code>, <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects:</p> <ul> <li><code>ProviderTemplate</code>    The template containing the configuration of the provider (e.g., k0smotron, AWS). These are cluster-scoped.</li> <li><code>ClusterTemplate</code>    The template containing the configuration of the cluster objects. These are namespace-scoped.</li> <li><code>ServiceTemplate</code>    The template containing the configuration of the service to be installed on the cluster deployment. These are namespace-scoped.</li> </ul> <p>All Templates are immutable, if you want to change something about a cluster that has been deployed, you have to apply a whole new template. You can also build your own templates and use them for deployment along with the templates shipped with k0rdent.</p>"},{"location":"template-intro/#template-naming-convention","title":"Template Naming Convention","text":"<p>The templates can have any name. However, since they are immutable, we have adopted a naming convention that includes semantic versioning in the name, as in <code>template-&lt;major&gt;-&lt;minor&gt;-&lt;patch&gt;</code>. Below are some examples for each of the templates.</p> <p>Example</p> <p>An example of a <code>ProviderTemplate</code> with its status. <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ProviderTemplate\nmetadata:\n  name: cluster-api-0-0-4\nspec:\n  helm:\n    chartSpec:\n      chart: cluster-api\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-templates\n      version: 0.0.4\nstatus:\n  capiContracts:\n    v1alpha3: \"\"\n    v1alpha4: \"\"\n    v1beta1: \"\"\n  chartRef:\n    kind: HelmChart\n    name: cluster-api-0-0-4\n    namespace: kcm-system\n  config:\n    airgap: false\n    config: {}\n    configSecret:\n      create: false\n      name: \"\"\n      namespace: \"\"\n  description: A Helm chart for Cluster API core components\n  observedGeneration: 1\n  valid: true\n</code></pre></p> <p>Example</p> <p>An example of a <code>ClusterTemplate</code> with its status. <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterTemplate\nmetadata:\n  name: aws-standalone-cp-0-0-3\n  namespace: kcm-system\nspec:\n  helm:\n    chartSpec:\n      chart: aws-standalone-cp\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-templates\n      version: 0.0.3\nstatus:\n  chartRef:\n    kind: HelmChart\n    name: aws-standalone-cp-0-0-3\n    namespace: kcm-system\n  config:\n    bastion:\n      allowedCIDRBlocks: []\n      ami: \"\"\n      disableIngressRules: false\n      enabled: false\n      instanceType: t2.micro\n    clusterIdentity:\n      kind: AWSClusterStaticIdentity\n      name: \"\"\n    clusterNetwork:\n      pods:\n        cidrBlocks:\n        - 10.244.0.0/16\n      services:\n        cidrBlocks:\n        - 10.96.0.0/12\n    controlPlane:\n      amiID: \"\"\n      iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n      imageLookup:\n        baseOS: \"\"\n        format: amzn2-ami-hvm*-gp2\n        org: \"137112412989\"\n      instanceType: \"\"\n      rootVolumeSize: 8\n    controlPlaneNumber: 3\n    extensions:\n      chartRepository: \"\"\n      imageRepository: \"\"\n    k0s:\n      version: v1.31.1+k0s.1\n    publicIP: false\n    region: \"\"\n    sshKeyName: \"\"\n    worker:\n      amiID: \"\"\n      iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n      imageLookup:\n        baseOS: \"\"\n        format: amzn2-ami-hvm*-gp2\n        org: \"137112412989\"\n      instanceType: \"\"\n      rootVolumeSize: 8\n    workersNumber: 2\n  description: 'An kcm template to deploy a k0s cluster on AWS with bootstrapped control\n    plane nodes. '\n  observedGeneration: 1\n  providerContracts:\n    bootstrap-k0smotron: v1beta1\n    control-plane-k0smotron: v1beta1\n    infrastructure-aws: v1beta2\n  providers:\n  - bootstrap-k0smotron\n  - control-plane-k0smotron\n  - infrastructure-aws\n  valid: true\n</code></pre></p> <p>Example</p> <p>An example of a <code>ServiceTemplate</code> with its status. <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ServiceTemplate\nmetadata:\n  name: kyverno-3-2-6\n  namespace: kcm-system\nspec:\n  helm:\n    chartSpec:\n      chart: kyverno\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-templates\n      version: 3.2.6\nstatus:\n  chartRef:\n    kind: HelmChart\n    name: kyverno-3-2-6\n    namespace: kcm-system\n  description: A Helm chart to refer the official kyverno helm chart\n  observedGeneration: 1\n  valid: true\n</code></pre></p>"},{"location":"template-intro/#template-life-cycle-management","title":"Template Life Cycle Management","text":"<p>Cluster and Service Templates can be delivered to target namespaces using the <code>AccessManagement</code>, <code>ClusterTemplateChain</code> and <code>ServiceTemplateChain</code> objects. The <code>AccessManagement</code> object contains the list of access rules to apply. Each access rule contains the namespaces' definition for delivering templates into and the template chains. Each <code>ClusterTemplateChain</code> and <code>ServiceTemplateChain</code> contains the supported templates and the upgrade sequences for them.</p> <p>The example of <code>ClusterTemplate</code> Management:</p> <ol> <li> <p>Create a <code>ClusterTemplateChain</code> object in the system namespace (defaults to <code>kcm-system</code>). Properly configure     the list of <code>.spec.supportedTemplates[].availableUpgrades</code> for the specified <code>ClusterTemplate</code> if you want to     allow upgrading. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterTemplateChain\nmetadata:\n  name: aws\n  namespace: kcm-system\nspec:\n  supportedTemplates:\n    - name: aws-standalone-cp-0-0-1\n      availableUpgrades:\n        - name: aws-standalone-cp-0-0-2\n    - name: aws-standalone-cp-0-0-2\n</code></pre> </li> <li> <p>Edit the <code>AccessManagement</code> object and configure the <code>.spec.accessRules</code>.     For example, to apply all templates and upgrade sequences defined in the <code>aws</code> <code>ClusterTemplateChain</code> to the     <code>default</code> namespace, the following <code>accessRule</code> should be added:</p> <pre><code>spec:\n  accessRules:\n  - targetNamespaces:\n      list:\n        - default\n    clusterTemplateChains:\n      - aws\n</code></pre> </li> </ol> <p>The kcm controllers will deliver all the <code>ClusterTemplate</code> objects across the target namespaces. As a result, the following new objects should be created:</p> <ul> <li><code>ClusterTemplateChain</code> <code>default/aws</code></li> <li><code>ClusterTemplate</code> <code>default/aws-standalone-cp-0-0-1</code></li> <li><code>ClusterTemplate</code> <code>default/aws-standalone-cp-0-0-2</code> (available for the upgrade from <code>aws-standalone-cp-0-0-1</code>)</li> </ul> <p>Note</p> <ol> <li>The target <code>ClusterTemplate</code> defined as being available for the upgrade should reference the same helm chart name as the source <code>ClusterTemplate</code>. Otherwise, after the upgrade is triggered, the cluster will be removed and then recreated from scratch, even if the objects in the helm chart are the same.</li> <li>The target template should not affect immutable fields or any other incompatible internal objects upgrades, otherwise the upgrade will fail.</li> </ol>"},{"location":"template-openstack/","title":"OpenStack Machine parameters","text":""},{"location":"template-openstack/#clusterdeployment-parameters","title":"ClusterDeployment Parameters","text":"<p>To deploy an OpenStack cluster, the following are the primary parameters in the <code>ClusterDeployment</code> resource:</p> Parameter Example Description .spec.credential <code>openstack-cluster-identity-cred</code> Reference to the Credential object. .spec.template <code>openstack-standalone-cp-0-0-1</code> Reference to the ClusterTemplate. .spec.config.authURL <code>https://keystone.yourorg.net/</code> Keystone authentication endpoint for OpenStack. .spec.config.controlPlaneNumber <code>3</code> Number of control plane nodes. .spec.config.workersNumber <code>2</code> Number of worker nodes. .spec.config.clusterLabels(optional) <code>k0rdent: demo</code> Labels to apply to the cluster. Used by MultiClusterService."},{"location":"template-openstack/#ssh-configuration","title":"SSH Configuration","text":"<p><code>sshPublicKey</code> is the reference name for an existing SSH key configured in OpenStack.</p> <ul> <li>ClusterDeployment: Specify the SSH public key using the <code>.spec.config.controlPlane.sshPublicKey</code> and <code>.spec.config.worker.sshPublicKey</code> parameters (for the standlone control plane).</li> </ul>"},{"location":"template-openstack/#machine-configuration","title":"Machine Configuration","text":"<p>Configurations for control plane and worker nodes are specified separately under <code>.spec.config.controlPlane</code> and <code>.spec.config.worker</code>:</p> Parameter Example Description <code>flavor</code> <code>m1.medium</code> OpenStack flavor for the instance. <code>image.filter.name</code> <code>ubuntu-22.04-x86_64</code> Name of the image. <code>sshPublicKey</code> <code>ramesses-pk</code> Reference name for an existing SSH key. <code>securityGroups.filter.name</code> <code>default</code> Security group for the instance. <p>Note</p> <p> Make sure <code>.spec.credential</code> references the <code>Credential</code> object. The recommended minimum vCPU value for the control plane flavor is 2, while for the worker node flavor, it is 1. For detailed information, refer to the machine-flavor CAPI docs.</p>"},{"location":"template-openstack/#example-clusterdeployment","title":"Example ClusterDeployment","text":"<pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-openstack-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: openstack-standalone-cp-0-0-2\n  credential: openstack-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    clusterLabels:\n      k0rdent: demo\n    controlPlaneNumber: 1\n    workersNumber: 1\n    controlPlane:\n      sshPublicKey: my-public-key\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    worker:\n      sshPublicKey: my-public-key\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    authURL: https://my-keystone-openstack-url.com\n    identityRef:\n      name: openstack-cloud-config\n      cloudName: openstack\n      region: RegionOne\n</code></pre>"},{"location":"template-predefined/","title":"Using predefined templates","text":""},{"location":"template-predefined/#remove-templates-shipped-with-k0rdent","title":"Remove Templates shipped with k0rdent","text":"<p>If you need to limit the templates that exist in your k0rdent installation, follow the instructions below:</p> <ol> <li> <p>Get the list of <code>ProviderTemplates</code>, <code>ClusterTemplates</code> or <code>ServiceTemplates</code> shipped with k0rdent. For example, for <code>ClusterTemplate</code> objects, run:</p> <p><pre><code>kubectl get clustertemplates -n kcm-system -l helm.toolkit.fluxcd.io/name=kcm-templates\n</code></pre> <pre><code>NAME                       VALID\naws-hosted-cp              true\naws-standalone-cp          true\n</code></pre></p> </li> <li> <p>Remove the template from the list using <code>kubectl delete</code>. For example:</p> <pre><code>kubectl delete clustertemplate -n kcm-system &lt;template-name&gt;\n</code></pre> </li> </ol>"},{"location":"template-vsphere/","title":"vSphere cluster template parameters","text":""},{"location":"template-vsphere/#clusterdeployment-parameters","title":"ClusterDeployment parameters","text":"<p>To create a cluster deployment a number of parameters should be passed to the <code>ClusterDeployment</code> object.</p>"},{"location":"template-vsphere/#parameter-list","title":"Parameter list","text":"<p>The following is the list of vSphere specific parameters, which are required for successful cluster creation.</p> Parameter Example Description <code>.spec.config.vsphere.server</code> <code>vcenter.example.com</code> Address of the vSphere instance <code>.spec.config.vsphere.thumbprint</code> <code>\"00:00:00:...\"</code> Certificate thumbprint <code>.spec.config.vsphere.datacenter</code> <code>DC</code> Datacenter name <code>.spec.config.vsphere.datastore</code> <code>/DC/datastore/DS</code> Datastore path <code>.spec.config.vsphere.resourcePool</code> <code>/DC/host/vCluster/Resources/ResPool</code> Resource pool path <code>.spec.config.vsphere.folder</code> <code>/DC/vm/example</code> Folder path <code>.spec.config.controlPlane.network</code> <code>/DC/network/vm_net</code> Network path for <code>controlPlane</code> <code>.spec.config.worker.network</code> <code>/DC/network/vm_net</code> Network path for <code>worker</code> <code>.spec.config.*.ssh.publicKey</code> <code>\"ssh-ed25519 AAAA...\"</code> SSH public key in <code>authorized_keys</code> format <code>.spec.config.*.vmTemplate</code> <code>/DC/vm/templates/ubuntu</code> VM template image path <code>.spec.config.controlPlaneEndpointIP</code> <code>172.16.0.10</code> <code>kube-vip</code> vIP which will be created for control plane endpoint <p>To get the vSphere certificate thumbprint you can use the following command:</p> <pre><code>curl -sw %{certs} https://vcenter.example.com | openssl x509 -sha256 -fingerprint -noout | awk -F '=' '{print $2}'\n</code></pre> <p><code>govc</code>, a vSphere CLI, can also help to discover proper values for some of the parameters:</p> <pre><code># vsphere.datacenter\ngovc ls\n\n# vsphere.datastore\ngovc ls /*/datastore/*\n\n# vsphere.resourcePool\ngovc ls /*/host/*/Resources/*\n\n# vsphere.folder\ngovc ls -l /*/vm/**\n\n# controlPlane.network, worker.network\ngovc ls /*/network/*\n\n# *.vmTemplate\ngovc vm.info -t '*'\n</code></pre> <p>Note</p> <p> Follow official <code>govc</code> installation instructions from here. The <code>govc</code> usage guide is here.</p> <p>Minimal <code>govc</code> configuration requires setting: <code>GOVC_URL</code>, <code>GOVC_USERNAME</code>, <code>GOVC_PASSWORD</code> environment variables.</p>"},{"location":"template-vsphere/#example-of-clusterdeployment-cr","title":"Example of ClusterDeployment CR","text":"<p>With all above parameters provided your <code>ClusterDeployment</code> can look like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: vsphere-standalone-cp-0-0-2\n  credential: vsphere-credential\n  config:\n    clusterLabels: {}\n    vsphere:\n      server: vcenter.example.com\n      thumbprint: \"00:00:00\"\n      datacenter: \"DC\"\n      datastore: \"/DC/datastore/DC\"\n      resourcePool: \"/DC/host/vCluster/Resources/ResPool\"\n      folder: \"/DC/vm/example\"\n    controlPlaneEndpointIP: \"&lt;VSPHERE_SERVER&gt;\"\n    controlPlane:\n      ssh:\n        user: ubuntu\n        publicKey: |\n          ssh-rsa AAA...\n      rootVolumeSize: 50\n      cpus: 2\n      memory: 4096\n      vmTemplate: \"/DC/vm/template\"\n      network: \"/DC/network/Net\"\n    worker:\n      ssh:\n        user: ubuntu\n        publicKey: |\n          ssh-rsa AAA...\n      rootVolumeSize: 50\n      cpus: 2\n      memory: 4096\n      vmTemplate: \"/DC/vm/template\"\n      network: \"/DC/network/Net\"\n</code></pre> <p>Don't forget to replace placeholder values such as <code>VSPHERE_SERVER</code> with actual values for your environment.</p>"},{"location":"template-vsphere/#ssh","title":"SSH","text":"<p>Currently SSH configuration on vSphere expects that the user is already created before template creation. Because of that you must pass username along with SSH public key to configure SSH access.</p> <p>The SSH public key can be passed to <code>.spec.config.ssh.publicKey</code> (in the case of a hosted control plane) parameter or <code>.spec.config.controlPlane.ssh.publicKey</code> and <code>.spec.config.worker.ssh.publicKey</code> parameters (in the case of a standalone control) of the <code>ClusterDeployment</code> object.</p> <p>SSH public key must be passed literally as a string.</p> <p>You can pass the username passed to <code>.spec.config.controlPlane.ssh.user</code>, <code>.spec.config.worker.ssh.user</code> or <code>.spec.config.ssh.user</code> depending on you deployment model.</p>"},{"location":"template-vsphere/#vm-resources","title":"VM resources","text":"<p>The following parameters are used to define VM resources:</p> Parameter Example Description <code>.rootVolumeSize</code> <code>50</code> Root volume size in GB (can't be less than one defined in the image) <code>.cpus</code> <code>2</code> Number of CPUs <code>.memory</code> <code>4096</code> Memory size in MB <p>The resource parameters are the same for hosted and standalone CP deployments, but they are positioned differently in the spec, which means that they're going to:</p> <ul> <li><code>.spec.config</code> in case of hosted CP deployment.</li> <li><code>.spec.config.controlPlane</code> in in case of standalone CP for control plane   nodes.</li> <li><code>.spec.config.worker</code> in in case of standalone CP for worker nodes.</li> </ul>"},{"location":"template-vsphere/#vm-image-and-network","title":"VM Image and network","text":"<p>To provide image template path and network path the following parameters must be used:</p> Parameter Example Description <code>.vmTemplate</code> <code>/DC/vm/template</code> Image template path <code>.network</code> <code>/DC/network/Net</code> Network path <p>As with resource parameters the position of these parameters in the <code>ClusterDeployment</code> depends on deployment type and these parameters are used in:</p> <ul> <li><code>.spec.config</code> in case of hosted CP deployment.</li> <li><code>.spec.config.controlPlane</code> in in case of standalone CP for control plane   nodes.</li> <li><code>.spec.config.worker</code> in in case of standalone CP for worker nodes.</li> </ul>"},{"location":"template-vsphere/#hosted-control-plane-k0smotron-deployment","title":"Hosted control plane (k0smotron) deployment","text":""},{"location":"template-vsphere/#prerequisites","title":"Prerequisites","text":"<ul> <li>Management Kubernetes cluster (v1.28+) deployed on vSphere with k0rdent installed   on it</li> </ul> <p>Keep in mind that all control plane components for all managed clusters will reside in the management cluster, so make sure the server is robust enough to handle it.</p>"},{"location":"template-vsphere/#clusterdeployment-manifest","title":"ClusterDeployment manifest","text":"<p>The hosted CP template has mostly identical parameters to the standalone CP, and you can check them in the template parameters section.</p> <p>Note</p> <p> The vSphere provider requires the control plane endpoint IP to be specified before deploying the cluster. Ensure that this IP matches the IP assigned to the k0smotron load balancer (LB) service. Provide the control plane endpoint IP to the k0smotron service via an annotation accepted by your LB provider (e.g., the <code>kube-vip</code> annotation in the example below).</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: vsphere-hosted-cp-0-0-2\n  credential: vsphere-credential\n  config:\n    clusterLabels: {}\n    vsphere:\n      server: vcenter.example.com\n      thumbprint: \"00:00:00\"\n      datacenter: \"DC\"\n      datastore: \"/DC/datastore/DC\"\n      resourcePool: \"/DC/host/vCluster/Resources/ResPool\"\n      folder: \"/DC/vm/example\"\n    controlPlaneEndpointIP: \"&lt;VSPHERE_SERVER&gt;\"\n    ssh:\n      user: ubuntu\n      publicKey: |\n        ssh-rsa AAA...\n    rootVolumeSize: 50\n    cpus: 2\n    memory: 4096\n    vmTemplate: \"/DC/vm/template\"\n    network: \"/DC/network/Net\"\n    k0smotron:\n      service:\n        annotations:\n          kube-vip.io/loadbalancerIPs: \"&lt;VSPHERE_LOADBALANCER_IP&gt;\"\n</code></pre> <p>Don't forget to substitute placeholders such as <code>&lt;VSPHERE_SERVER&gt;</code> with actual values.</p>"},{"location":"user-create-cluster/","title":"Deploying a Cluster","text":"<p>k0rdent simplifies the process of deploying and managing Kubernetes clusters across various cloud platforms through the use of <code>ClusterDeployment</code>s, which include all of the information k0rdent needs to know in order to create the cluster you want. This <code>ClusterDeployment</code> system relies on predefined templates and credentials. </p> <p>A cluster deployment typically involves:</p> <ol> <li>Credentials for the infrastructure provider (e.g. AWS, vSphere, etc).</li> <li>A template that defines the desired cluster configuration (for example, number of nodes, instance types).</li> <li>Submitting the configuration for deployment and monitoring the process.</li> </ol> <p>Follow these steps to deploy a standalone Kubernetes cluster:</p> <ol> <li> <p>Obtain the <code>Credential</code> object</p> <p>k0rdent needs credentials to communicate with the infrastructure provider (for example, AWS, Azure, vSphere). These credentials enable k0rdent to provision resources such as virtual machines, networking components, and storage.</p> <p><code>Credential</code> objects are generally created ahead of time and made available to users. You can see all of the existing <code>Credential</code> objects by querying the management cluster:</p> <p><pre><code>kubectl get credentials -n accounting\n</code></pre> When you find a <code>Credential</code> that looks appropriate, you can get more information by <code>describe</code>-ing it, as in:</p> <pre><code>kubectl describe credential accounting-cluster-credential -n accounting\n</code></pre> <p>You'll see the YAML for the <code>Credential</code> object, as in:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: accounting-cluster-credential\n  namespace: accounting\nspec:\n  description: \"Credentials for Accounting AWS account\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: accountingd-cluster-identity\n</code></pre> <p>As you can see, the <code>.spec.description</code> field gives more information about the <code>Credential</code>.</p> <p>If the <code>Credential</code> you need doesn't yet exist, you can ask your cloud administrator to create it, or you can follow the instructions in the Credential System, as well as the specific instructions for your target infrastructure, to crete it yourself.</p> <p>Tip</p> <p>Double-check to make sure that your credentials have sufficient permissions to create resources on the target infrastructure.</p> </li> <li> <p>Select a Template</p> <p>Templates in k0rdent are predefined configurations that describe how to set up the cluster. Templates include details such as:</p> <ul> <li>The number and type of control plane and worker nodes.</li> <li>Networking settings.</li> <li>Regional deployment preferences.</li> </ul> <p>Templates act as a blueprint for creating a cluster. To see the list of available templates, use the following command:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n</code></pre></p> <p>You can then get information on the actual template by describing it, as in:</p> <pre><code>kubectl describe clustertemplate aws-standalone-cp-0-0-5 -n kcm-system\n</code></pre> </li> <li> <p>Create a ClusterDeployment YAML Configuration</p> <p>Once you have the <code>Credential</code> and the <code>ClusterTemplate</code> you can create the <code>ClusterDeployment</code> object configuration.  It includes:</p> <ul> <li>The template to use.</li> <li>The credentials for the infrastructure provider.</li> <li>Optional customizations such as instance types, regions, and networking.</li> </ul> <p>Create a <code>ClusterDeployment</code> configuration in a YAML file, following this structure:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;cluster-name&gt;\n  namespace: &lt;kcm-system-namespace&gt;\nspec:\n  template: &lt;template-name&gt;\n  credential: &lt;infrastructure-provider-credential-name&gt;\n  dryRun: &lt;\"true\" or \"false\" (default: \"false\")&gt;\n  config:\n    &lt;cluster-configuration&gt;\n</code></pre> <p>You will of course want to replace the placeholders with actual values. (For more information about <code>dryRun</code> see Understanding the Dry Run) For example, this is a simple AWS infrastructure provider <code>ClusterDeployment</code>:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-managed-cluster\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-3\n  credential: aws-credential\n  config:\n    clusterLabels: {}\n    region: us-west-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre> Note that the <code>.spec.credential</code> value should match the <code>.metadata.name</code> value of a created <code>Credential</code> object.</p> </li> <li> <p>Apply the Configuration</p> <p>Once the <code>ClusterDeployment</code> configuration is ready, apply it to the k0rdent management cluster:</p> <pre><code>kubectl apply -f clusterdeployment.yaml\n</code></pre> <p>This step submits your deployment request to k0rdent. </p> </li> <li> <p>Verify Deployment Status</p> <p>After submitting the configuration, verify that the <code>ClusterDeployment</code> object has been created successfully:</p> <pre><code>kubectl -n &lt;namespace&gt; get clusterdeployment.kcm &lt;cluster-name&gt; -o=yaml\n</code></pre> <p>The output shows the current status and any errors.</p> </li> <li> <p>Monitor Provisioning</p> <p>k0rdent will now start provisioning resources (e.g., VMs, networks) and setting up the cluster. To monitor this process, run:</p> <pre><code>kubectl -n &lt;namespace&gt; get cluster &lt;cluster-name&gt; -o=yaml\n</code></pre> </li> <li> <p>Retrieve the Kubernetes Configuration</p> <p>When provisioning is complete, you can retrieve the kubeconfig file for the new cluster so you can interact with the cluster using <code>kubectl</code>:</p> <p><pre><code>kubectl get secret -n &lt;namespace&gt; &lt;cluster-name&gt;-kubeconfig -o=jsonpath={.data.value} | base64 -d &gt; kubeconfig\n</code></pre> You can then use this file to access the cluster, as in:</p> <pre><code>export KUBECONFIG=kubeconfig\nkubectl get pods -A\n</code></pre> <p>Store the kubeconfig file securely, as it contains authentication details for accessing the cluster.</p> </li> </ol>"},{"location":"user-create-cluster/#cleanup","title":"Cleanup","text":"<p>When you're finished you'll want to remove the cluster. Because the cluster is represented by the <code>ClusterDeployment</code> object, deleting the cluster is a simple matter of deleting that object.  For example:</p> <pre><code>kubectl delete clusterdeployment &lt;cluster-name&gt; -n kcm-system\n</code></pre> <p>Note that even though the Kubernetes object is deleted immediately, it will take a few minutes for the actual resources to be removed.</p>"},{"location":"user-create-service/","title":"Deploy Services to a Managed Cluster","text":"<p>At its heart, everything in k0rdent is based on templates that help define Kubernetes objects. For clusters, these are <code>ClusterTemplate</code>s. For applications and services, these are <code>ServiceTemplate</code>s.</p>"},{"location":"user-create-service/#understanding-servicetemplates","title":"Understanding <code>ServiceTemplate</code>s","text":"<p><code>ServiceTemplate</code>s are meant to let k0rdent know where to find a Helm chart with instructions for installing an application. In many cases, these charts will be in a private repository.  For example, consider this template for installing Nginx Ingress:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ServiceTemplate\nmetadata:\n  name: project-ingress-nginx-4.11.0\n  namespace: tenant42\nspec:\n  helm:\n    chartSpec:\n      chart: demo-ingress-nginx\n      version: 4.11.0\n      interval: 10m0s\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-demos\n---\napiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ServiceTemplateChain\nmetadata:\n  name: project-ingress-nginx-4.11.0\n  namespace: tenant42\nspec:\n  supportedTemplates:\n    - name: project-ingress-nginx-4.11.0\n    - name: project-ingress-nginx-4.10.0\n      availableUpgrades:\n        - name: project-ingress-nginx-4.11.0\n</code></pre> <p>Here you see a template called <code>project-ingress-nginx-4.11.0</code> that is meant to be deployed in the <code>tenant42</code> namespace. The <code>.spec.helm.chartSpec</code> specifies the name of the Helm chart and where to find it, as well as the version and other  important information. The <code>ServiceTemplateChain</code> shows that this template is also an upgrade path from version 4.10.0.</p> <p>If you wanted to deploy this as an application, you would first go ahead and add it to the cluster in which you were working, so if you were to save this YAML to a file called <code>project-ingress.yaml</code> you could run this command on the management cluster:</p> <pre><code>kubectl apply -f project-ingress.yaml -n tenant42\n</code></pre>"},{"location":"user-create-service/#adding-a-service-to-a-clusterdeployment","title":"Adding a <code>Service</code> to a <code>ClusterDeployment</code>","text":"<p>To add the service defined by this template to a cluster, you would simply add it to the <code>ClusterDeployment</code> object when you create it, as in:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-managed-cluster\n  namespace: tenant42\nspec:\n  config:\n    clusterLabels: {}\n  template: aws-standalone-cp-0-0-3\n  credential: aws-credential\n  services:\n    - template: project-ingress-nginx-4.11.0\n      name: ingress-nginx\n      namespace: tenant42\n  servicesPriority: 100\n  stopOnConflict: false\n</code></pre> As you can see, you're simply referencing the template in the <code>.spec.services.template</code> field of the <code>ClusterDeployment</code> to tell k0rdent that you want this service to be part of this cluster.</p> <p>If you wanted to add this serice to an existing cluster, you would simply patch the definition of the <code>ClusterDeployment</code>, as in:</p> <pre><code>kubectl patch clusterdeployment my-managed-cluster -n tenant42 --type='merge' -p '\nspec:\n  services:\n    - template: project-ingress-nginx-4.11.0\n      name: ingress-nginx\n      namespace: tenant42\n</code></pre> <p>Let's look at a more complex case, involving deploying beach-head services on a single cluster.</p>"},{"location":"user-create-service/#deployment-of-beach-head-services","title":"Deployment of beach-head services","text":"<p>Beach-head services can be installed on a cluster deployment (that is, a target cluster) using the <code>ClusterDeployment</code> object, just as with a single service. Consider the following example of a <code>ClusterDeployment</code> object for AWS Infrastructure Provider with beach-head services.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-managed-cluster\n  namespace: kcm-system\nspec:\n  config:\n    clusterLabels: {}\n    clusterIdentity:\n      name: aws-cluster-identity\n      namespace: kcm-system\n    controlPlane:\n      amiID: ami-0eb9fdcf0d07bd5ef\n      instanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n      instanceType: t3.small\n      controlPlaneNumber: 1\n      publicIP: true\n      region: ca-central-1\n    worker:\n      amiID: ami-0eb9fdcf0d07bd5ef\n      instanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io\n      instanceType: t3.small\n    workersNumber: 1\n  credential: aws-cluster-identity-cred\n  serviceSpec:\n    services:\n      - template: kyverno-3-2-6\n        name: kyverno\n        namespace: kyverno\n      - template: ingress-nginx-4-11-3\n        name: ingress-nginx\n        namespace: ingress-nginx\n    priority: 100\n  template: aws-standalone-cp-0-0-6\n</code></pre> <p>In the example above the fields under <code>serviceSpec</code> are relevant to the deployment of beach-head services.</p> <p>Note</p> <p> Refer to the Template Guide for more detail about these fields.</p> <p>This example <code>ClusterDeployment</code> object deploys kyverno and ingress-nginx, as referred to by their service templates respectively, on the target cluster.  As before, the <code>ServiceTemplate</code> includes information on the service. For example, here is the <code>ServiceTemplate</code> for kyverno:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ServiceTemplate\nmetadata:\n  name: kyverno-3-2-6\n  annotations:\n    helm.sh/resource-policy: keep\nspec:\n  helm:\n    chartSpec:\n      chart: kyverno\n      version: 3.2.6\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: kcm-templates\n      version: 3.2.6\n        name: k0rdent-templates\n</code></pre> <p>The <code>k0rdent-templates</code> helm repository hosts the actual kyverno chart version 3.2.6. For more details see the Bring your own Templates guide.</p>"},{"location":"user-create-service/#configuring-custom-values","title":"Configuring Custom Values","text":"<p>Helm values can be passed to each beach-head services with the <code>.spec.serviceSpec.services[].values</code> field in the <code>ClusterDeployment</code> or <code>MultiClusterService</code> object. For example:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-clusterdeployment\n  namespace: kcm-system\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - template: ingress-nginx-4-11-3\n      name: ingress-nginx\n      namespace: ingress-nginx\n      values: |\n        ingress-nginx:\n          controller:\n            replicaCount: 3\n    - name: kyverno\n      namespace: kyverno\n      template: kyverno-3-2-6\n      values: |\n        kyverno:\n          admissionController:\n            replicas: 3\n    - name: motel-regional\n      namespace: motel\n      template: motel-regional-0-1-1\n      values: |\n        victoriametrics:\n          vmauth:\n            ingress:\n              host: vmauth.kcm0.example.net\n            credentials:\n              username: motel\n              password: motel\n        grafana:\n          ingress:\n            host: grafana.kcm0.example.net\n   . . .\n</code></pre> </p> <p>Note</p> <p>The values for ingress-nginx and kyverno start with the \"ingress-nginx:\" and \"kyverno:\" keys respectively because the helm charts used by the ingress-nginx-4-11-3 and kyverno-3-2-6 <code>ServiceTemplates</code> use the official upstream helm charts for ingress-nginx and kyverno as dependencies.</p>"},{"location":"user-create-service/#templating-custom-values","title":"Templating Custom Values","text":"<p>Using the Sveltos templating feature, we can also write templates that can be useful for automatically fetching pre-existing information within the cluster. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-clusterdeployment\n  namespace: kcm-system\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - template: motel-0-1-0\n      name: motel\n      namespace: motel\n    - template: myappz-0-3-0\n      name: myappz\n      namespace: myappz\n      values: |\n        controlPlaneEndpointHost: {{ .Cluster.spec.controlPlaneEndpoint.host }}\n        controlPlaneEndpointPort: \"{{ .Cluster.spec.controlPlaneEndpoint.port }}\"\n    priority: 100\n    . . .        \n</code></pre> <p>In this case, the host and port information will be fetched from the spec of the CAPI cluster that hosts this <code>ClusterDeployment</code>.</p>"},{"location":"user-create-service/#checking-status","title":"Checking status","text":"<p>The <code>.status.services</code> field of the <code>ClusterDeployment</code> object shows the status for each of the beach-head services. For example, if you were to <code>describe</code> the <code>ClusterDeployment</code> with these services, you would see <code>condition</code>s that show status information, as in:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  . . .\n  generation: 1\n  name: wali-aws-dev\n  namespace: kcm-system\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-3\n    - name: kyverno\n      namespace: kyverno\n      template: kyverno-3-2-6\n    . . .\nstatus:\n  . . .\n  observedGeneration: 1\n  services:\n  - clusterName: my-managed-cluster\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-12-11T23:03:05Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-12-11T23:03:05Z\"\n      message: Release kyverno/kyverno\n      reason: Managing\n      status: \"True\"\n      type: kyverno.kyverno/SveltosHelmReleaseReady\n    - lastTransitionTime: \"2024-12-11T23:03:05Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre> <p>Based on the information above both kyverno and ingress-nginx are installed in their respective namespaces on the target cluster. You can check to see for yourself:</p> <p><pre><code>kubectl get pod -n kyverno\n</code></pre> <pre><code>NAME                                             READY   STATUS    RESTARTS   AGE\nkyverno-admission-controller-96c5d48b4-sg5ts     1/1     Running   0          2m39s\nkyverno-background-controller-65f9fd5859-tm2wm   1/1     Running   0          2m39s\nkyverno-cleanup-controller-848b4c579d-ljrj5      1/1     Running   0          2m39s\nkyverno-reports-controller-6f59fb8cd6-s8jc8      1/1     Running   0          2m39s\n</code></pre> <pre><code>kubectl get pod -n ingress-nginx \n</code></pre> <pre><code>NAME                                       READY   STATUS    RESTARTS   AGE\ningress-nginx-controller-cbcf8bf58-zhvph   1/1     Running   0          24m\n</code></pre></p> <p>Youc an get more information on how to access the managed cluster in the create a cluster deployment chapter, and more on <code>ServiceTemplate</code>s in the Template Guide.</p>"},{"location":"user-create-service/#removing-beach-head-services","title":"Removing beach-head services","text":"<p>To remove a beach-head service simply remove its entry from <code>.spec.serviceSpec.services</code>. The example below removes <code>kyverno-3-2-6</code>, so its status also removed from <code>.status.services</code>.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  . . .\n  generation: 2\n  name: wali-aws-dev\n  namespace: kcm-system\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-3\n    priority: 100\n    . . .\nstatus:\n  . . .\n  observedGeneration: 2\n  services:\n  - clusterName: wali-aws-dev\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-12-11T23:15:45Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-12-11T23:15:45Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre>"},{"location":"user-create-service/#parameter-list","title":"Parameter List","text":"<p>Here is an idea of the parameters involved.</p> Parameter Example Description <code>.spec.serviceSpec.syncMode</code> <code>Continuous</code> Specifies how beach-head services are synced i the target cluster (default:<code>Continuous</code>) <code>.spec.serviceSpec.DriftIgnore</code> specifies resources to ignore for drift detection <code>.spec.serviceSpec.DriftExclusions</code> specifies specific configurations of resources to ignore for drift detection <code>.spec.serviceSpec.priority</code> <code>100</code> Sets the priority for the beach-head services defined in this spec (default: <code>100</code>) <code>.spec.serviceSpec.stopOnConflict</code> <code>false</code> Stops deployment of beach-head services upon first encounter of a conflict (default: <code>false</code>) <code>.spec.serviceSpec.services[].template</code> <code>kyverno-3-2-6</code> Name of the <code>ServiceTemplate</code> object located in the same namespace <code>.spec.serviceSpec.services[].name</code> <code>my-kyverno-release</code> Release name for the beach-head service <code>.spec.serviceSpec.services[].namespace</code> <code>my-kyverno-namespace</code> Release namespace for the beach-head service (default: <code>.spec.services[].name</code>) <code>.spec.serviceSpec.services[].values</code> <code>replicas: 3</code> Helm values to be used with the template while deployed the beach-head services <code>.spec.serviceSpec.services[].valuesFrom</code> `` Can reference a ConfigMap or Secret containing helm values <code>.spec.serviceSpec.services[].disable</code> <code>false</code> Disable handling of this beach-head service (default: <code>false</code>)"},{"location":"user-enable-drift-detection/","title":"Detecting and Correcting Drift","text":""},{"location":"user-enable-drift-detection/#enabling-drift-detection","title":"Enabling Drift Detection","text":"<p>Set <code>.spec.serviceSpec.syncMode=Continuous</code> in the <code>ClusterDeployment</code> or <code>MultiClusterService</code> object to enable drift detection and correction. Sveltos will then automatically deploy the drift-detection-manager on the targeted clusters:</p> <p><pre><code>kubectl -n projectsveltos get deployments.apps \n</code></pre> <pre><code>NAME                      READY   UP-TO-DATE   AVAILABLE   AGE\ndrift-detection-manager   1/1     1            1           152m\nsveltos-agent-manager     1/1     1            1           152m\n</code></pre></p>"},{"location":"user-enable-drift-detection/#how-drift-detection-and-correction-works","title":"How Drift Detection and Correction Works","text":"<p>The drift-detection-manager watches for the deployed helm chart resources (i.e., the resources deployed via a <code>ServiceTemplate</code>) and if it detects any changes  in the spec of the resources based on hash value, it updates the status of the <code>ResourceSummary</code> object. This change triggers the addon-controller in the \"projectsveltos\" namespace in the management cluster to update the status of the associated <code>ClusterSummary</code> object, which then triggers a reconcile to  re-deploy the spec to the target cluster.</p> <p>Note</p> <p>The <code>ResourceSummary</code> and <code>ClusterSummary</code> are CRDs provided by Sveltos.</p>"},{"location":"user-enable-drift-detection/#using-drift-ignore","title":"Using Drift Ignore","text":"<p>Certain resources can be completely opted out of drift correction by using this feature. In the following example, the \"ingress-nginx/ingress-nginx-controller\" deployment is ignored for drift correction on the target cluster.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n      . . .\n    priority: 100\n    syncMode: ContinuousWithDriftDetection\n    driftIgnore:\n      - target:\n        group: apps\n        version: v1\n        kind: Deployment\n        name: ingress-nginx-controller\n        namespace: ingress-nginx\n  . . .\n</code></pre> <p>If we manually remove the <code>app.kubernetes.io/managed-by=Helm</code> label, we can observe that the drift is not corrected as can be seen in the following watch output.</p> <p><pre><code>kubectl -n ingress-nginx get deployments.apps ingress-nginx-controller --show-labels -w\n</code></pre> <pre><code>NAME                       READY   UP-TO-DATE   AVAILABLE   AGE     LABELS\ningress-nginx-controller   3/3     3            3           3h58m   app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginx,app.kubernetes.io/version=1.11.0,helm.sh/chart=ingress-nginx-4.11.0\ningress-nginx-controller   3/3     3            3           3h59m   app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginx,app.kubernetes.io/version=1.11.0,helm.sh/chart=ingress-nginx-4.11.0\n</code></pre></p> <p>This can also be verified by observing that <code>ignoreForConfigurationDrift: true</code> for the targeted resource in the <code>ResourceSummary</code> spec on the target cluster.</p> <pre><code>kind: ResourceSummary\nmetadata:\n  . . .\nspec:\n  chartResources:\n  - chartName: ingress-nginx\n    group:\n    . . .\n    - group: apps\n      ignoreForConfigurationDrift: true\n      kind: Deployment\n      name: ingress-nginx-controller\n      namespace: ingress-nginx\n      version: v1\n    releaseName: ingress-nginx\n    releaseNamespace: ingress-nginx\nstatus:\n  helmResourceHashes:\n  . . .\n</code></pre> <p>Yet another way to check if a resource is being ignored for drift is by verifying that <code>projectsveltos.io/driftDetectionIgnore: ok</code> annotation has been applied to it as can be seen below.</p> <pre><code>kubectl -n ingress-nginx get deployments.apps ingress-nginx-controller -o=jsonpath='{.metadata.annotations}'\n{\"deployment.kubernetes.io/revision\":\"1\",\"meta.helm.sh/release-name\":\"ingress-nginx\",\"meta.helm.sh/release-namespace\":\"ingress-nginx\",\"projectsveltos.io/driftDetectionIgnore\":\"ok\"}%\n</code></pre>"},{"location":"user-enable-drift-detection/#removing-drift-ignore","title":"Removing Drift Ignore","text":"<p>The drift ignore setting can be removed by removing the <code>.spec.serviceSpec.driftIgnore</code>.</p>"},{"location":"user-enable-drift-detection/#using-drift-exclusions","title":"Using Drift Exclusions","text":"<p>Certain fields of a resource can be excluded from drift detection using this feature. In the following example, the <code>.spec.replicas</code> field of the \"ingress-nginx/ingress-nginx-controller\" deployment on the target cluster is excluded from drift detection.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n      . . .\n    priority: 100\n    syncMode: ContinuousWithDriftDetection\n    driftExclusions:\n      - paths:\n        - \"/spec/replicas\"\n        target:\n          kind: Deployment\n          name: ingress-nginx-controller\n          namespace: ingress-nginx\n  . . .\n</code></pre> <p>If we manually edit the replicas to be 1, the number of replicas is not corrected back to 3 as is indicated by the following watch output.</p> <pre><code>kubectl -n ingress-nginx get deployments.apps ingress-nginx-controller -o=jsonpath='{.spec.replicas}' -w\n3111\n</code></pre> <p>We can also verify that this is the case by observing that the <code>ResourceSummary</code> object has the following patch in its spec now.</p> <pre><code>kind: ResourceSummary\nmetadata:\n  . . .\nspec:\n  chartResources:\n  - chartName: ingress-nginx\n    group:\n    . . .\n    releaseName: ingress-nginx\n    releaseNamespace: ingress-nginx\n  patches:\n  - patch: |-\n      - op: remove\n        path: /spec/replicas\n    target:\n      kind: Deployment\n      name: ingress-nginx-controller\n      namespace: ingress-nginx\nstatus:\n  helmResourceHashes:\n  . . .\n</code></pre>"},{"location":"user-enable-drift-detection/#removing-drift-exclusion","title":"Removing Drift Exclusion","text":"<p>The drift exclusion can be removed by removing the <code>.spec.serviceSpec.driftExclusion</code> field and re-triggering the drift correction by editing any field in the \"ingress-nginx/ingress-nginx-controller\" deployment. This will force a drift correction and since the drift exclusion has been removed, it will restore the deployment to it's original spec.</p>"},{"location":"why-k0rdent/","title":"Why k0rdent?","text":"<p>k0rdent was developed to provide for the needs of platform engineers and the developers that they serve, as well as the application workloads that they support. </p>"},{"location":"why-k0rdent/#applications-and-workloads","title":"Applications and Workloads","text":"<p>Workloads evolve and grow, often faster then the infrastructure needed to support them. Infrastructure is almost always the lagging factor in getting new or existing applications into the hands of users and scaling them to meet user expectations. This challenge has grown exponentially with the rise of AI and ML workloads. Specifically:</p> <ul> <li>Workload complexity is increasing</li> <li>Modern workloads depend on specialized infrastructure</li> <li>Developers have high expectations of time to value</li> </ul>"},{"location":"why-k0rdent/#platform-engineering","title":"Platform Engineering","text":"<p>Modern infrastructure systems are increasingly complex, and administrators need to manage that complexity while still responding quickly to developers' needs efficiently as possible. This has led to development of internal developer platforms (IDP) and platform engineering. These environments provide the frameworks and tools for increasing developer productivity when developing, deploying, and managing applications and services, enabling developers to focus on their specific tasks or goals and not the underlying complexities. Overall:</p> <ul> <li>Developer platforms increase developer productivity</li> <li>Platform engineers need to implement and grow the platforms</li> <li>Infrastructure needs to support the required complexity</li> </ul>"},{"location":"why-k0rdent/#modern-infrastructure-systems","title":"Modern Infrastructure Systems","text":"<p>The increasingly distributed nature of modern infrastructure systems and the demands of modern workloads is leading to increasing complexity. Solutions need to solve a diverse set of challenges and provide consistency, repeatability, and prevention of lock-in, all without increasing the burden on operators. Modern platform engineers and operators are increasingly time constrained with the vast number of challenges they need to overcome, including security and compliance, cost management, resilience, and scale to name but a few. Keep in mind that:</p> <ul> <li>Distributed deployments are the new normal</li> <li>Infrastructure management is not just a technical problem</li> <li>Operators need to focus on building value chains</li> </ul>"},{"location":"why-k0rdent/#open-source","title":"Open Source","text":"<p>The open source ecosystem, and especially Kubernetes, is mature and offers an increasing number of tools that solve real problems. The open source ecosystem, if leveraged correctly, also supports building unique architectures to support a business' needs while helping to avoid lock-in and architectural dead ends. All of these tools need to be selected, deployed, and lifecycle-managed in a way that is repeatable and traceable. Open source:</p> <ul> <li>Prevents lock-in and supports architectural self determination</li> <li>Solutions allow for solving of problems in unique ways</li> <li>Can help solve the problem of managing the complexity of modern infrastructure</li> </ul>"}]}